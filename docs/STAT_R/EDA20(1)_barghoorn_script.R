Digitale Datenanalyse und statistische Methoden
1.1 Was ist Statistik? 
W�hrend die meisten Wissenschaften eine zumindest formal klare Definition besitzen und sich somit von anderen Wissenschaften abgrenzen k�nnen, gelingt dies bei der Statistik nicht so einfach. Ein Anhaltspunkt daf�r sind schon die vielen sehr unterschiedlichen Definitionen von �Statistik�, die in den verschiedenen Lehrb�chern zu finden sind. Aus diesem Grund soll hier auch nicht eine weitere Definition hinzugef�gt werden. Vielmehr ist dies der Versuch, eine kurze �bersicht �ber die Geschichte, die Entwicklung und die Einsatzgebiete von statistischen Methoden zu geben.
Definition Meyers Lexikon 1861: �Statistik (entweder vom lat. status, Zustand, oder von dem seit der Mitte des 17. Jahrhunderts in Deutschland gebr�uchlichen Worte statista), Staatenkunde, d.i. die Darstellung der zu einem bestimmten Zeitpunkt innerhalb eines gewissen politisch abgegrenzten Landes vorhandenen Staatskr�fte und der Gesetze ihrer Wirksamkeit, wobei das wesentlich Gleichartige nach allgemeinen Gesichtspunkten zusammengefasst wird. Um aber die Wirksamkeit der Staatskr�fte bemessen zu k�nnen und die Gesetze ihrer Wirksamkeit kennen zu lernen, m�ssen fr�here mit sp�teren, �ltere mit j�ngeren Zust�nden verglichen, muss die Gegenwart aus der Vergangenheit, d.h. aus der Geschichte, erkl�rt werden. Die Gegenst�nde der Statistik sind zun�chst die allgemeinen Verh�ltnisse von Land und Volk�. 
�Obgleich sich die Statistik erst in neuerer Zeit zu einer selbstst�ndigen Wissenschaft gestaltet hat, so war doch die Erfassung und Zusammenstellung statistischer Momente auch dem Altertum und Mittelalter nicht fremd. So findet sich statistisches Material bei den Griechen in den Schriften des Herodot, Aristoteles, Eratosthenes, Strabo und Pausanias, bei den R�mern in denen des Tacitus und Plinius des J�ngeren.�
�In der neueren Zeit (1867) sind die statistischen Bureaux in Aufnahme gekommen, in welche die tabellarischen Erhebungen �ber das statistische Material zusammenflie�en, und denen dann die Zusammenstellung und Publikation desselben und der sich daraus ergebenden Resultate obliegt�. 
1.2 Geschichte, zwei Wurzeln
Die Wurzeln der heutigen Statistik liegen in zwei ganz unterschiedlichen Bereichen. Eine wichtige Grundlage der Wahrscheinlichkeitsrechnung wurde im 17. Jahrhundert gelegt, als bedeutende Mathematiker wie Blaise Pascal oder Pierre Simon de Laplace und Gl�cksspieler wie Girolamo Cardano sich f�r die Mechanismen und den Determinismus im Rahmen von Gl�cksspielen zu interessieren begannen. Determinismus bedeutet hier die M�glichkeit, auch �ber zuf�llige Ereignisse sichere Aussagen machen zu k�nnen, wenn man diese Ereignisse nur oft genug wiederholt bzw. lange genug beobachtet. Dass dies m�glich sei, war lange Zeit nicht denkbar. Auch Bernoulli meinte noch 1713: �Eines Tages w�rden Gl�cksspiele mit W�rfeln und M�nzen genauso primitiv erscheinen, dann n�mlich, wenn die Mechanik vollkommen entwickelt sei.� Erst als die Vereinbarkeit von Determinismus und Wahrscheinlichkeiten erkannt wurde, konnte die Wahrscheinlichkeitstheorie wissenschaftlich behandelt und entwickelt werden. 
Der zweite wichtige Ausgangspunkt lag in der �Zustandsbeschreibung des Staates�. Bereits im 16. Jahrhundert wurden in vielen Pfarrgemeinden Geburten und Sterbef�lle aufgezeichnet. Aus der Londoner Sterbeliste fertigte der englische Kaufmann John Graunt eine erste Sterbetafel, die ein Vorbild f�r weitere Entwicklungen (z.B. Edmund Halley: Sterblichkeitstafel von Breslau) darstellte. Diese Sterblichkeitstafeln und damit verbunden die M�glichkeit, die Lebenserwartung zu berechnen, waren Grundlagen f�r die Bestimmung von Rentenpreisen, die man als Vorform der heute �blichen Lebensversicherungen bezeichnen k�nnte, H�he der Mitgift, Aussichten auf eine Erbschaft usw. Die Erhebung dieser Daten war auch f�r die Regierung vieler Staaten von Interesse, wurde aber in verschiedenen Regionen mit sehr unterschiedlicher Konsequenz und Genauigkeit vorangetrieben. Als f�hrend stellten sich in diesen Bereichen Schweden und Frankreich heraus. Auch in diesem Zusammenhang spielte der Determinismus eine gro�e Rolle, wenn auch in einer anderen Auspr�gung. So haben John Arbuthnot (1710) bzw. Peter S��milch (1775) anhand der Gleichm��igkeit der Geschlechter in den Geburtstafeln �bewiesen�, dass es eine g�ttliche Vorsehung geben m�sse. 
1.3 Entwicklung der Statistik
Ab dem 19. Jahrhundert wurde die Wissenschaft Statistik mit der Gr�ndung von statistischen Gesellschaften erstmals institutionalisiert. Gleichzeitig war man sich einig, eine ganz bestimmte Richtung vertreten zu wollen. Gem�� einem stark positivistischen Ansatz sollte die Statistik m�glichst objektiv neutrales Wissen ansammeln und Aufzeichnungen zur Verf�gung stellen, keinesfalls aber �ber Ursachen und Wirkungen nachdenken. William Farr meinte sogar: �Je trockener desto besser. Statistiken sollten die trockenste Lekt�re �berhaupt sein.� 
Selbstverst�ndlich wurden aber auch bereits zu dieser Zeit statistische Erkenntnisse als Grundlage f�r wichtige Entscheidungen, etwa in der �konomie oder der Gesetzgebung verwendet. Wie sehr sich das Bild der Statistik von offizieller Seite her gewandelt hat, illustriert eine Erkenntnis des Bundesverfassungsgerichts der BRD, 1983: �Die Statistik hat eine erhebliche Bedeutung f�r eine staatliche Politik. Wenn die �konomische und soziale Entwicklung nicht als unab�nderliches Schicksal hingenommen, sondern als permanente Aufgabe verstanden werden soll, bedarf es einer umfassenden, kontinuierlichen sowie laufend aktualisierten Information �ber die wirtschaftlichen, �kologischen und sozialen Zusammenh�nge. Erst die Kenntnis der relevanten Daten und die M�glichkeit, die durch sie vermittelten Informationen f�r die Statistik zu nutzen, schafft die f�r eine am Sozialstaatsprinzip orientierte Politik unentbehrliche Handlungsgrundlage�.
1.4 Definition heute
Wie wird Statistik heute definiert? In verschiedenen Lehrb�chern findet sich eine F�lle unterschiedlicher Ans�tze.
Eine Beispieldefinition: Statistik ist eine wissenschaftliche Disziplin, deren Gegenstand die Entwicklung und Anwendung formaler Methoden zur Gewinnung, Beschreibung und Analyse sowie zur Beurteilung quantitativer Beobachtungen (Daten) ist.
Statistik wird oft als Hilfswissenschaft aufgefa�t. Sie ist eine der Methoden, mit der die Theorie und Erfahrung (Empirie) systematisch reflektiert werden. Die Einsatzm�glichkeit der statistischen Methoden reicht demnach von Naturwissenschaften wie Physik Astronomie, Biologie, Umwelttechnik bis zu den Gesellschafts- und Geisteswissenschaften wie VWL, BWL, Linguistik, Geschichte usw.
2.1 Deskriptive Statistik, Datenanalyse
Durch die Verfahren der deskriptiven (�beschreibenden�) Statistik lassen sich Daten �bersichtlich darstellen, zusammenfassen und auswerten. Eine deskriptive Auswertung sollte am Anfang jeder statistischen Analyse stehen, um die Daten kennenzulernen und eventuell vorhandene Fehler zu entdecken und zu korrigieren. 
Gegenstand der deskriptiven Statistik sind die Einheiten einer Grundgesamtheit, ihre Eigenschaften und die Art ihrer Merkmale, die H�ufigkeit der Merkmalsauspr�gungen und die Abh�ngigkeit zwischen den Merkmalen.
Beginnt man mit der statistischen Auswertung, k�nnen Lage- und Skalenparameter�erste wichtige Informationen liefern. Durch Visualisierung der Daten�mit digitaler Analysetechnik l�sst sich das Verst�ndnis der Daten verbessern. 
Daten kann man durch Befragungen, Beobachtungen oder Messungen gewinnen. Es gibt Untersuchungseinheiten und Merkmale. In einer Tabelle werden die Untersuchungseinheiten oft als F�lle waagerecht in Zeilen und die Merkmale als Variablen in Spalten angeordnet. Definition Tabelle von 1867: (v. Lat.) �bersichtlich, gew�hnlich nach einzelnen Gesichtspunkten in Rubriken geordnete Zusammenstellungen des Gesamtinhalts irgend eines Lehrstoffes, gewisserma�en das Gerippe, an welches sich eine ausf�hrliche Darstellung des Gegenstandes anschlie�t. In der neueren Zeit spielen die statistischen Tabellen eine gro�e Rolle, indem die ganze Organisation unseres modernen Verwaltungssystems haupts�chlich auf die Anfertigung von Tabellen gegr�ndet ist, woraus die Zust�nde der Staatsangeh�rigen im Allgemeinen und insbesondere die Stufe ihres Wohlstandes zur Kenntnis der h�chsten Beh�rden gebracht werden sollen (Meyers, 1867).
Es gibt quantitative und qualitative Merkmale. Quantitative Merkmale unterscheiden sich durch ihre Gr��e, die Auspr�gung qualitativer Merkmale durch ihre Art.
Man unterscheidet zwischen Totalerhebungen und Stichproben. Bei Stichpropenerhebungen wird nur ein Teil der Grundgesamtheit untersucht. Stichproben werden meist mit dem Anspruch ausgew�hlt, die Gesamtheit zu repr�sentieren.
Die Auspr�gungen eines Merkmals werden durch Skalen beschrieben. Skalen haben ein unterschiedliches Niveau.
Die Datentypen der Variablen in einer Tabelle sollten entsprechend der Merkmalsskalen gew�hlt werden.
2.2 Charakterisierung von Merkmalen
Betrachtet man z.B. bei einer Gruppe von Kindern die Merkmale Name, Geschlecht und K�rpergr��e, sind Name und Geschlecht nominal skaliert. Wenn sich die Kinder nach Geschlecht getrennt der Gr��e aufstellen, l�sst sich eine ordinal skalierte Reihung der Kinder nach K�rpergr��e ermitteln. 
Wenn man zus�tzlich durch Verwiegung das K�rpergewicht als Messgr��e ermittelt, handelt es sich um eine metrische Skala.
Beispiele f�r Skalen

Nominalskala
Name, Geschlecht, Beruf etc.
Sortieren nach Alphabet m�glich
Modus, h�ufigster Wert, Anzahl
Ordinalskala

Testantwort auf einer Skala 
gut�mittel�schlecht
Rangfolge, Abst�nde nicht interpretierbar
Metrische Skala
Lebensdauer, Gewicht, Gr��e etc
Basiert auf Meter, Liter, physikalische Merkmale, statistische Kennwerte

Es gibt diskrete und stetige Merkmale. Geschlecht ist diskret, Temperatur und Zeit sind stetig.
2.3 H�ufigkeits- und Kontingenztabellen
H�ufigkeitstabellen
Das einfachste Mittel zur Darstellung der Verteilung von Auspr�gungen eines Merkmals ist die H�ufigkeitstabelle. Das Aufstellen einer H�ufigkeitstabelle ist insbesondere f�r diskrete Merkmale mit einer �berschaubaren Anzahl an Auspr�gungen sinnvoll. Das Beispiel zeigt eine H�ufigkeitstabelle f�r eine fiktive Umfrage zur Zufriedenheit mit einem bestimmten Produkt 
(1 bedeutet sehr unzufrieden, 5 sehr zufrieden):
Merkmalsauspr�gung
     Produkt
absolute H�ufigkeit
relative H�ufigkeit
kumulierte relative H�ufigkeit
1
1
0,05
0,05
2
2
0,10
0,15
3
4
0,20
0,35
4
8
0,40
0,75
5
5
0,25
1,00
Gesamt
20
1,00
1,00
Die erste Spalte der Tabelle enth�lt die Auspr�gungen, die zweite Spalte (�absolute H�ufigkeit�) z�hlt, wie oft die jeweilige Auspr�gung im Datensatz vorkommt. Die �relative H�ufigkeit� gibt den Anteil des Vorkommens der Auspr�gung an. Dieser berechnet sich aus der absoluten H�ufigkeit dividiert durch die Gesamtanzahl der Untersuchungseinheiten. Die Anzahl der Untersuchungseinheiten wird bei Teilerhebungen auch als Stichprobenumfang (Symbol: n) bezeichnet und entspricht der Summe der absoluten H�ufigkeiten. Die Summe der relativen H�ufigkeiten ergibt 1 (100%). Die letzte Spalte enth�lt die sog. �kumulierte relative H�ufigkeit�, also die Summe der relativen H�ufigkeiten bis zur aktuellen Zeile. Diese Spalte l�sst sich nur bei Variablen mit ordinalem oder metrischem Messniveau sinnvoll interpretieren.
Beispiele:
� absolute H�ufigkeit der Zufriedenheit = 5: 5 (f�nf Personen sind mit dem Produkt sehr zufrieden)
� relative H�ufigkeit der Zufriedenheit = 2: 0,1 (10% = 1/10 der Personen sind mit dem Produkt tendenziell unzufrieden)
� kumulierte relative H�ufigkeit der Zufriedenheit = 2: 0,15 (15% der befragten Personen sind mit dem Produkt unzufrieden oder tendenziell unzufrieden)
Auswertung der Tabelle mit R:
> c(1,2,4,8,5)/20               # relative H�ufigkeit, R-Funktion c
[1] 0.05 0.10 0.20 0.40 0.25    	 combine oder catenate
       
> (e<-c(1,2,4,8,5))/sum(e)          # andere Variante mit Zuweisung
[1] 0.05 0.10 0.20 0.40 0.25               Pfeil nach links
       
> cumsum(c(1,2,4,8,5)/20)            # kumulierte rel. H�ufigkeit
[1] 0.05 0.15 0.35 0.75 1.00

> matrix(c((1:5),d<-c(1,2,4,8,5),e<-d/sum(d),cumsum(e)),ncol=4)
     [,1] [,2] [,3] [,4]               # kleine Auswertung mit R
[1,]    1    1 0.05    1
[2,]    2    2 0.10    3
[3,]    3    4 0.20    7
[4,]    4    8 0.40   15
[5,]    5    5 0.25   20
Bei Variablen mit sehr vielen Auspr�gungen (oft bei metrischem Messniveau) werden H�ufigkeitstabellen zu un�bersichtlich, weil sie f�r jeden Datensatz eine Zeile ben�tigen. Abhilfe schafft hier die Klassierung des Merkmals. Statt der einzelnen Auspr�gungen werden nun die Klassen verwendet, f�r die dann absolute, relative und kumulierte relative H�ufigkeit in einer H�ufigkeitstabelle zusammengef�hrt werden. Eine einfache Klassifizierung von metrischen Daten mit Komma (real) kann man erreichen, indem man sie aufrundet.
Beispiel mit R-Funktion ceiling:
> (s<-sample(10,30, repl=T)/2)           # Zufallsstichprobe
 [1] 5.0 3.5 2.5 4.0 1.5 4.0 2.5 1.5 4.5 1.5 0.5 3.0 1.0 4.5 5.0 4.5 1.5 5.0 2.0 1.5 2.0 4.0
[23] 5.0 5.0 3.5 1.0 3.5 2.5 2.5 3.5

> head(s)                                 # die ersten 5 Elemente
[1] 5.0 3.5 2.5 4.0 1.5 4.0

> mat<-rbind(s,ceiling(s))[,1:10]             # rowbind, Matrix
  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
s    5  3.5  2.5    4  1.5    4  2.5  1.5  4.5   1.5
     5  4.0  3.0    4  2.0    4  3.0  2.0  5.0   2.0    # gerundet
> is.matrix(mat)   ; dim(mat)      # dim, Zeilen, Spalten der Matrix
[1] TRUE
[1]  2 10
2.4 Kontingenztabellen, Kreuztabellen
Die H�ufigkeitstabelle stellt die Verteilung eines Merkmals dar. Nicht selten liegt das Interesse aber dar�ber hinaus bei der Suche nach Zusammenh�ngen zwischen zwei oder mehr Merkmalen. F�r diesen Zweck eignen sich Kontingenztabellen (auch �Kreuztabellen� oder �Pivot Tabelle� genannt), die simultan die Verteilung von zwei oder auch mehr Merkmalen darstellen. Die Kontingenztabelle enth�lt in der Regel absolute oder relative H�ufigkeiten. In bestimmten Situationen hilfreich sind auch Kontingenztabellen mit sog. bedingten relativen H�ufigkeiten in den Zeilen oder Spalten.�In Abh�ngigkeit von der Zahl der Merkmale werden die Kreuztabellen ein- bis mehrdimensional.
H�ufigkeitstabellen aus dem Beispiel:
> table(s)                # Anzahl mit table, one-way, nur 1 Merkmal
s
0.5   1 1.5   2 2.5   3 3.5   4 4.5   5      # level 0.5, 1, 1,5 etc
  1   2   5   2   4   1   4   3   3   5 
> table(ceiling(s))               # table, Anzahl pro level, Klasse
1 2 3 4 5                               # help(table)
3 7 5 7 8
2.5 Statistische Lage- und Skalenparameter 
Lage- und Skalenparameter sind bei der ersten Analyse von Daten �u�erst n�tzlich, da viele Dateninformationen auf einen aussagekr�ftigen Wert verdichtet werden k�nnen.
Modus
Als Lageparameter f�r nominal-skalierte Variablen wird in der Regel der Modus empfohlen, denn sowohl arithmetisches Mittel, als auch Median k�nnen nicht verwendet werden. Der Modus entspricht immer der Merkmalsauspr�gung, die am h�ufigsten realisiert wurde. Bei einer Umfrage nach den Geburtsmonaten in einer Klasse beispielsweise, stellt der Monat den Modus dar, in dem die meisten Sch�ler Geburtstag haben.
Hat das untersuchte Merkmal mindestens ein ordinales Skalenniveau, kann die zugeh�rige H�ufigkeitsverteilung auch bi- oder sogar multimodal sein. In diesem Fall ist der Modus nicht mehr eindeutig.
Verwendet man klassierte Daten, gibt man die modale Klasse an und wei�t als Modus die Klassenmitte der modalen Klasse aus.
R-Beispiel, nominale Skala, 

> names<- c("Max", "Lise", "Lilo", "Evi", "Olga", "Moritz", "Max", "Jona", "Evi")

> length(names)                # wieviele Namen? L�nge des Vektors
[1] 9
> names.tab<- table(names) ; names.tab     # funktion table, Anzahl
names
   Evi   Jona   Lilo   Lise    Max Moritz   Olga 
     2      1      1      1      2      1      1 
# Eine Modus-Funktion ist nicht im R_System enthalten
Eigene Modus Funktion definieren (das ist noch kein Program):
> modus<-names.tab[max(names.tab)==names.tab] ; modus
names
Evi Max 
  2   2 

Unser erstes R-programm (Funktion):  # Ein R-programm hei�t Funktion
> fix(modus)
> modus
function(dat) {
tab<-table(dat)                  # Anzahl mit table
res<-tab[max(tab)==tab]          # Wo sind die Maxima von dat?
return(res)  }

Test von modus:
> modus(names)
dat
Evi Max 
  2   2
> modus(rep(names,3))            # recplicate, help(rep)
dat
Evi Max 
  6   6
Arithmetisches Mittel
Das arithmetische Mittel wird umgangssprachlich als Durchschnitt bezeichnet. Es berechnet sich aus der Summe aller beobachteten Merkmalsauspr�gungen x1, x2, � ,xn dividiert durch den Stichprobenumfang n:
                      X_arithm = 1/n � xi
Somit stellt das arithmetische Mittel einen Spezialfall gewichteter Mittel dar, da jede Merkmalsauspr�gung mit der Konstanten 1/n gleichgewichtet wird. Vor-/Nachteile des arithmetischen Mittels:
+ bekanntester Lageparameter
+ leicht und anschaulich zu interpretieren
- wird stark durch extreme Werte in den Daten beeinflusst, z.B. Milliard�re

- nur f�r Merkmale mit metrischem Messniveau geeignet (die Abst�nde zwischen den Werten m�ssen interpretierbar sein)

Median
Der Median wird in statistischen Untersuchungen immer h�ufiger anstelle des arithmetischen Mittels oder erg�nzend zu diesem ausgewiesen. Anschaulich l�sst sich der Median als der Wert �in der Mitte� der Daten interpretieren. Bei geradem Stichprobenumfang ist der Median als die Mitte zwischen den beiden mittleren Werten definiert. Es seien x(1), x(2), . . . , x(n) die Beobachtungen aufsteigend nach ihrer Gr��e geordnet. x(1) ist also die kleinste Beobachtung und x(n) ist die gr��te Beobachtung. Dann ist der Median definiert als:
     X_median = x(n+1)/2                       wenn n ungerade
     X_median = 1/2 (xn/2 + x(n/2)+1)   wenn n gerade
Beispiel: Es sei eine Stichprobe vom Umfang n = 8 mit folgenden Werten gegeben: 

> s<-c(10,3,7,51,5,13,14,8)                   # Definition

> sort(s)                                     # Sortieren nach Gr��e
[1]  3  5  7  8 10 13 14 51
Der Stichprobenumfang ist gerade, so berechnet sich der Median als x=1/2(x4+x5)=9. Im Vergleich dazu ergibt sich f�r das Arithmetische Mittel ein Wert von mean(s) = 13.875. �
Der Unterschied zwischen Median und dem arithetischen Mittel wird im Beispiel vor allem durch die in Relation zu den restlichen Daten gro�e Beobachtung �51� verursacht, die den Mittelwert stark, den Median aber im Gegensatz dazu nicht beeinflusst.
Vor-/Nachteile des Medians:
+ unempfindlich gegen�ber extremen Werten (�robust�)
+ bei ordinalem oder metrischem Messniveau sinnvoll interpretierbar
- nutzt bei metrischem Messniveau nicht alle verf�gbaren Informationen
Ausrei�er und Extremwerte
Praktiker bezeichnen h�ufig Werte, die weit von der Masse der Daten entfernt liegen als Ausrei�er. Dies ist streng genommen nicht immer korrekt. Es wird nach Extremwerten und Ausrei�ern unterschieden. Ausrei�er sind Werte, die ung�ltig sind (z.B. durch Fehler bei der �bertragung der Daten oder durch bewusste Falschangaben zustande gekommen sind), w�hrend es sich bei Extremwerten um Werte handelt, die (weit) vom Zentrum der Daten entfernt liegen, die jedoch g�ltig sind (z.B. die Einkommen von Milliard�ren in der Einkommensstatistik). 
Varianz und Standardabweichung
Arithmetisches Mittel und Median beschreiben die Lage der Daten. Erg�nzend dazu sind Informationen �ber die Streuung der Daten von Interesse. Die Daten streuen wenig, wenn sich alle Beobachtungen auf einen relativ kleinen Bereich um den Mittelwert konzentrieren. Ist das Gegenteil der Fall, spricht man von starker Streuung. Da die Erfassung der Ausbreitung eines Datensatzes auf Abstandsmessungen basiert, k�nnen Streuungsparameter nur f�r metrisch skalierte Variablen berechnet werden. Als Standardma� f�r die Streuung hat sich die �Varianz� etabliert. Mit der Varianz berechnet man die Streuung als den mittleren quadrierten Abstand der Beobachtungen zum Zentrum der Daten (gemessen �ber das arithmetische Mittel):
                 s2=1/n�(xi?x�)2
Der Wert der Varianz l�sst sich �ber die Definition hinaus nicht unmittelbar intuitiv interpretieren. Dagegen ist die Varianz gut geeignet, um die Streuung unterschiedlicher Merkmale miteinander zu vergleichen.
G�ngige Statistikprogramm-Pakete verwenden in der Formel f�r�die Varianz statt 1/n meist den Term 1/n?1 (sogenannte korrigierte Stichprobenvarianz). Der Unterschied zwischen beiden Varianten ist bei gro�en Stichprobenumf�ngen vernachl�ssigbar.
Arbeitet man mit klassierten Daten, verwendet man eine Varianzformel bei der die einzelnen Beobachtungen xi durch die jeweiligen Klassenmitten mi ersetzt werden.
Vor-/Nachteile der Varianz:
+ bekanntestes Streuungsma�
- wird stark durch extreme Werte in den Daten beeinflusst
- nur f�r Merkmale mit metrischem Messniveau geeignet (die Abst�nde zwischen den Werten m�ssen interpretierbar sein)
- hat eine andere Einheit als die Daten, weswegen die Varianz einer direkten Interpretation des Wertes nicht zug�nglich ist
Der letzte Kritikpunkt l�sst sich durch eine Modifikation beheben: Wenn das Merkmal z.B. das in��� gemessene Einkommen ist, hat die Varianz die kaum interpretierbare Einheit �2. Die quadratwurzel aus der Varianz, die sog. �Standardabweichung� hat dann wieder dieselbe Einheit wie die Daten und ist damit besser interpretierbar.
Quantile
Quantile dienen der genaueren Beschreibung von Lage und Streuung der Daten. Ein Quantil xp ist definiert als die kleinste Auspr�gung x mit der Eigenschaft, dass mindestens p% der Beobachtungen h�chstens so gro� sind wie x. Quantile lassen sich teilweise sehr einfach aus der H�ufigkeitstabelle ablesen. Als Beispiel wird auf die �H�ufigkeitstabelle oben in diesem Kapitel verwiesen.
Ein Quantil�xp l�sst sich ablesen, in dem man die Spalte der kumulierten relativen H�ufigkeiten von oben nach unten durchgeht. Das Quantil entspricht der Merkmalsauspr�gung in der Zeile, in der die kumulierte relative H�ufigkeit erstmalig gr��er oder gleich p ist. Als Beispiel sollen das 5%-, 25%- und das 75%-Quantil bestimmt werden:
a) x0,05 = 1: 5% der befragten Kunden sind sehr unzufrieden mit dem Produkt
b) x0,25 = 3: 25% der Kunden bewerten das Produkt bestenfalls neutral
c) x0,75  = 4: 75% der Kunden waren mit ihrem Besuch bestenfalls eher zufrieden
In den F�llen a) und c) aus dem Beispiel stimmt der Wert der kumulierten relativen H�ufigkeit exakt mit p �berein. Im Fall b) wird mit x = 3 die kleinste Auspr�gung verwendet, f�r die die kumulierte relative H�ufigkeit gr��er als p ist. Viele Statistikprogramme �interpolieren� in dieser Situation und geben einen Wert aus, der � abh�ngig vom verwendeten Verfahren � zwischen 2 und 3 liegt. Im Beispiel wurde ein kleiner Datensatz verwendet. Quantile sind speziell bei gr��eren Datens�tzen geeignet, um einen Eindruck �ber die Verteilung der Daten zu vermitteln. So werden z.B. bei der Verteilung des Einkommens h�ufig die 1%-, 5%-, 10%-, 25%-, 50%-, 75%-, 90%-, 95%-, 99%- und 99.9%-Quantile angegeben. Die �unteren� Quantile werden zur Definition einer relativen Armutsschranke genutzt. Im Bezug auf Einkommensdaten werden oft die Lorenzkurve und der Gini-Koeffizient verwendet, um sich ein Bild der Verteilung des Merkmals zu verschaffen. Folgende spezielle Quantile werden in der Praxis h�ufig verwendet:
� Dezile: x 0,1, x0,2, . . . , x0,9
� Quartile: x0,25, x0,5, x0,75 (Viertelung des Datensatzes)
� Median: x0,5 (Halbierung des Datensatzes)
Der Begriff �Median� ist in der Praxis sowohl f�r das 50%-Quantil als auch f�r den Median im Sinne der Definition aus vorherigem Abschnitt Median gebr�uchlich. In kleinen Stichproben k�nnen sich die Werte des Quantils und des Lageparameters jedoch durchaus unterscheiden. Mit wachsendem Stichprobenumfang wird ein ggf. existierender Unterschied jedoch immer kleiner und verschwindet asymptotisch, wenn der Stichprobenumfang gegen unendlich geht.

Der Interquartilsabstand IQR
Ein weiteres Streuungsma� ist der Interquartilsabstand. Der Interquartilsabstand (kurz: �IQR� f�r �interquartile range�) ist der Abstand zwischen dem 25%-Quantil (auch �unteres Quartil� genannt) und dem 75%-Quantil (auch �oberes Quartil� genannt): IQR = x0,75 � x0,25
Vor-/Nachteile des Interquartilsabstands:
+ anschaulich interpretierbar als die Breite des Bereichs in dem die mittleren 50% der Beobachtungen liegen
+ relativ unempfindlich gegen�ber extremen Werten (�robust�)
- bei ordinalem oder metrischem Messniveau sinnvoll interpretierbar
- nutzt bei metrischem Messniveau nicht alle verf�gbaren Informationen
Der Interquartilsabstand spielt eine wichtige Rolle in der grafischen Auswertung von Daten insbesondere bei den sogenannten Boxplots.
2.6 Zusammenhangsma�e
Kovarianz
Neben Lage- und Streuungsparamtern einzelner Merkmale sind Ma�e f�r den Zusammenhang mehrerer Variablen von Interesse. Um zu �berpr�fen ob zwischen zwei Variablen x und y ein Zusammenhang besteht, kann als erstes Ma� die Kovarianz verwendet werden. Die Kovarianz stellt eine Verallgemeinerung der Varianz dar, da die Varianz eines Merkmals auch als Kovarianz des Merkmals mit sich selbst ausgedr�ckt werden kann. Auf Grund dieses Zusammenhangs teilt die Kovarianz auch die Eigenschaft der Varianz, dass der berechnete Wert nicht direkt in der H�he interpretiert werden kann. Ein negativer Wert entspricht einem negativen Zusammenhang (kleine Werte von x gehen mit gro�en Werten von y einher und andersherum), ein positiver Wert deutet darauf hin, dass gro�e Werte von x mit gro�en Werten von y einhergehen.�
                 sxy=1/n �(xi?x�)(yi?y�)
Zu beachten ist, dass die Kovarianz skalenabh�nig ist.
Korrelationskoeffizient nach Bravais-Pearson 
Der Korrelationskoeffizient nach Bravais-Pearson wird verwendet, um die St�rke eines linearen Zusammenhangs zweier metrisch skalierter Variablen zu quantifizieren. Nur die Kovarianz der Merkmale zu betrachten reicht nicht aus, da diese skalenabh�ngig ist und somit durch Lineartransformationen beliebig verg��ert oder verkleinert werden kann. Um diese Eigenschaft zu beseitigen, wird die Kovarianz auf die jeweiligen Standardabweichungen der Variablen bezogen.

Die Ma�zahl (geschrieben rx,y ) misst den linearen Zusammenhang zwischen zwei Merkmalen x und y und kann Werte zwischen -1 und +1 annehmen. Bei rx,y = 1 l�sst sich die Beziehung zwischen x und y in der Form y = a + bx mit b > 0 angeben. Bei rx,y� = ?1 gilt analog y = a + bx mit b < 0. Bei r = 0 existiert keine Beziehung der Form y = a + bx zwischen x und y. Dies bedeutet allerdings nur, dass kein linearer Zusammenhang besteht. Ein intuitives Beispiel f�r einen Korrelationskoeffizienten von Null bei perfekter nichtlinearer Korrelation stellt die Funktion y = f(x) = x2 dar. Jeder Wert von y ist durch die Funktion bei gegebenem x-Wert eindeutig bestimmt. Trotzdem spiegelt sich das im Ma� des linearen Zusammenhangs nicht wider.
Zur Veranschaulichung des Konzepts werden Umfagedaten des ALLBUS Datensatzes herangezogen. 
Der Datensatz Umfragedaten der FU enth�lt 17 Variablen von 3471 Befragten. Dabei handelt es sich um soziodemographische Variablen wie das Geburtsjahr, Geschlecht oder das Netto-Einkommen. Diese 17 Variablen sind eine Auswahl aus der allgemeinen Bev�lkerungsumfrage der Sozialwissenschaften 2014 (ALLBUS 2014). Je nach dem, mit welchem Programm gearbeitet werden soll, bieten sich unterschiedliche Dateiformate an. Der Datensatz Umfragedaten wird hier in den Dateiformaten .xlsx, .txt, .RData, .dta, .sd2 und .sav angeboten. Im Datensatz Umfragedaten sind Variablen unterschiedlicher Messniveaus zu finden. Dabei handelt es sich beispielsweise um metrische Variablen wie die Gr��e oder um kategoriale (nominale) Variablen wie das Geschlecht oder der Schulabschluss. Dabei gibt es beim Geschlecht zwei Auspr�gungen (m�nnlich/weiblich) und beim Schulabschluss sieben verschiedene Auspr�gungen. Alle Informationen �ber die Variablen, die im Datensatz enthalten sind, k�nnen einem sogenannten Codebook (siehe Downloads) entnommen werden. F�r den vorliegenden Datensatz wird dieses zusammengesetzt aus Abschnitten aus dem ALLBUS 2014 - Variable Report und eigener Beschreibungen f�r editierte Variablen. In den Beschreibungen werden auch die Fragestellungen aus dem zugrundeliegenden Fragebogen genannt. Dar�ber hinaus ist der Datensatz bereits bereinigt. Das hei�t in diesem Fall, dass alle Angaben wie Keine Angabe, Wei� nicht oder Trifft nicht zu als fehlend kodiert wurden. Dies erleichtert f�r den Einstieg die Analyse.
Im grafischen Analyseteil werden Streudiagramme (oder engl. Scatterplot) vorgestellt, welche ebenso wie Korrelationskoeffizienten einen Eindruck des Zusammenhangs zweier Merkmale liefern. Je st�rker sich die Punktewolke um eine aufsteigende (absteigende) Gerade gruppiert, desto n�her liegt der Wert des Koeffizienten bei +1 (-1). F�r die ersten 300 Befragten des Beispieldatensatzes ergibt sich ein Wert des Zusammenhangs von 0,55. Wie erwartet gehen mit kleinen (gro�en) Werten des Merkmals Gr��e tendenziell kleine (gro�e) Werte des Merkmals Gewicht einher.
Datenimport von Umfragedaten mit read.csv2. Der Dateipfad ist individuell verschieden einzutragen. Bei Verzeichniswechsel entweder / wie bei UNIX oder \\ bei Windows verwenden.

> dat<-read.csv2("D:\\Lehre\\Statistik3\\Umfrage.csv")
> head(dat,2)
    ID   GESCHL GEBJAHR         BERUFSTAETIG ARBEITSSTD ARZTBES RAUCH GRO GEW SCHULABSCHLUSS
1 1359 WEIBLICH    1967  NICHT ERWERBSTAETIG         NA       1    JA 162  79 MITTLERE REIFE
2 2455 WEIBLICH    1964 HAUPTBERUFL.HALBTAGS         30       1  NEIN 165  59 MITTLERE REIFE
    SCHULABSCHLUSS_V   SCHULABSCHLUSS_M HOE_ABSCHLUSS      HOE_ABSCHLUSS_V      HOE_ABSCHLUSS_M NETTO
1     MITTLERE REIFE     HOCHSCHULREIFE         LEHRE GEWERBL.,LANDW.LEHRE   MEISTER, TECHNIKER   475
2 VOLKS-,HAUPTSCHULE VOLKS-,HAUPTSCHULE         LEHRE   MEISTER, TECHNIKER KAUFMAENNISCHE LEHRE   780

> dat<-dat[1:300,]           # Auswahl der ersten 300 Datensaetze
Berechnung des Korrelationskoeffizienten zwischen Gr��e und Gewicht.
> cor(dat$GEW, dat$GRO)
[1] NA                                  # es gibt NA

> sum(!is.na(dat$GEW))                  # 6 NA F�lle bei Gewicht
[1] 294
                                        # help(is.na)
> erg<-cor(dat[!is.na(dat$GEW),c("GRO","GEW")]); erg # ohne NA F�lle
          GRO       GEW                   
GRO 1.0000000 0.5499379
GEW 0.5499379 1.0000000

> round(erg, 2)                   # mit Rundung auf 2 Stellen
     GRO  GEW
GRO 1.00 0.55
GEW 0.55 1.00

Graphische Darstellung von GEW und GRO

> plot(dat$GEW, dat$GRO, col="BLUE")              # Scatterplot
> title("Gr��e und Gewicht von 300 Befragten")    # mit Titel

Rangkorrelationskoeffizient nach Spearman
Sofern die Variablen x und y zumindest ordinales Messniveau haben, l�sst sich der Korrelationskoeffizient nach Spearman (auch �Rangkorrelationskoeffizient� genannt) berechnen.
Im Vergleich zum Korrelationskoeffizienten von Pearson ist der Rangkorrelationskoeffizient wesentlich weniger�von einzelnen Ausrei�ern beinflusst. Gemessen wird der monotone Zusammenhang zweier Variablen. Der Koeffzient nimmt tendenziell hohe Werte an, wenn mit hohen Werten der einen auch hohe Werte der anderen Variable einhergehen.
Die Berechnung dieses Ma�es beruht im Gegensatz Pearsons Ma� nicht auf den Werten der Beobachtungen selbst, sondern auf ihren R�ngen. Bei dieser Methode erh�lt jede Auspr�gung einen Rang gem�� ihrer Gr��e zugewiesen, d.h. der kleinste Wert erh�lt jeweils den Rang eins, der zweitkleinste den Rang zwei usw.
Treten gleiche Werte auf (sogenannte Bindungen), werden mittlere R�nge vergeben. Beispielsweise erh�lt man f�r die beobachteten Werte von x folgende R�nge:�
�
x1
x2
x3
x4
x5
x6
x7
x8
x9
x10
Auspr�gung
7
3,2
8
24,6
12,5
7
10
2
10
26,3
Rang
3,5
2
5
9
8
3,5
6,5
1
6,5
10
Nach der Rangbestimmung f�r beide Merkmale kann anhand folgender Formel der Korrelationskoeffizient bestimmt werden:

Sollten alle x-Werte und y-Werte in sich unterschiedlich sein, d.h. keine Bindungen auftreten, vereinfacht sich die Berechnung� folgenderma�en:
     rs=1?6�di2 / n?(n2�1),
wobei di die jeweilige Differenz zwischen den R�ngen von x und y einer Beobachtung angibt.
2.7 Fragen und Stundenaufgaben
1. Warum hat man statistische Methoden entwickelt? 2 Gr�nde.
2. Nennen Sie eine Definition von Statistik
3. Welche Begriffe sind f�r die deskriptive Statistik von Bedeutung und erkl�ren Sie diese.
4. Beipiele f�r Skalen
5. Erkl�ren Sie die Begriffe H�ufigkeits- und Kontingenztabelle, Unterschiede?
6. Was ist eine Matrix
7. Welche Lageparamter gibt es bei nominalen Daten
8. Welche Auswertungsm�glichkeiten gibt es bei nominalen Daten
9. Vorteil Median gen�ber Mittelwert, praktisches Beispiel
10. Beschreiben sie die statistische Kenngr��e Streuung verbal
11. Erkl�ren Sie den IQR
12. Erkl�ren Sie den Unterschied zwischen Kovarianz und Korrelationskoffizienten�
3.1 Technische Grundlagen des R-Systems
Internet Download (frei) bei:

http://cran.r-project.org/

Lokale Installation von R und R-Studio je nach Plattform unter Windows, Linux oder Mac: 
Verkn�pfung von 
	C:\Programme\R\R-2.4.1\bin\Rgui.exe auf der Arbeitsoberfl�che.

Im Zedat-Rechnernetz der FU sind R und R-Studio schon instaluliert. F�r R-Studio sind die Rechneranforderungen h�her. R-Studio ist komfortabler. F�r Animationen lieber R-Studio nicht nehmen. Je nach Bedarf werden f�r bestimmte Zwecke einzelne R-Pakete nachinstalliert. In R-Paketen sind zus�tzliche Funktionen f�r R enthalten.

Kurzlehrgang im Internet bei:

http://www.barghoorn.com/pages/aplsplus.html

Mit File/Save Image.... speichert man den sogenannten workspace im eigenen Bereich lokal oder auf einem Datentr�ger. Nach dem Start von R: File/Load Image..... sind alle Programme und Variablen wieder im workspace vorhanden.
 
> load("Z:/Dat_R/eins.RData") 
R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.1  (2004-06-21), ISBN 3-900051-00-3

R is a collaborative project with many contributors.
Type 'contributors()' for more information and 'citation()' on how to cite R in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for a HTML browser interface to help. Type 'q()' to quit R.
Eine Auflistung der Namen der aktuell im workspace vorhandenen Objekte erh�lt man durch den Befehl
> objects()
Dasselbe erreicht man mit dem k�rzeren Aufruf
> ls()

Die Objekte k�nnen haupts�chlich sein: Daten (Variable) und Programme (eigene Funktionen).
Zu�tzliche Informationen �ber die Struktur der Workspace-Objekte bekommt man mit
>ls.str()

Weiterhin lassen sich die session (Abfolge der R-Befehle) und ein Protokoll (Eingabe Befehl + Ausgabe des Systems) speichern.
Um sich das Laden und Speichern von Dateien zu erleichtern, sollte man ein Verzeichnis, die working directory (Arbeitsverzeichnis) wd f�r R definieren. Diese wd kann man auch abfragen:

> getwd()
[1] "C:/Users/barg/Documents"
Den Arbeitspfad, das Arbeitsverzeichnis f�r R kann man auch im Men� oder durch Kommandos festlegen und abfragen mit:

setwd ("d:\\Lehre" oder setwd("d:/lehre") und getwd()
Man beachte den Schr�gstrich, entweder doppelbackslash \\ oder einfach slash / beim Verzeichniswechsel.
3.2 Mathematische Operatoren und ihre Symbole im R-System

 Operator 	Operation 	Beispiel	 Output
+ 			Addition 		1+3 		[1] 4
- 			Subtraktion  	3-1 		[1] 2
* 			Multiplikation 2*5 		[1] 10
/ 			Division 	 	10/2 	[1] 5
� 			Potenz 		3�3 		[1] 27
** 			Potenz 		3**3		[1] 27
3.3 Logische Operatoren
 Symbol 	Bedeutung 	Beispiel	Output
== 		Gleich 		1==2 	[1] FALSE
!= 		Ungleich 		1!=2 	[1] TRUE
< 		Kleiner als 	1<2 		[1] TRUE
> 		Gr��er als 	1>2 		[1] FALSE
<= 		Kleiner-gleich	1<=2 	[1] TRUE
>= 		Gr��er-gleich 	1>=2 	[1] FALSE
&		UND			1&1		[1] TRUE
|		ODER			1|0		[1] TRUE
!		NICHT / NON	!1		[1] FALSE

Es gibt auch viele spezielle Funktionen, mit denen man den Datentyp oder andere wichtige Eigenschaften abfragen kann. 
Beispiel: Datentyp logisch, JA oder NEIN?
> is.logical(c(TRUE,NA,FALSE,TRUE))
[1] TRUE
3.4 Wichtige elementare Datenstrukturen in R, vgl. Video
Kurze Erkl�rung von Skalar, Vektor, Matrix. Mit diesen Begriffen l�sst sich die �u�ere Struktur (Abmessung) von Datenobjekten beschreiben.

Eine skalare Variable ist im Kontext von Programmiersprachen eine Variable, die einen einzelnen Wert speichert. Der Begriff ist angelehnt an den mathematischen Skalar. Ein skalarer Datentyp ist der Datentyp einer skalaren Variable, �blicherweise ein elementarer Datentyp der verwendeten Programmiersprache. Das Gegenst�ck zu einer skalaren Variable sind etwa Arrays, Listen und Strukturen, die jeweils mehrere Werte speichern k�nnen.

Skalare werden in R als Vektoren mit nur einem Element aufgefasst und k�nnen ohne die Funktion c() einer Variable zugewiesen werden:
> skalar<-999                                     # numerisch
> length(skalar)
[1] 1

Oder
> skalar<-"333"                                   #character
> length(skalar)
[1] 1

Zum Skalar geh�rt die geniale Skalar-Extension Regel.
Zwei Beispiele:
> v<-c('eene','meene','muh')   ; length(v)   #Textvektor L�nge 3
[1] 3

Die Struktur bleibt erhalten:
> v[]<-4   ; v
[1] "4" "4" "4"

Erweiterung
> rbind(v,skalar)                # immer die richtige L�nge
       [,1]  [,2]  [,3] 
v      "4"   "4"   "4"  
skalar "333" "333" "333"

> cbind(rbind(v,skalar),skalar)  # in jeder Richtung, cbind - column
                         skalar
v      "4"   "4"   "4"   "333" 
skalar "333" "333" "333" "333"

Vektoren:
vector produces a vector of the given length and mode
> vek<-1:10                            # Index generieren
> length(vek)
[1] 10

Oder mit rep(1:10, 1)                   # replicate, vgl. Seite 8
 [1]  1  2  3  4  5  6  7  8  9 10
Oder > vek<-rep(-3:3, 2) ; vek             
 [1] -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3

Matrix:
matrix creates a matrix from the given set of values
> mat<-matrix(1:12,ncol=4)                      # 4 Spalten
> mat
     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12

> dim(mat)                                        # Dimension mat
[1] 3 4
Mehrere Variante f�r Definition von mat mit Zuweisung von dim()
> mat<-matrix(1:12)      # Index von 1 bis 12 als einspaltige Matrix
> dim(mat)<-3:4
> mat
     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12

> mat<-matrix(1:12, dim<-3:4) ; mat         # in einem Befehl
     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12

> mat<-matrix(1:12, dim<-3:4, byrow=T) ; mat   # zeilenweise
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
[3,]    9   10   11   12

mat<-matrix(seq(1,12), dim<-3:4, byrow=T) ; mat     # Zahlenfolge mit seq
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
[3,]    9   10   11   12


> mat<-matrix(1:12, dim<-3:4, dimnames=list(NULL, c("IMI", "ATA", "PERSIL", "FEWA")))                  # mit Spaltenlabels
> mat
     IMI ATA PERSIL FEWA
[1,]   1   4      7   10
[2,]   2   5      8   11
[3,]   3   6      9   12

> mat[,3]                     # Indizierung nur 3. Spalte
[1] 7 8 9
> mat[,'PERSIL']                  # Spalte PERSIL
[1] 7 8 9

> mat[3,'PERSIL']                 # Spalte PERSIL, 3. Zeile
PERSIL 
     9
3.5 Die R-Import- und Exportprogramme f�r Daten (Tabellen)
Wie und wo bekommt man bei R Hilfe? Z.B. Datenimport??
help(read.table) oder  ??read
vorher 
help.start() f�r html-Hilfe, au�erdem Internet

read.table(file, header = FALSE, sep = "", quote = "\"'",
           dec = ".", row.names, col.names,
           as.is = !stringsAsFactors,
           na.strings = "NA", colClasses = NA, nrows = -1,
           skip = 0, check.names = TRUE, fill = !blank.lines.skip,
           strip.white = FALSE, blank.lines.skip = TRUE,
           comment.char = "#",
           allowEscapes = FALSE, flush = FALSE,
           stringsAsFactors = default.stringsAsFactors(),
           fileEncoding = "", encoding = "unknown")

read.csv(file, header = TRUE, sep = ",", quote="\"", dec=".",
         fill = TRUE, comment.char="", ...)

read.csv2(file, header = TRUE, sep = ";", quote="\"", dec=",",
          fill = TRUE, comment.char="", ...)

read.delim(file, header = TRUE, sep = "\t", quote="\"", dec=".",
           fill = TRUE, comment.char="", ...)

read.delim2(file, header = TRUE, sep = "\t", quote="\""dec=",",
            fill = TRUE, comment.char="", ...)

Beispiel: Import von BSR-Daten aus dem Jahr 1994
> dat<-read.table("D:/Lehre/Statistik3/bsrorg.txt", header=T)
Andere Variante mit backslash
> dat<-read.table("D:\\Lehre\\Statistik3\\bsrorg.txt", header=T)
Es m�ssen Doppelbackslash sein!

> dim(dat)
[1] 15580     7
> mode(dat)
[1] "list"
> class(dat)
[1] "data.frame"
> head(dat,3)
   tag zeit   Mg    kfznr bsrkey  Laga P
1 2609  514 0.76 B-EE2388    UNB 91200 A
2 2609  536 6.38 B-CX2104    UNB 91200 A
3 2609  542 2.28  B-CR332    UNB 91200 A

Analog gibt es diverse Export-Programme.

Stunden-Aufgaben:     # Stunden-Aufgaben sind f�r eigene �bung
* BSR-Daten nach R importieren
* Was ist dat f�r ein Objekt?
* Welcher Datentyp kommt vor in der importierten Tabelle
* Export der Daten als ASCII und CSV-Datei
* Berechnen von statistischen Kenngr��en f�r die Spalten von dat
3.6 Programme sind Funktionen, R ist eine funktionale Sprache
Ein ganz kleines R-Programm (on the fly)
mittel<-function(x) sum(x)/length(x) # Mittelwert
> mittel(1:10)          # die ganzen Zahlen von 1 bis 10
[1] 5.5

Das Schl�sselwort f�r Programme ist function.
3.7 Datenauswertung: Lebendgeborene seit 1950 in Deutschland
Daten: geburten_d.txt, eine Textdatei

Die ersten 10 Zeilen

maennlich	weiblich
1950	578191	538510
1960	648928	612686
1970	537922	509815
1980	444148	421641
1990	465379	440296
2000	393323	373676
2001	377586	356889
2002	369277	349973
2003	362709	344012
2004	362017	343605

Tabulatoren Sonderzeichen, sind im Text nicht sichtbar

Start von Microsoft-Excel

In Excel neues Tabellenblatt anlegen

Excel-Import der Textdatei geburten_d.txt

3-D-Plot und 
gegebenenfalls Ascii-Export.
Stundenaufgabe (gemeinsam) die Tabelle geburten_d.txt soll aktualisiert werden mit den Daten von 2019.
1. Tabelleninhalt Kopieren vom Statistischen Bundesamt
2. Lebendgeborene nach Monaten und Geschlecht
3. Summen mit Excel berechnen, update in geburten_d.csv
4. Import nach R 




Import in den R-Workspace
> geb<-read.csv2("geburten_d.csv")

> dim(geb)                  # hier fehlen noch die Daten von 2019
[1] 24  2

> mode(geb)
[1] "list"

>attributes(geb)

$names
[1] "maennlich" "weiblich"

$class
[1] "data.frame"      maennlich weiblich

$row.names
 [1] "1950"  "1960"  "1970"  "1980"  "1990"  "2000"  "2001"  "2002"  "2003"  "2004"  "2005"  "2006"  "2007"  "2008"  "2009"  "2010"  "2011" "2012"  "2013"  "2014"  "2015"  "2016"  "2017"

> head(geb,3)
     maennlich weiblich
1950    578191   538510
1960    648928   612686
1970    537922   509815

# Auswertung, Gesamtspalte mit den R-Funktionen 
cbind 
Take a sequence of vector, matrix or data-frame arguments and combine by columns or rows
apply
Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix
sum
sum returns the sum of all the values present in its arguments

gew<-cbind(geb,apply(geb,1,sum))
> dim(gew)                                # Dimension
[1] 23  3

> head(gew,3)
     maennlich weiblich apply(geb, 1, sum)
1950    578191   538510            1116701
1960    648928   612686            1261614
1970    537922   509815            1047737

# Update Labels
colnames(gew)[3]<-"Gesamt"
Unser zweites richtiges R-Programm Tabellensumme tabs, noch ohne Kommentare
Eingabe direkt im R-Eingabefenster (Console).
tabs<-function(x) {
gew<-cbind(x,apply(x,1,sum))
colnames(gew)[4]<-"Gesamt"
gew
}

> mode(tabs)
[1] "function"
# Testen des Programs, Zuweisung nach rechts geht auch
> tabs(geb)->e1
> dim(e1)
[1] 24  3

Korrigieren und Editieren der Funktion, wenn n�tig, mit fix(tabs). Achtung: fix() speichert ohne Nachfrage.

Man beachte den Unterschied zwischen edit() und fix().

Gew�nscht wird weiterhin die Spaltensumme (colSums).

> colSums(m)
maennlich weiblich 
 3952680  3739371

Verbessertes Program tabs mit beliebiger Spaltenzahl

tabs<-function(x) {
gew<-cbind(x,apply(x,1,sum)) 
m<-dim(x)                          # dimension(x)
colnames(gew)[1+m[2]]<-"Gesamt"    # update je nach Spalten-zahl
rerurn(gew) }

Erweitertes Programm f�r Randsummen von Tabellen tabss.
Zuerst eine Kopie von tabs erzeugen. Dann editieren mit fix(), dabei Kommentar(e) hinzuf�gen.

tabss<-tabs

> tabss
function(x) {
gew<-cbind(x,apply(x,1,sum)) # Spaltensumme verketten
colnames(gew)[3]<-"Gesamt"
gew<-rbind(gew,colSums(gew))
return(gew) }

Aufruf von tabss mit der importierten Geburten-Datentabelle geb
> m<-tabss(geb)
      maennlich weiblich   Gesamt
1950     578191   538510  1116701
1960     648928   612686  1261614
1970     537922   509815  1047737
1980     444148   421641   865789
1990     465379   440296   905675
2000     393323   373676   766999
2001     377586   356889   734475
2002     369277   349973   719250
2003     362709   344012   706721
2004     362017   343605   705622
2005     351757   334038   685795
2006     345816   326908   672724
2007     351839   333023   684862
2008     349862   332652   682514
2009     341257   313885   655142
2010     347237   330710   677947
2011     339899   322786   662685
2012     345629   327915   673544
2013     349820   332249   682069
2014     366835   348092   714927
2015     378478   359097   737575
2016     405585   386546   792131
2017     402510   382374   784884
24      9216004  8721378 17937382

> (rownames(tabss(m)))[25]
[1] "25"
Labelupdate bitte noch in das Programm einzubauen. Das Programm dynamisieren. Mithilfe der funktion dim(). Wie bei tabs
Hilfestellung:
> d<-dim(m)

Spaltenanzahl, das 2. Element von dim(m). Plus 1.
> is.vector(d)                  # Datenform von m?
[1] TRUE

> attributes(d)
NULL

Die Funktionen first oder last hei�en in R head() oder tail().
Mit head() ohne weitere Angabe kann man sich die ersten 6 Elemente oder Zeilen etc. anzeigen lassen. 
3.8 Berechnung von Prozentwerten, Spaltendivision

> proz<-geb/apply(geb,1,sum)
 > dim(proz)
[1] 24  2

# mit der Funktion round() das Ergebnis noch versch�nern

> e1<-round(proz,3)
> e1
      maennlich weiblich
1950      0.518    0.482
1960      0.514    0.486
1970      0.513    0.487
1980      0.513    0.487
1990      0.514    0.486
2000      0.513    0.487
2001      0.514    0.486
2002      0.513    0.487
2003      0.513    0.487
2004      0.513    0.487
2005      0.513    0.487
2006      0.514    0.486
2007      0.514    0.486
2008      0.513    0.487
2009      0.521    0.479
2010      0.512    0.488
2011      0.513    0.487
2012      0.513    0.487
2013      0.513    0.487
2014      0.513    0.487
2015      0.513    0.487
2016      0.512    0.488
2017      0.513    0.487

Noch sch�ner mit 1 Stelle hinter dem Komma.
 > head(round(100*proz,1))
     maennlich weiblich
1950      51.8     48.2
1960      51.4     48.6
1970      51.3     48.7
1980      51.3     48.7
1990      51.4     48.6
Prozentberechnung von einer Tabelle in einem Programm.

> proztab
function(x) {
s<-apply(x,1,sum)         # Spaltensumme
p<-x/s                    # Tabelle / Spaltensumme
p<-round ((100*p),1)      # *100 gerundet auf eine Stelle
p                         # Ergebnis
}

Wie berechnet man die Prozente �ber die Zeilen? Hilfe bei R-Newsgroup. Leider nicht so einfach.

Berechnung Prozent mit der funktion t() # transpose

> proz2<-t((t1<-t(geb))/apply(t1,1,sum)) #Variable t1 gespeichert

> e2<-round(proz2,3)
> e2
> head(e2)
     maennlich weiblich
1950    0.0627   0.0617
1960    0.0704   0.0703
1970    0.0584   0.0585
1980    0.0482   0.0483
1990    0.0505   0.0505
2000    0.0427   0.0428

Dieses Verfahren ist etwas umst�ndlich (transpose), deshalb hier eine andere Variante mit Verwendung der funktion prop.table(). Achtung Datentyp!

> prop.table(geb,2)
Fehler in margin.table(x, margin) : 'x' ist kein Array

> mode(geb)
[1] "list"

> is.data.frame(geb)
[1] TRUE

> is.array(geb)
[1] FALSE

> is.matrix(geb)
[1] FALSE

Umwandeln des dataframe in eine matrix. Geht, weil alle Daten numerisch sind.

> m1<-as.matrix(geb)                       # wie sieht m1 aus?
> m1[1,]
maennlich weiblich 
  578191   538510

> m1$maennlich                           # geht nun nicht mehr
Fehler in m1$maennlich : $ operator is invalid for atomic vectors

Alternative:
> head(m1[,"weiblich"])
  1950   1960   1970   1980   1990   2000 
538510 612686 509815 421641 440296 373676

> prop.table(m1,2)
       maennlich   weiblich
1950  0.06273771 0.06174598
1960  0.07041316 0.07025105
1970  0.05836825 0.05845579
1980  0.04819312 0.04834569
1990  0.05049683 0.05048468
2000  0.04267826 0.04284598
2001  0.04097069 0.04092117
2002  0.04006910 0.04012818
2003  0.03935643 0.03944468
2004  0.03928134 0.03939802
2005  0.03816806 0.03830106
2006  0.03752342 0.03748353
2007  0.03817696 0.03818468
2008  0.03796244 0.03814214
2009  0.03702874 0.03599030
2010  0.03767761 0.03791947
2011  0.03688139 0.03701089
2012  0.03750313 0.03759899
2013  0.03795788 0.03809593
2014  0.03980413 0.03991250
2015  0.04106747 0.04117434
2016  0.04400877 0.04432167
2017  0.04367511 0.04384330

Interessante Frage: Die meisten Geburten? �hnlich wie Datenbankabfrage.
> apply(geb,2,max)
  maennlich weiblich 
  648928   612686

Aber wann? In welchem Jahr?

> geb[geb$maennlich==max(geb$maennlich),]
     maennlich weiblich
1960    648928   612686

> geb[which.max(geb$maennlich),]  # andere Variante, help(which.min)
     maennlich weiblich
1960    648928   612686
Und wann sind die wenigsten Geburten?
> geb[which.min(geb$maennlich),]         # Index Minimum
      maennlich weiblich
2011     339899   322786
Die Funktion which.
Give the TRUE indices of a logical object, allowing for array indices.
Gesamtentwicklung als Barplot
> barplot(apply(geb,1,sum))
> title("Geburtenentwicklung seit 1950 in Deutschland")

3.9 Analyse weiterer Datens�tze der amtlichen Statistik
Stundenaufgabe:
Erwerbst�tige und Bev�lkerung nach Bundesl�ndern und Jahr
Bev�lkerung nach L�ndern bis 2011, Datei erwerbstaet_land.txt
Auswertung in Excel, Entwicklung im Zeitverlauf nach Bundesland
4.0 Stichproben
Es ist die Aufgabe der Statistik, verl�ssliche Informationen (Daten) in Wirtschaft und Gesellschaft, Umwelt etc. zu erheben und zur Verf�gung zu stellen. Zust�ndig ist hierf�r im Besonderen die amtliche Statistik. Dabei ist es unm�glich, die Gesellschaft in ihrer ganzen Totalit�t vollst�ndig zu erfassen, vielmehr muss man sich darauf beschr�nken, gewisse Ausschnitte bzw. Querschnitte zu untersuchen, jeweils nur unter bestimmten, ausgew�hlten Aspekten. Die Begrenzung der Ziele ist schon aus logischen Gr�nden geboten, denn eine vollst�ndige Beschreibung von Gesellschaft ist genauso wenig m�glich, wie die l�ckenlose Erforschung eines ihrer Mitglieder in all seinen Merkmalen. �Man kann ein einzelnes Ereignis nicht in all seinen Details beschreiben, weil dies praktisch eine Beschreibung des ganzen Universums einschlie�en w�rde� (Stegm�ller, Springer 1969).
Wegen der daraus resultierenden Einengung des Forschungsbereiches ist es notwendig, den interessierenden Bereich eindeutig abzugrenzen die f�r die Problematik relevanten Merkmale genau zu definieren. In der Statistik bezeichnet man diesen Vorgang als sachliche, zeitliche und r�umliche Abgrenzung der Grundgesamtheit (Population, Universum) und Festlegung der Erhebungsmerkmale. Erhebungsmerkmale deshalb, weil die erforderlichen Daten in den meisten F�llen nicht durch blo�e Beobachtung gewonnen werden k�nnen, sondern durch Befragungen oder Messungen erhoben werden m�ssen.
4.1 Vorteile von Stichproben
Kosten
Stichprobenverfahren erlauben Kosteneinsparungen, das die Erhebungs- und Aufbereitungskosten geringen sind.
Zeit
Wegen der Tatsache, dass die sozialen Gesamtheiten und die Umwelt st�ndigen Ver�nderungen unterworfen ist und sich somit ihre Merkmale h�ufig andern k�nnen, gewinnt der Zeitfaktor bei der Erhebung aus Auswertung der Daten eine herausragende Bedeutung. Daten sind aktuell k�nnen interessant sein oder von historischem Interesse.
Genauigkeit
Bei Untersuchung einer verminderten Anzahl von Einheiten kann man bei gleichem Aufwand die Erforschung der Einheiten intensivieren, indem man z.B. zus�tzliche Merkmale einbezieht oder dieselben Merkmale mit detaillierteren Fragen und speziell geschulten Interviewern eingehender untersucht.
Praktikabilit�t
F�r bestimmte Bereiche ist eine Totalerhebung nicht praktikabel, wie bei unendlichen Gesamtheiten. Ebenso wenig w�re es vern�nftig eine Totalerhebung durchzuf�hren, wenn diese die Zerst�rung s�mtlicher Einheit der Population bedingen w�rde. Weiterhin muss man damit rechnen, dass f�r bestimmte Bereiche eine Totalerhebung derart aufw�ndig w�re, dass man auf die statistische Erfassung dieser Bereiche verzichten muss. Diese gilt beispielsweise f�r die Ernteermittlung oder die Ermittlung der Zusammensetzung von Hausm�ll.
4.2 Nachteile von Stichproben
Auswahlfehler
Die durch den Stichprobenfehler hervorgerufene Ungenauigkeit kann im Widerspruch zu den Anforderungen stehen.
Besetzung der Untergruppen
Bei Aufgliederung der Stichprobenergebnisse in verschiedene Untergruppen oder nach bestimmten einzelnen oder kombinierten Merkmalen w�chst der Grad der Unsicherheit �ber die Verl�sslichkeit der ermittelten Daten, weil dann die Untergruppen oft nur schwach besetzt sind (Stichprobenumfang).
4.3 Repr�sentativit�t von Stichproben
F�r das Kriterium der Repr�sentativit�t von Stichproben ist das angewendete Auswahlverfahren von entscheidender Bedeutung. 
Definition: Eine Stichprobe vom Umfang n soll als repr�sentativ f�r eine Grundgesamtheit mit N Elementen angesehen werden, wenn sie m�glichst hinsichtlich vieler Merkmale (eingeschlossen die Untersuchungsmerkmale) ein verkleinertes getreues Abbild der Grundgesamtheit darstellt (The ideal  sample is one which is a microcosm of the population which it puports to represent (BBC, 1970). 
Als Kriterien, an Hand derer wir die Qualit�t des verwendeten Auswahlverfahrens in Bezug auf die Repr�sentativit�t beurteilen, werden die Erwartungstreue oder Unverzerrtheit und die Varianz der Sch�tzung verwendet.
4.4 Zufallsstichproben
Simple Random Sampling ohne Zur�cklegen (Notation: SI, SRS, SRSWOR) oder mit Zur�cklegen (Notation: SIR, SRSWR)
Eigenschaften: Fester Stichprobenumfang n. Konstante Auswahlwahrscheinlichkeiten �k=nN und �k,l=n(n?1)N(N?1) (Ohne Zur�cklegen).
Populationssch�tzer:
t^y=Ny�sy�s=1n�k?syk
Varianz:
V^(t^�)=N2(1n?1N)S2y,sS2y,s=1n?1�k?s(yk?y�s)2
4.5 Sequentielle Stichprobenverfahren
F�r jede Einheit der Grundgesamtheit wird per Zufall unabh�ngig entschieden, ob die Einheit in die Stichprobe gelangt oder nicht: Bernoulliexperiement! Damit ist der Stichprobenumfang zuf�llig (? Varianzvergr��erung).
Bernoulli-Sampling: Erfolgswahrscheinlichkeit konstant �. Erwarteter Stichprobenumfang: E(n)=N�
Poisson-Sampling: Erfolgswahrscheinlichkeit �k variiert. Erwarteter Stichprobenumfang: E(n)=�k?U�k
�4.6 Probability Proportional Size
Motivation: Die Varianz des �-Sch�tzers wird sehr klein, falls �k�const?yk, da dann t^y=�k?syk�k�n?1/const.
In der Praxis wird man ein Merkmal xk w�hlen, das mit yk hoch korreliert ist und f�r alle k?U bekannt sein muss. Dies ist bei Firmendaten h�ufig der Fall. Beispiel: yk= Ausgaben F+E in Betrieb k. xk=Anzahl Beschaftigte in Betrieb k
Die Realisierung eines Ziehungsverfahrens ist beim Ziehen mit Zur�cklegen (PPS) relativ einfach: Jede Einheit k wird durch eine Strecke der L�nge xk auf einem Intervall der Gesamtl�nge L=�k?Uxk repr�sentiert. Es wird eine gleichverteilte Zufallszahl aus dem Intervall [0,L] gezogen und die Einheit gew�hlt, in deren Bereich die Zufallszahl gefallen ist.
Die Realisierung eines Ziehungsverfahrens ohne Zur�cklegen (�ps) ist schwierig und aufw�ndig.
4.7 Geschichtete Stichprobe
Das Ziehen einer geschichteten Zufallsstichprobe (auch: stratifizierte Zufallsstichprobe) kann in der Regel Vorteile bringen, wenn die Grundgesamtheit in sinnvolle Gruppen, die sogenannten Schichten, unterteilt werden kann. Man schr�nkt nun die rein zuf�llige Auswahl der Stichprobenelemente insofern ein, dass man die Stichprobenumf�nge pro Schicht vorgibt und danach in jeder Schicht eine reine Zufallsstichprobe zieht. Man "verbietet" damit extreme Stichproben, die beispielsweise zuf�llig fast nur Elemente aus einer Schicht enthalten und bekommt in der Konsequenz bessere Punktsch�tzer, d.�h. Sch�tzer mit kleinerer Varianz. In Monte-Carlo-Simulationen kann man geschichtete Zufallsziehungen als Mittel der Varianzreduktion einsetzen. Die Schichtungsmerkmale (Paradaten) m�ssen vorab bekannt sein (wiki).
Die Verteilung der Stichprobenumf�nge auf einzelne Teilgruppen (Englisch: Strata) der Grundgesamtheit. Hierzu� muss die separate Ziehung einer Stichprobe innerhalb jedes Stratums realisierbar sein. Die Strata sollen so gew�hlt werden, dass die Streuung zwischen den Strata-Mittelwerten (Between-Varianz) m�glichst gro� ist.� Die Varianz des �-Sch�tzers ist dann durch die verbliebene Within-Streuung gegeben. Verwendet man den SI-Sampling in jeder Schicht, so erh�lt man:
t^y=�h=1HNhy�sh
�und:
V^(t^y)=�h=1HN2h(1nh?1Nh)S2y,shS2y,sh=1nh?1�k?sh(yk?y�sh)2
Hierbei ist� h der Schichtindex, nh der Umfang der Stichprobe sh in Schicht h und Nh der Populationsumfang der Schicht h.
H�ufig wird nach regionalen Merkmalen, z.B. Bundesland, und innerhalb dieser Regionalschichten nach Siedlungstypen, z.B. fachliche Gliederung beim Mikrozensus, geschichtet.
H�ufig wird der Umfang der Stichproben in der jeweiligen Schicht proportional zur (bekannten!) Schichtgr��e gew�hlt.
4.8 Klumpen-Stichproben (Cluster Sampling)
Klumpen-Stichproben ben�tigt man, wenn man keinen Auswahlrahmen auf den ben�tigten Einheiten hat. Beispielsweise gibt es kein nationales Verzeichnis von Sch�lern. Allerdings kann man zun�chst Schulen ausw�hlen und dann alle Sch�ler der ausgew�hlten Schule.
Eine weitere Motivation f�r Cluster Sampling ist die Reduktion der Feldkosten in einer Interviewer-basierten Befragung wie dem Mikrozensus. Hier wird ganz Deutschland in kleineste Fl�chenst�cke von jeweils ca. 12 Haushalten eingeteilt und alle Haushalte dieses ''Auswahlbezirks'' werden interviewt. Cluster Sampling f�hrt aufgrund der h�ufigen r�umlichen Korrelation der Merkmale in der Stichprobe zu einer Vergr��erung der Varianz des Populationssch�tzers.
4.9 Auswertung von Zufallsstichproben der Umfragedaten
> dat<-read.csv2("D:\\Lehre\\Statistik3\\Umfrage.csv")
> dim(dat)
[1] 3471   17
> head(dat, 2)
    ID   GESCHL GEBJAHR         BERUFSTAETIG ARBEITSSTD ARZTBES RAUCH GRO GEW SCHULABSCHLUSS   SCHULABSCHLUSS_V
1 1359 WEIBLICH    1967  NICHT ERWERBSTAETIG         NA       1    JA 162  79 MITTLERE REIFE     MITTLERE REIFE
2 2455 WEIBLICH    1964 HAUPTBERUFL.HALBTAGS         30       1  NEIN 165  59 MITTLERE REIFE VOLKS-,HAUPTSCHULE
    SCHULABSCHLUSS_M HOE_ABSCHLUSS      HOE_ABSCHLUSS_V      HOE_ABSCHLUSS_M NETTO ZUFR
1     HOCHSCHULREIFE         LEHRE GEWERBL.,LANDW.LEHRE   MEISTER, TECHNIKER   475   10
2 VOLKS-,HAUPTSCHULE         LEHRE   MEISTER, TECHNIKER KAUFMAENNISCHE LEHRE   780    9
Mittelwert Nettoeinkommen, Grundgesamtheit: Die ganze Stichprobe

> mean(dat$NETTO)
[1] NA
> mean(dat$NETTO, na.rm=T) ; median(dat$NETTO, na.rm=T) #mean,median
[1] 1569.513
[1] 1300
> sum(!is.na(dat$NETTO))             # Wieviele Nettof�lle vorhanden
[1] 2717
> sum(is.na(dat$NETTO))                # Anzahl NA
[1] 754
Einige 10 % Stichproben
> s1<-dat$NETTO[sample(3471,340)] ; length(s1) ; mean(s1, na.rm=T)
[1] 340
[1] 1431.252
> s2<-dat$NETTO[sample(3471,340)] ; length(s2) ; mean(s2, na.rm=T)
[1] 340
[1] 1486.515
Arbeitserleichterung: Erzeugen einer Matrix mit Stichproben

> stich<-rbind(NULL,summary(dat$NETTO[sample(3471,340)],na.rm=T))
> stich<-rbind(stich,summary(dat$NETTO[sample(3471,340)],na.rm=T))
> stich<-rbind(stich,summary(dat$NETTO[sample(3471,340)],na.rm=T))
> stich<-rbind(stich,summary(dat$NETTO[sample(3471,340)],na.rm=T))
> dim(stich)
> 4 7
Die Ergebnisse von 4 Stichproben in einer Matrix:
> stich
     Min. 1st Qu. Median     Mean 3rd Qu.  Max. NA's
[1,]  120   797.5   1300 1490.805  2000.0  6000   84
[2,]  100   800.0   1300 1541.970  1842.5 17500   74
[3,]   45   742.5   1300 1450.821  1900.0 10000   78
[4,]   60   800.0   1345 1498.426  2000.0  7500   82

Mittelwert der Mittelwerte:
> stich[,"Mean"]
[1] 1490.805 1541.970 1450.821 1498.426

Bemerkenwert: Median ist jeweils �hnlich, was noch fehlt, ist Varianz bzw. Standardabweichung

Stundenaufgaben:
1. Welche Vorteile haben Stichproben im Vergleich zu Totalerhebungen?
2. Nennen Sie einen Nachteil von Stichproben
3. Was ist Representativit�t einer Stichprobe und wie kann man sie erreichen?
4. Was ist eine einfache Zufallsauswahl?
5. Was ist eine geschichtete Stichprobe
6. Was ist eine Klumpenauswahl
7. Was ist ein Sch�tzer bzw. eine Sch�tzfunktion, Beispiele
8. Berechnung des Totals des Nettoeinkommens der Umfragedaten
Sch�tzen Sie das Total aus der Stichprobe, Achtung NA!
Hochrechnung des Totals aus der Stichprobe
Was ist hier besser Median oder Mittelwert?
4.10 Das Problem fehlende Werte (NA)
Fehlende Werte sind in der empirischen Forschung h�ufig nicht vermeidbar und k�nnen bei der Anwendung von klassischen statistischen Analyseverfahren zu Problemen f�hren - da diese in der Regel komplette F�lle erfordern. Je mehr Variablen mit fehlenden Werten behaftet sind, desto kleiner wird die Schnittmenge mit F�llen, bei denen keine fehlenden Werte (komplette F�lle) auftreten.
Ein weiteres Problem entsteht dadurch, dass fehlende Werte selten zuf�lliger Natur sind. Es kann vorkommen, dass fehlende Werte in einer Variable von anderen Variablen abh�ngen.
�In einer Umfragen wird nach Einkommen und Bildungsniveau gefragt und es kommt dazu, dass Personen mit einem h�heren Bildungsniveau h�ufiger die Angabe ihres Einkommens verweigern als Personen mit einem niedrigeren Bildungsniveau, dann sind die fehlenden Werte in der Variable Einkommen nicht zuf�lliger Natur.�
Es ist auch denkbar, dass die fehlenden Werte von den Auspr�gungen der eigentlichen Variable abh�ngen. Auf das vorherige Beispiel bezogen w�rde dies bedeuten, dass Personen mit einem h�heren Einkommen h�ufiger die Angabe ihres Einkommens verweigern als Personen mit einem niedrigeren Einkommen � unabh�ngig von ihrem Bildungsniveau.
Die Missachtung dieser Abh�ngigkeitsstrukturen und der fallweise Ausschluss fehlender Werte kann unter anderem zu verzerrten Sch�tzergebnissen (z.B. verzerrte Parametersch�tzer) und zu einem Verlust an Pr�zision (z.B. gr��ere Standardfehler und Konfidenzintervalle) f�hren.
Um die Probleme, die fehlende Werte mit sich bringen, zu vermeiden, sollte darauf geachtet werden, dass fehlende Werte, wenn m�glich, gar nicht erst entstehen.
Es werden 3 Kategorien von fehlenden Werten unterschieden: Missing completely at random (MCAR), Missing at random (MAR) und Missing not at random (MNAR).
MCAR bedeutet, dass die Wahrscheinlichkeit f�r einen fehlenden Wert bei der Variable Y2 unabh�ngig von den Werten der Variable Y2 und unabh�ngig von dem Wert der restlichen Variablen ist. Demnach entstehen die fehlenden Werte rein zuf�llig und es gibt keine systematisch fehlenden Werte.
Von MAR wird gesprochen, wenn die Wahrscheinlichkeit f�r einen fehlenden Wert bei der Variable y2 von dem Wert einer anderen Variable y1 abh�ngt. Die Wahrscheinlichkeit f�r einen fehlenden Wert in y2 wird aber nicht von den Werten der eigentlichen Variable y2 beeinflusst.
Von MNAR wird gesprochen, wenn die Wahrscheinlichkeit f�r das Auftreten eines fehlenden Werts bei der Variable y2 von der Variable selbst abh�ngt, nachdem f�r den Einfluss aller anderen beobachteten Variablen kontrolliert wurde.
help(complete.cases)
4.10 Vergleich der einfachen Zufallsstichprobe und der 
     geschichteten Stichprobe, Einkommen und K�rpergr��e
Antwortquote der Umfragedaten, NA-F�lle

> tab<-cbind(a<-colSums(is.na(dat)),round(100*a/3471,1))  # tab

> colnames(tab)<-c("Antwort abs","Anwort rel") ; tab
                 Antwort abs Anwort rel
ID                         0        0.0
GESCHL                     0        0.0
GEBJAHR                    3        0.1
BERUFSTAETIG               0        0.0
ARBEITSSTD              1550       44.7
ARZTBES                    5        0.1
RAUCH                      0        0.0
GRO                        5        0.1
GEW                       44        1.3
SCHULABSCHLUSS             0        0.0
SCHULABSCHLUSS_V           0        0.0
SCHULABSCHLUSS_M           0        0.0
HOE_ABSCHLUSS              0        0.0
HOE_ABSCHLUSS_V            0        0.0
HOE_ABSCHLUSS_M            0        0.0
NETTO                    754       21.7
ZUFR                       5        0.1
Schichtung des Nettoeinkommens mit dem Merkmal Geschlecht
Da die na-F�lle den Bearbeitungsaufwand erh�hen, sollen diese von Anfang an entfernt werden. Es kommt die sehr n�tzliche R-Funktion subset zum Einsatz.
help(subset)
> red<-subset(dat[!is.na(dat$NETTO),])  # bereinigt um na-F�lle
> dim(red)
1] 2717   17
Der Datensatz red wird nun als die Grundgesamtheit angenommen.
gg<-mean(red$NETTO)     # durchnittliches Einkommen in Grundg.
Mit der extrem wichtigen R-Funktion tapply (wie Kreuztabelle) l�sst sich die Verteilung des Einkommens nach Geschlecht berechnen.
help(tapply)
# geschichtet nach Geschlecht, Anwendung von tapply auf NETTO
> tapply(red$NETTO, list(red$GESCHL), FUN=mean)
MAENNLICH  WEIBLICH 
 1908.214  1195.946
Auch Varianz und Standardabweichung lassen sich so berechnen.
> tapply(red$NETTO, list(red$GESCHL), var)
MAENNLICH  WEIBLICH 
6225828.0  629790.6 
> tapply(red$NETTO, list(red$GESCHL), sd)
MAENNLICH  WEIBLICH 
2495.1609  793.5935
Um den Schichtungseffekt nachzuweisen, soll eine Stichprobe im Umfang 2 % der gg gezogen werden.

> 2717*.02                             # Stichprobe 54 Antworten
[1] 54.34
> s<-sample(2717,54) ; length(s)       # Zufallsauswahl
[1] 54
> head(s)                              # Stichprobe als Index
[1] 2076 2209 1525 1774  470 1725
Einfache Zufallsstichprobe
> simple<-mean(red$NETTO[s]) ; simple
[1] 1617.056
Mit Merkmal Geschlecht geschichtete Stichprobe
> strata<-tapply(red$NETTO[s], list(red$GESCHL[s]), mean); strata
MAENNLICH  WEIBLICH 
 1880.562  1233.773 
> mean(strata)             # man k�nnte noch nach Shcicht gewichten
[1] 1557.168

> c(simple, mean(strata))/gg   # Vergleich simple und strato mit gg
[1] 1.0302912 0.9921342

Resultat: Strata ist hier eindeutig �berlegen.
Derselbe Versuch (Schichtung) mit der K�rpergr��e.

> red<-subset(dat[!is.na(dat$GRO),])  ; dim(red)
[1] 3466   17
> gg<-mean(red$GRO)          # Grundgesamtheit K�rpergr��e

> 3466*.02                   # 2 % Stichprobenumfang
[1] 69.32

> s<-sample(3466,69) ; head(s)               # Stichprobenindex
[1] 2684 2497  708 1466 2315  412
> strata<-tapply(red$GRO[s], list(red$GESCHL[s]), mean); strata
MAENNLICH  WEIBLICH 
 178.0250  167.0345 
> simple<- mean(red$GRO[s]) ; simple
[1] 173.4058

> c(simple, mean(strata))/gg              # Vergleich gg
[1] 1.0046410 0.9995655

> 1-c(simple, mean(strata))/gg
[1] -0.0046410398  0.0004344626

Auch hier ist strata eindeutig besser, Interpretieren Sie bitte dieses Ergebnis! Wer Lust hat, kann auch Varianz (var) und Standardabweichung (sd) mit und ohne Schichtung untersuchen.
5. Besonderheiten der Markt- und Meinungsforschung
Meinungsforschung � Demoskopie: Teilgebiet der empirischen Sozialforschung zur Ergr�ndung der �ffentlichen Meinung (Umfrage). Analyse von gesellschaftlichen und wirtschaftlichen Tatbest�nden, v.a. Marktanalyse, Werbewirkungs-Analyse und Erfassung der Auswirkungen gesellschafts- oder betriebspolitischer Ma�nahmen (innerbetriebliche Meinungsforschung, Betriebsklima).
Meinungsforschung beruht meist auf Repr�sentativerhebungen mithilfe von Zufallsstichprobenverfahren oder Quotenverfahren (Repr�sentativerhebung). Meinungsforschung ist kostspielig, auch bei repr�sentativer Erfassung eines kleinen Querschnitts (Querschnittuntersuchung), da erforderlich: (Interviewer-)Bias eine gr�ndliche Ausbildung der Interviewer (psychologische und technische Schulung sowie Spezialausbildung �ber den interessierenden Fragenkomplex); technische Einrichtungen zur Ermittlung des optimalen Ausleseverfahrens und zur statistischen Auswertung.
Ablauf eines Erhebungsprojektes
Projekte der Markt und Meinungsforschung orientieren sich im Ablauf an den Schritten einer empirischen Erhebung. 
Gilt auch f�r Systemanalyse und Unternehmensberatung.
Man kann folgende Phasen unterscheiden: 
1 Formulierung des Problems, Auftrag, Zielsetzung
2 Festlegung des Untersuchungsdesigns, Merkmale, Grundgesamtheit
3 Erfassung der Informationsquellen, Sekund�rdaten
4 Festlegung der Datenerhebungsmethode, Stichprobenplanung
5 Entwurf des Fragebogens
6 Umfrage per Post oder Interviewer
7 Durchf�hrung der Erhebung
8 Eingabe und Kodierung der Daten
9 Analyse und Auswertung und Interpretation der Daten
10 Pr�sentation der Forschungsergebnisse, Bericht
Dies entspricht schematisch dem Ablauf vieler sozialwissenschaftlicher Erhebungen sowie anderer empirischer Studien und Projekte.
6 R-Datenstrukturen
6.1 Datentyp und Datenstruktur. Vektoren, Listen und Arrays
Ermittlung des Datentyps einer importierten BSR-Datei durch Abfragen. Es handelt sich um Anlieferdaten aus dem Jahr 1994.

> bsr<-read.table("bsrorg.txt",header=T)        # Datenimport

> dim(dat)
[1] 15580     7

> class(dat)                                     # ein Data.frame
[1] "data.frame"

> dat[1,]
   tag zeit   Mg    kfznr bsrkey  Laga P           # erste Zeile
1 2609  514 0.76 B-EE2388    UNB 91200 A
Oder

> head(dat)                                   # die ersten Zeilen
   tag zeit   Mg    kfznr bsrkey  Laga P
1 2609  514 0.76 B-EE2388    UNB 91200 A
2 2609  536 6.38 B-CX2104    UNB 91200 A
3 2609  542 2.28  B-CR332    UNB 91200 A
4 2609  613 1.00  B-DK236    UNB 91200 A
5 2609  623 2.36 B-CX5170    UNB 91200 A
6 2609  624 0.98 B-DK8221    UNB 91200 A
Erzeugen eines leeren Vektors der L�nge sieben mit der Funktion rep (replicate)

> e1<-rep(NULL,7)

Abfrage und selektive Zuweisung des Ergebnisses. Anwendung der Funktion sapply.
help(sapply)
lapply returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.
Mit sapply statt lapply kann man oft die Syntax  oder das Ergebnis vereinfachen.
Aus den Beispielen von sapply:                # Liste
> x <- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE))
> # compute the list mean for each list element
> lapply(x, mean)
$a
[1] 5.5

$beta
[1] 4.535125

$logic
[1] 0.5
> unlist(lapply(x, mean))
       a     beta    logic 
5.500000 4.535125 0.500000

Die 1. Zeile von bsr wird abgefragt, welcher Datentyp?
> e1[sapply(bsr[1,], is.character)]<-"character"
> e1[sapply(bsr[1,], is.numeric)]<-"numerisch"
> e1[sapply(bsr[1,], is.logical)]<-"logisch"
> e1[sapply(bsr[1,], is.factor)]<-"faktor"
  e1                                 # Ergebnis
[1] "numerisch" "numerisch" "numerisch" "faktor" "faktor" "numerisch" "faktor"   

Definierte Datentypen in R
� null
� logical
� numeric
� complex
� character
� factor
Anmerkung: In arithmetischen Ausdr�cken findet eine automatische Umwandlung logical -> numeric statt und zwar wird TRUE zu 1 und FALSE zu 0.
Werden character-Vektoren mit Vektoren anderen Datentyps wie z.B. numeric oder logical mithilfe der Funktion c() verkettet, so werden alle Elemente in den Modus character konvertiert:

> sauber<-c("IMI", "ATA", "PERSIL")           # Textvektor
> c(999, sauber, T,F)                         # Konkatinieren
[1] "999"    "IMI"    "ATA"    "PERSIL" "TRUE"   "FALSE"

Um Daten von unterschiedlichem Datentyp in einem Objekt zu vereinen, ben�tigen wir den Modus Liste. In einem data.frame kann man verschiedene Datentypen mischen (mixed). Jede Spalte kann anderen Typ haben.

Welchen mode hat die erste Zeile von bsr?
> is.list(bsr[1,])
[1] TRUE

> mode(bsr[1,])
[1] "list"

Man kann auch Datenvektoren mit Labels versehen.
> menge<-c(50,100,200)
> names(menge)<-sauber ; menge                 # Funktion names
   IMI    ATA PERSIL 
    50    100    200 
> sum(menge)                                   # noch numeric
[1] 350
> summary(menge)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   50.0    75.0   100.0   116.7   150.0   200.0
6.2 Indizierung von R-Objekten 
Es gibt mindesten 5 M�glichkeiten, um auf Teilmengen von R-Objekten (Vektor, Matrix, Array) zugreifen zu k�nnen:

1. positive Integer                            # Index positv
> menge[2:3]
   	ATA PERSIL 
   	100    200

2. negative Integer
> menge[-1]
  ATA PERSIL 
  100    200

3. logischer Index
> menge[c(F,T,T)]
  ATA PERSIL 
  100    200

4. Index durch Labels
> menge[c("ATA","PERSIL")]
  ATA PERSIL 
100  200
5. > menge[rev(1:3)]                        # r�ckw�rts
> PERSIL    ATA    IMI 
     200    100     50
6.3 Selektive Zuweisung (update)
Zuweisungen d�rfen auf der linken Seite des Zuweisungspfeils einen in Klammern eingeschlossenen Index enthalten. Dabei wird die Zuweisung der Elemente rechts vom Zuweisungspfeil nur auf die indizierten Elemente des Datenobjekts angewendet.

> menge1<-menge                          # Kopie von menge1 erzeugen
> menge1[2:3]<-menge1[2:3]*5 ; menge1
   IMI    ATA PERSIL 
    50   2500   5000

> menge1[menge1 == 50]<- 1000 ; menge1  # Zuweisung logische Abfrage
   IMI    ATA PERSIL 
  1000   2500   5000 

> menge1["IMI"]<- 2000 ; menge1         # nur label IMI
   IMI    ATA PERSIL 
  2000   2500   5000

Die Struktur soll beim Update des Datenobjekts erhalten bleiben.
> menge1[]<-555 ; menge1
   IMI    ATA PERSIL
   555    555    555


6.4 Listen durch Gruppierung mit split, Gruppenvektor
Warum Listen statt Tabellen? Vorteile? Nachteile?
Listen sind universelle Daten-Objekte. Sie habe die Form eines Vektors.

Der abstrakte Datentyp Liste (ADT Liste).
Um Implementierungsdetails vor dem Programmierer bzw. Benutzer zu verbergen, werden sog. abstrakte Datentypen eingef�hrt. Es ist oft zweckm��iger, Datenstrukturen mit Hilfe der Operationen zu beschreiben, als durch Einzelheiten der Implementierung. Wenn eine Datenstruktur auf diese Weise beschrieben wird, wird sie abstrakter Datentyp genannt. Die Grundidee ist, dass das, was die Datenstruktur leisten soll, von der Implementierung getrennt ist. Die "von Au�en" zugelassenen Operationen werden �ber wohldefinierte Methodenaufrufe abgewickelt (Kapselungsprinzip, Geheimnisprinzip). Die Kapselung versteckt die innere Struktur und Implementierung des Datentyps. Zugriffe d�rfen nur �ber die Schnittstelle erfolgen.
Man kann einen ADT als Blackbox mit Gebrauchsanweisung verstehen. Der Programmierer verwendet diesen Datentyp gem�� seiner ver�ffentlichten Spezifikationen.
In der Programmierung steht der Begriff �Liste� f�r �eine verkettete Folge von Elementen eines gegebenen Datentyps� mit einer endlichen Anzahl von Elementen. Diese Elemente k�nnen unterschiedlichen Typs sein, siehe z.�B. Liste (Datenstruktur). Das Einf�gen von Elementen wird als Einlisten, das Gegenteil � Austragen � wird als Auslisten bezeichnet.

R-Funktion split. 
split divides the data in the vector x into the groups defined by f. The replacement forms replace values corresponding to such a division. unsplit reverses the effect of split. F�r unsplit braucht man wieder den splitverktor (die Zuordnungsvorschrift).
Beispiel: 
Definition einer Liste aus den Zahlen 1 bis 10 mit einem Gruppierungsvektor
liste<-split (1:10, c(1,1,1,2,2,2,3,3,3,4))

> length(liste)                                   # wieviele Gruppen
[1] 4

> names(liste)
[1] "1" "2" "3" "4"

> names(liste)<-c("IMI", "ATA", "PERSIL", "FEWA")  # update Labels
> liste
$IMI
[1] 1 2 3

$ATA
[1] 4 5 6

$PERSIL
[1] 7 8 9

$FEWA
[1] 10
Anwendung von verschiedenen Funktionen auf Listen mit lapply (Listapply)

> lapply(liste,length)                                 # Anzahl
$IMI
[1] 3

$ATA
[1] 3

$PERSIL
[1] 3

$FEWA
[1] 1

> lapply(liste,max)                              # Maximum
$IMI
[1] 3

$ATA
[1] 6

$PERSIL
[1] 9

$FEWA
[1] 10

Dieser Output ist etwas unhandlich. Deshalb gibt es die function unlist und die Alternativ-Funktion zu lapply, n�mlich sapply.

> unlist(lapply(liste,mean))                # Mittelwert in den Gruppen
   IMI    ATA PERSIL   FEWA 
     2      5      8     10
Erleichterung und Zusammenfassung durch die Funktion sapply.
help(sapply) 
sapply returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.
TIP: Wendet eine Funktion an und entlistet (auspacken) sofort

sapply(liste, mean)
   IMI    ATA PERSIL   FEWA 
     2      5      8     10

Faustregel: Anstatt die Funktion lapply und dann unlist anzuwenden, gleich sapply nehmen.

Indizierung von Listen, Verpackungsgrad (Tiefe) beachten

> length(liste[1])
[1] 1

> length(liste[[1]])
[1] 3
Selektive Zuweisung auf einzele Listenelemente, z.B. soll die 3. Zahl vom 1. Listenelement ersetzt werden.
> liste[[1]][3]<-7
> liste[[1]]
[1] 1 2 7

> sapply(liste,mean)                         # Mittelwert anders
     IMI      ATA   PERSIL     FEWA 
3.333333 5.000000 8.000000 10.00000
Das 4. Listenelement soll ersetzt werden. 2 Varianten.
> liste[[4]]<-1:10                            #  unverpackt
> liste
$IMI
[1] 1 2 7

$ATA
[1] 4 5 6

$PERSIL
[1] 7 8 9

$FEWA
 [1]  1  2  3  4  5  6  7  8  9 10
Oder
> liste[4]<-1:10
Warnmeldung:
In liste[4] <- 1:10 :
  Anzahl der zu ersetzenden Elemente ist kein Vielfaches der Ersetzungsl�nge
Also vorher Liste Einlisten (Einpacken)

> liste[4]<-list(1:10)                     # vorher verpacken
> liste
$IMI
[1] 1 2 7

$ATA
[1] 4 5 6

$PERSIL
[1] 7 8 9

$FEWA
 [1]  1  2  3  4  5  6  7  8  9 10

Kann man auch ein Programm (function) in eine Liste packen?

> liste[4]<-list(tabs)
> liste
$IMI
[1] 1 2 7

$ATA
[1] 4 5 6

$PERSIL
[1] 7 8 9

$FEWA
function(x) {
gew<-cbind(x,apply(x,1,sum)) 
m<-dim(x)                          # dimension(x)
colnames(gew)[1+m[2]]<-"Gesamt"    # update je nach Spaltenzahl
gew}
Auch Vektoren, Matrizen und Dataframes k�nnen gesplittet und zu Listen umgeformt werden. 
Wichtig:
# Erzeugen einer Liste aus den Spalten einer Matrix. Dabei wird jede Spalte zu einem Listenelement.
>  mat<-matrix(1:25, ncol=5)
> mat
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16   21
[2,]    2    7   12   17   22
[3,]    3    8   13   18   23
[4,]    4    9   14   19   24
[5,]    5   10   15   20   25

> split(mat, col(mat))                  # Spalten als Listenelemente
$`1`
[1] 1 2 3 4 5

$`2`
[1]  6  7  8  9 10

$`3`
[1] 11 12 13 14 15

$`4`
[1] 16 17 18 19 20

$`5`
[1] 21 22 23 24 25
Noch wichtiger, split mit Zeilen. Jede Zeile oder jeder Datensatz wird zu einem Listenelement. Dadurch kann man Datens�tze untereinander oder mit anderen Datens�tzen vergleichen. Siehe unten!

> split(mat, row(mat))                    # Zeilen als Listenelemente
$`1`
[1]  1  6 11 16 21

$`2`
[1]  2  7 12 17 22

$`3`
[1]  3  8 13 18 23

$`4`
[1]  4  9 14 19 24

$`5`
[1]  5 10 15 20 25
Suchen in Listen, Funktion match.
> liste["ATA"]
$ATA
[1] 4 5 6

Suche nach Namen
> liste[names(liste)=="ATA"]
$ATA
[1] 4 5 6

Suche nach Daten in Listen                         # gesucht 4,5,6

liste==list(4:6)
Fehler in liste == list(4:6) : 
  Vergleich dieser Typen ist nicht implementiert
Also vielleicht noch was f�r Euch!

Anwendung der Function match
match returns a vector of the positions of (first) matches of its first argument in its second.

> match(4:6, liste)                            # 1. Versuch
[1] NA NA NA

> match(list(4:6), liste)                      # das 2. Element
[1] 2
  
> liste[match(list(4:6), liste)]               # mit Ausgabe
$ATA
[1] 4 5 6

Gleichzeitiges Suchen mehrerer Listenelemente

> match(liste,list(4:6,7:9))
[1] NA  1  2 NA

> liste[!is.na(match(liste,list(4:6,7:9)))]
$ATA
[1] 4 5 6

$PERSIL
[1] 7 8 9

Datens�tzen vergleichen und finden. Siehe oben. Matrix mat.
> listdata<-split(mat, row(mat))

Die 2. Zeile (Datensatz) wird auf Variable zweite zugewiesen und listdata angeh�ngt.

> listdata<-c(listdata, zweite<-listdata[2]) ; length(listdata)
Listenelemente mit names benennen.
> names(listdata)<-c("EINS","ZWEI","DREI","VIER","FUENF","SECHS")
> listdata                   
$EINS
[1]  1  6 11 16 21
$ZWEI
[1]  2  7 12 17 22

$DREI
[1]  3  8 13 18 23

$VIER
[1]  4  9 14 19 24

$FUENF
[1]  5 10 15 20 25

$SECHS
[1]  2  7 12 17 22

Ausfindigmachen des 2. Datensatzes (zweite) in der Liste mit der Funktion match.

> match(listdata, zweite)                      
[1] NA  1 NA NA NA  1

> listdata[!is.na(match(listdata, zweite))] # die identischen Zeilen                
$ZWEI
[1]  2  7 12 17 22

$SECHS
[1]  2  7 12 17 22              

Diese geniale R-Funktionalit�t wird sp�ter beim Eier-Datensatz angewendet. Ist auch wichtig f�r Datenkontrolle (EDA).
6.5 Der Array und seine Regeln
An array is a systematic arrangement of similar objects, usually in rows and columns.
In more theoretical contexts, especially in type theory and in the description of abstract algorithms, the terms "array" and "array type" sometimes refer to an abstract data type (ADT) also called abstract array or may refer to an associative array, a mathematical model with the basic operations and behavior of a typical array type in most languages � basically, a collection of elements that are selected by indices computed at run-time. 
Depending on the language, array types may overlap (or be identified with) other data types that describe aggregates of values, such as lists and strings. Array types are often implemented by array data structures, but sometimes by other means, such as hash tables, linked lists, or search trees. 
Definition deutsch: Datenstruktur, in der Datenelemente des gleichen Datentyps unter einem gemeinsamen Namen zusammengefasst werden (homogene Struktur). Die Elemente des Array werden durch Indizierung des Arraynamens angesprochen.
R-Definition
An array in R can have one, two or more dimensions. It is simply a vector which is stored with additional attributes giving the dimensions (attribute "dim") and optionally names for those dimensions (attribute "dimnames"). 
A two-dimensional array is the same thing as a matrix. 
One-dimensional arrays often look like vectors, but may be handled differently by some functions: str does distinguish them in recent versions of R. 
Ein dreidimensionaler Fahrrad-Array f.a. dim(f.a)= 1,2,4

Ein Array kann beliebigen Dimensionen haben, enth�lt Daten von demselben Datentyp.
> a <- array(data=1:24, dim=c(4,3,2))        3D-Array

dimnames(a)=list(letters[1:4],LETTERS[1:3],c("Mann","Frau"))

> dimnames(a)                            # dimnames hat listenform
[[1]]
[1] "a" "b" "c" "d"

[[2]]
[1] "A" "B" "C"

[[3]]
[1] "Mann" "Frau"

> length(dim(a))
[1] 3

> length(dim(a))>2
[1] TRUE

6.6 Funktionen auf arrays

# mit der Funktion aperm lassen sich arrays im Raum um bestimmte Achsen bewegen. Die Funktion t (transpose) ist ein Spezialfall f�r zweidimensionale Matrizen.

> dim(a)
[1] 4 3 2

> a
, , Mann

  A B  C
a 1 5  9
b 2 6 10
c 3 7 11
d 4 8 12

, , Frau

   A  B  C
a 13 17 21
b 14 18 22
c 15 19 23
d 16 20 24

> dim(a) ; dim(aperm(a, c(2,1,3)))           # Zeilen und Spalten vertauscht
[1] 4 3 2
[1] 3 4 2

> aperm(a, c(2,1,3))                          # Zeilen und Spalten vertauscht
, , Mann

  a  b  c  d
A 1  2  3  4
B 5  6  7  8
C 9 10 11 12

, , Frau

   a  b  c  d
A 13 14 15 16
B 17 18 19 20
C 21 22 23 24

# mit der Funktion apply und sweep (sweep out) k�nnen Aggragatfunktionen �ber �ber bestimmte Koordinaten des Arrays angewendet werden (Bedeutung: L�schen oder Reduzieren).
Genannt im Funktionsaufruf werden die Ebenen, die betroffen sind. Die ungenannte Dimension wird reduziert.

> apply(a,c(1,2),sum)       # Summe �ber die planes (Ebenen) des arrays
   A  B  C
a 14 22 30
b 16 24 32
c 18 26 34
d 20 28 36

> dim(apply(a,c(1,2),sum))              # planes verschwinden
[1] 4 3

# Mittelwerte �ber die Ebenen werden abgezogen


help(sweep)
Return an array obtained from an input array by sweeping out a summary statistic.

Von beiden planes des arrays soll der Mittelwert der planes abgezogen werden.
> apply(a,c(1,2),mean)                     # planes Mittelwert
   A  B  C
a  7 11 15
b  8 12 16
c  9 13 17
d 10 14 18

Darstellung der planes nebeneinander

> cbind(a[,,1],a[,,2], apply(a,c(1,2),mean))
  A B  C 	 A  B  C  A  B  C
a 1 5  9 	13 17 21	 7 11 15
b 2 6 10 	14 18 22 	 8 12 16
c 3 7 11 	15 19 23 	 9 13 17
d 4 8 12 	16 20 24	10 14 18

> sweep(a, c(1,2), apply(a, c(1,2), mean), "-")    # Mittelwert abziehen

, , Mann

   A  B  C
a -6 -6 -6
b -6 -6 -6
c -6 -6 -6
d -6 -6 -6

, , Frau

  A B C
a 6 6 6
b 6 6 6
c 6 6 6
d 6 6 6

6.7 Zusammenf�gen von arrays
Zun�chst ben�tigt wird das Paket abind. Das muss noch installiert und geladen werden. 
help(abind)                        # zun�chst nicht zu finden
Ein zus�tzliches Paket (abind) soll verf�gbar gemacht werden.
Kann man �ber GUI machen, Men�punkt: Packete
Oder mit den nachfolgenden Befehlen.
1. Setze CRAN Mirror 
>chooseCRANmirror()               # Fenster f�r Server

2. Installiere gew�nschtes Package
>install.packages("abind")          # Package wird installiert
Kontrolle, ob jetzt installiert?
> head(.packages(all.available = TRUE))
[1] "abind" "maps"  "proto" "RODBC" "base"  "boot"

3. Lade Package (oder auch library)
> library("abind")
>library(abind)                    # installiertes Paket laden  
help(abind)   
Combine multi-dimensional arrays. This is a generalization of cbind and rbind. Takes a sequence of vectors, matrices, or arrays and produces a single array of the same or higher dimension.
# diverse Varianten von Verbindungen

dim(abind(x,y,along=0))     # binds on new dimension before first
dim(abind(x,y,along=1))     # binds on first dimension
dim(abind(x,y,along=1.5))
dim(abind(x,y,along=2))
dim(abind(x,y,along=3))
dim(abind(x,y,rev.along=1)) # binds on last dimension
dim(abind(x,y,rev.along=0)) # binds on new dimension after last
Zwei Matrizen sollen zu einem array zusammengef�gt werden.
Mit cbind bleibt man immer in derselben Dimension!

> cbind(matrix(1:4, ncol=2), matrix(4:1,ncol=2))
     [,1] [,2] [,3] [,4]
[1,]    1    3    4    2
[2,]    2    4    3    1

Mit abind (arraybind) erzeugt man eine neue, h�here Dimension.
along = 2.5 bedeutet, dass eine neue Dimension nach der ersten und zweiten entstehen soll.
> abind(matrix(1:4, ncol=2), matrix(4:1, ncol=2),along=2.5)
, , 1

     [,1] [,2]
[1,]    1    3
[2,]    2    4

, , 2

     [,1] [,2]
[1,]    4    2
[2,]    3    1

1. Der Array a soll so zusammengef�gt werden, dass die vierte als neue und letzte Dimension entstehen soll.
> length(dim(abind(a, a, along=3.5)) )        # nun 4 dimensional
[1] 4
> dim(abind(a, a, along=3.5))                   
[1] 4 3 2 2
2. Der Array a wird mit der Summe �ber die Seiten (Geschlecht) verkettet. Der array bleibt dabei dreidimensional.

> asum<-abind(a,apply(a,c(1,2),sum),along=3) ; dim(asum)
[1] 4 3 3

# Update asum mit Gesamt, wie bei unserem Programm tabs.

dimnames(asum)[[3]][3]<-"Gesamt"
> asum
, , Mann

  A B  C
a 1 5  9
b 2 6 10
c 3 7 11
d 4 8 12

, , Frau

   A  B  C
a 13 17 21
b 14 18 22
c 15 19 23
d 16 20 24

, , Gesamt

   A  B  C
a 14 22 30
b 16 24 32
c 18 26 34
d 20 28 36

asum kann man auch als Data-Cube bezeichnen.
Stundenaufgaben:
* Schreiben Sie ein gut dokumentiertes listenorientiertes Programm wie in der Hausaufgabe Nr. 1 f�r Datentyperkennung von Listen bzw. Data-Frames.
* Schreiben Sie ein Programm tab3s f�r Summenberechung von 
3-D-arrays. Verwenden Sie u.a. die Funktion abind.
Hierf�r brauchen Sie auch das Package abind.
* Importieren Sie die Lebendgeborenen aus der Tabelle bev_ent.xls nach Bundesl�ndern (12), Jahren (2006 bis 2010) und Geschlecht (2). F�gen Sie diese Daten in einen 3-D-array der Dimension 16 5 2 ein. Hierbei wird es n�tig sein, die Funktionen array, dim, unlist und aperm anzuwenden. Beachten Sie bitte dabei, dass in R normalerweise erst die Spalten gef�llt werden. Die Option byrow=T habe ich f�r arrays nicht gefunden.  
* Erzeugen Sie einen Data-Cube, indem Sie das Programm tab3s auf diesen array anwenden und selektieren Sie die Tabellen Geborene nach Bundesland und Jahr sowie Geborene nach Jahr und Geschlecht aus dem Data-Cube.
* Entdecken Sie die Funktion sapply und zeigen Sie die Funktionsweise in Beispielen.
* Zeigen Sie bitte die Anwendbarkeit Funktion sweep f�r Hausaufgabe 2, Nr. 3 anstelle von proc.table.


Abk�rzung der deutschen Bundesl�nder


BW
Baden-W�rttemberg
BY
Bayern
BE
Berlin
BB
Brandenburg
HB
Bremen
HH
Hamburg
HE
Hessen
MV
Mecklenburg-Vorpommern
NI
Niedersachsen
NW
Nordrhein-Westfalen
RP
Rheinland-Pfalz
SL
Saarland
SN
Sachsen
ST
Sachsen-Anhalt
SH
Schleswig-Holstein
TH
Th�ringen
6.7 Datensatz geburt_land.csv umformen (3-D-Array, Data-Cube)
Import mit read.csv2, Variable geburt.
> dim(geburt)
[1] 48  31
> head(geburt[,1:8])
         Bundesland Geschlecht  X1990  X1991  X1992  X1993  X1994  X1995
1 Baden-W�rttemberg   m�nnlich  60680  60363  60600  60594  58497  57878
2 Baden-W�rttemberg   weiblich  57899  57165  56959  57388  54901  54581
3 Baden-W�rttemberg  Insgesamt 118579 117528 117559 117982 113398 112459
4            Bayern   m�nnlich  69950  69297  68396  68762  65808  64625
5            Bayern   weiblich  66172  65103  65550  65135  62020  61370
6            Bayern  Insgesamt 136122 134400 133946 133897 127828 125995

In einen Array k�nnen nur die numerischen Daten gepackt werden (homogen).
> is.numeric(as.matrix(geburt[,3:31]))  # ohne erste Spalten
[1] TRUE

> num<-geburt[,3:31]
> dim(num)
[1] 48  29
> 
> 48/3                         # Wieviel Bundesl�nder?
[1] 16

> bev.cube<-array(as.matrix(num), dim=c(3,16,29)) # array 
> dim(bev.cube)
[1]  3 16 29

Zeilen und Spaltenlabel sind verschwunden. Noch Erg�nzen.
> dimnames(bev.cube)[[1]]<-c("maennlich","weiblich", "Gesamt")

> bula<-unlist(strsplit("BW BY BE BB HB HH HE MV NI NW RP SL SN ST SH TH", " "))                  
# funktion straplit erleichtert die Eingabe
> bula
 [1] "BW" "BY" "BE" "BB" "HB" "HH" "HE" "MV" "NI" "NW" "RP" "SL" "SN" "ST" "SH" "TH"
> dimnames(bev.cube)[[2]]<-bula
> dimnames(bev.cube)
[[1]]
[1] "maennlich" "weiblich"  "Gesamt"   

[[2]]
 [1] "BW" "BY" "BE" "BB" "HB" "HH" "HE" "MV" "NI" "NW" "RP" "SL" "SN" "ST" "SH" "TH"

[[3]]
NULL
> dimnames(bev.cube)[[3]]<-1990:2018
> dimnames(bev.cube)
[[1]]
[1] "maennlich" "weiblich"  "Gesamt"   

[[2]]
 [1] "BW" "BY" "BE" "BB" "HB" "HH" "HE" "MV" "NI" "NW" "RP" "SL" "SN" "ST" "SH" "TH"

[[3]]
 [1] "1990" "1991" "1992" "1993" "1994" "1995" "1996" "1997" "1998" "1999" "2000" "2001" "2002" "2003" "2004" "2005"
[17] "2006" "2007" "2008" "2009" "2010" "2011" "2012" "2013" "2014" "2015" "2016" "2017" "2018"

Selektion von bev.cube:
>  bev.cube[3,,1:10]               # Gesamt (Geschlecht)
     1990   1991   1992   1993   1994   1995   1996   1997   1998   1999
BW 118579 117528 117559 117982 113398 112459 114657 116419 111056 107973
BY 136122 134400 133946 133897 127828 125995 129376 130517 126529 123244
BE  37596  30562  29667  28724  28503  28648  29905  30369  29612  29856
BB  29238  17215  13469  12238  12443  13494  15140  16370  17146  17928
HB   6895   6789   6757   6656   6288   6429   6623   6644   6360   6096
HH  16693  16503  16497  16257  16201  15872  16594  16970  16235  16034
HE  62026  61324  61146  61610  60565  59858  62391  63124  60567  58996
MV  23503  13635  10875   9432   8934   9878  11088  12046  12246  12589
NI  82452  83122  83669  84579  81520  80994  83655  85907  82207  80483
NW 199294 198436 196899 194156 186079 182393 188493 190386 182287 176578
RP  42732  42311  42722  42291  40539  39684  40926  41677  39639  38190
SL  11210  11052  10954  10653  10028   9727   9976   9987   9111   8941
SN  49672  31278  25298  23423  22734  24004  27006  29008  30190  31383
ST  31837  19459  16284  14610  14280  14568  16152  17194  17513  18176
SH  29046  28935  28757  28632  27542  27430  28766  29080  27729  27351
TH  28780  17470  14615  13307  12721  13788  15265  16475  16607  16926

> bev.cube[,,1]                     #  nur das Jahr 1990 
         BW     BY    BE    BB   HB    HH    HE    MV    NI     NW    RP    SL    SN    ST    SH    TH
maennlich  60680  69950 19351 15078 3588  8689 31815 11951 42505 102383 21961  5803 25507 16486 14936 14696
weiblich   57899  66172 18245 14160 3307  8004 30211 11552 39947  96911 20771  5407 24165 15351 14110 14084
Gesamt    118579 136122 37596 29238 6895 16693 62026 23503 82452 199294 42732 11210 49672 31837 29046 28780


7	Vergleich diverser Statistik Programme
https://www.inwt-statistics.de/blog-artikel-lesen/Statistik-Software-R_SAS_SPSS_STATA_im_Vergleich.html vom 25.4.2018
8 Datenbanken und R
8.1 Sprache SQL
SQL ist eine Datenbanksprache zur Definition von Datenstrukturen in relationalen Datenbanken sowie zum Bearbeiten (Einf�gen, Ver�ndern, L�schen) und Abfragen von darauf basierenden Datenbest�nden. 
Die Sprache basiert auf der relationalen Algebra, ihre Syntax ist relativ einfach aufgebaut und semantisch an die englische Umgangssprache angelehnt. Fast alle g�ngigen Datenbanksysteme unterst�tzen SQL � allerdings in unterschiedlichem Umfang und leicht voneinander abweichenden �Dialekten�. Durch den Einsatz von SQL strebt man die Unabh�ngigkeit der Anwendungen vom eingesetzten Datenbankmanagementsystem an. 
Die Bezeichnung SQL (offizielle Aussprache [?skju???l], oft aber auch [?si?kw?l] nach dem Vorg�nger; auf Deutsch auch h�ufig die deutsche Aussprache der Buchstaben) wird im allgemeinen Sprachgebrauch als Abk�rzung f�r �Structured Query Language� (auf Deutsch: �Strukturierte Abfrage-Sprache�) aufgefasst, obwohl sie laut Standard ein eigenst�ndiger Name ist. Die Bezeichnung leitet sich von dem Vorg�nger SEQUEL ([?si?kw?l], Structured English Query Language) ab, welche mit Beteiligung von Edgar F. Codd (IBM) in den 1970er Jahren von Donald D. Chamberlin und Raymond F. Boyce entworfen wurde.
SQL-Befehle lassen sich in drei Kategorien unterteilen (Zuordnung nach der Theorie der Datenbanksprachen in Klammern): 
* (DML) Befehle zur Datenmanipulation (�ndern, Einf�gen, L�schen) und lesendem Zugriff
* (DDL) Befehle zur Definition des Datenbankschemas
* (DCL) Befehle f�r die Rechteverwaltung und Transaktionskontrolle.
Mit Abfragen werden die in einer Datenbank gespeicherten Daten abgerufen, also dem Benutzer oder einer Anwendersoftware zur Verf�gung gestellt. 
Das Ergebnis einer Abfrage sieht wiederum aus wie eine Tabelle und kann oft auch wie eine Tabelle angezeigt, bearbeitet und weiterverwendet werden. 
8.2 Was ist eine Datenbank, ein DBMS, ein RDBMS
Eine Datenbank besteht aus Tabellen, die meist miteinander verk�pft sind. Eine grunds�tzliche Eigenschaft des Datenbankansatzes ist, dass ein Datenbanksystem nicht nur die Daten enth�lt, sondern auch eine komplette Definition oder Beschreibung der Daten. Diese Beschreibung besteht aus Angaben �ber den Umfang, die Struktur, die Art und das Format aller Daten sowie �ber die Beziehungen der Daten untereinander. Diese gespeicherten Informationen werden auch Metadaten genannt.
8.3 Vorteile von Datenbanken
Mehrfachnutzung
Ein Datenbanksystem erlaubt mehreren Benutzern gleichzeitig den Zugriff auf eine Datenbank. Die Beantwortung unterschiedlicher Fragestellungen diverser Benutzer mit den gleichen Daten ist ein zentraler Aspekt eines Informationssystems. Eine solche Mehrfachnutzung erh�ht auch die Wirtschaftlichkeit eines Systems. Die Datenerfassung und -haltung ist nicht redundant, das System kann zentralisiert betreut und die Daten k�nnen einfacher aktualisiert werden. 
Bei einer Mehrfachnutzung von Daten stellt sich aber die Frage, wie bei konkurrierenden �nderungen z. B. gleichzeitiges Ver�ndern der gleichen Daten von zwei verschiedenen Nutzern mit verschiedenen Anwendungen vorgegangen werden soll. 
Ein Beispiel f�r Mehrfachnutzung ist die Reise-Datenbank eines gr�sseren Reiseb�ros. Die Angestellten verschiedener Zweigstellen k�nnen gleichzeitig auf diese Datenbank zugreifen und Reisen buchen bzw. verkaufen. Jeder sieht sofort wie viele Pl�tze f�r eine bestimmte Reise noch erh�ltlich sind oder ob diese schon ausgebucht ist. Anderes Beispiel sind die noch vorhandenen Pl�tze f�r Theater oder Kino.
Datenintegrit�t
Datenintegrit�t ist ein Begriff f�r die Qualit�t und Zuverl�ssigkeit von Daten eines Datenbanksystems. Im weiteren Sinne z�hlt zur Integrit�t auch der Schutz der Datenbank vor unberechtigtem Zugriff (Vertraulichkeit) und Ver�nderungen.
Daten widerspiegeln Sachverhalte der realen Welt. Logischerweise wird verlangt, dass sie dies korrekt tun. Ein DBMS soll Unterst�tzung bieten bei der Aufgabe, nur korrekte und widerspruchsfreie (�konsistente�) Daten in die Datenbank gelangen zu lassen. Ausserdem wird mit korrekten Transaktionen die Konsistenz auch w�hrend des Systembetriebs aufrechterhalten. In derselben Datenbank d�rfen keine widerspr�chlichen Aussagen stehen.
Transaktionen
Eine Transaktion ist ein B�ndel von Aktionen, die in der Datenbank durchgef�hrt werden, um diese von einem konsistenten Zustand wieder in einen konsistenten widerspruchsfreien Zustand zu �berf�hren. Dazwischen sind die Daten zum Teil zwangsl�ufig inkonsistent. Eine Transaktion ist atomar, d. h. nicht weiter zerlegbar. Innerhalb einer Transaktion werden entweder alle Aktionen oder keine durchgef�hrt. Nur ein Teil der Aktionen w�rde zu einem inkonsistenten Datenbankzustand f�hren. 
Ein Beispiel einer Transaktion ist das Verschieben einer bestimmten Summe Geld von einem Konto auf ein anderes. Die Abbuchung des Geldes von einem Konto und die Gutschrift auf dem anderen Konto machen zusammen eine konsistente Transaktion aus. Diese Transaktion ist ausserdem atomar. Die Abbuchung oder die Gutschrift alleine w�rde zu einem inkonsistenten Zustand f�hren. Nach Abschluss der Transaktion Abbuchung und Gutschrift wird die �nderung an beiden Konten dauerhaft, und der Geldgeber sieht nun einen kleineren Kontostand, w�hrend der Empf�nger des Geldes sich �ber seinen h�heren Kontostand freuen kann. Wenn eine Transaktion erfolgreich war, muss sie best�tigt werden (COMMIT TRANSACTION). Ansonsten ROLLBACK. �nderungen, die eine Transaktion in einer Datenbank vornimmt, sind dauerhaft. Wenn die Transaktion abgeschlossen ist, kann auch ein darauf erfolgender Systemabsturz die Daten nicht mehr gef�hrden.
Datenbank � Datentypen und Spezifikation

Boolean
one bit per item

Integer byte
one bit per item

Integer halfword
SMALLINT
2 bytes per item
16-bit signed integer

Integer fullword
INTEGER

4 bytes per item
32-bit signed integer. INT can be used as a synonym

Real single precision
DECIMAL
4 bytes per item
Packed decimal. DEC, NUMERIC, and NUM are synonyms for this type.

Real double precision

8 bytes per item
DOUBLE              Double-precision floating point. DOUBLE PRECISION, and FLOAT are synonyms for this type

Character byte
CHAR
one bytes per item
Fixed-length character string of length 1 byte to 254 bytes. CHARACTER can be used as a synonym for this type

Character halfword
VARCHAR

2 bytes per item
Variable-length character string of length 1 byte to 4000 bytes. CHARACTER VARYING and CHAR VARYING are synonyms for this type.

Character fullword
LONG VARCHAR

4 bytes per item
Long variable-length character string of length
1 byte to 32700 bytes

8.4 Client-Server Prinzip
Das Client-Server Prinzip ist das Standardkonzept f�r die Verteilung von Aufgaben innerhalb eines Netzwerkes.
Die Aufgaben werden von Programmen erledigt, die in Clients und Server unterteilt werden. Der Client kann auf Wunsch eine Aufgabe vom Server anfordern (z. B. ein Betriebsmittel). Der Server, der sich auf einem beliebigen anderen Rechner im Netzwerk befindet, beantwortet die Anforderung (d. h. er stellt im Beispiel das Betriebsmittel bereit). Bei den Aufgaben kann es sich um Standardaufgaben (E-Mail-Versand, E-Mail-Empfang, Web-Zugriff, etc.) oder um spezifische Aufgaben einer Software oder eines Programms handeln. Eine Aufgabe wird im Client-Server-Modell als Dienst bezeichnet. 
Ein Server ist ein Programm oder ein Computer, das einen Dienst (Service) anbietet. Im Rahmen des Client-Server Prinzips kann ein anderes Programm, der Client, diesen Dienst nutzen. Die Kommunikation zwischen Client und Server ist abh�ngig vom Dienst, d. h. der Dienst bestimmt, welche Daten zwischen beiden ausgetauscht werden. Der Server ist in Bereitschaft, um jederzeit auf die Kontaktaufnahme eines Clients reagieren zu k�nnen. Im Unterschied zum Client, der aktiv einen Dienst anfordert, verh�lt sich der Server passiv und wartet auf Anforderungen. Die Regeln der Kommunikation f�r einen Dienst (Format, Aufruf des Servers, und die Bedeutung der zwischen Server und Client ausgetauschten Daten), werden durch ein Protokoll festgelegt. 
In der Regel l�uft das Datenbankprogramm auf einem Server und diverse Clients greifen darauf zu durch Abfragen.
8.5 ODBC und RODBC
Bei ODBC (Open Database Connectivity) handelt es sich um eine standardisierte, offene Schnittstelle f�r den Zugriff auf unterschiedliche Datenbankmanagementsysteme. �ber ODBC-Treiber k�nnen Anwendungen direkt Anweisungen an Datenbanken erteilen oder Abfragen ausf�hren.
Open Database Connectivity, abgek�rzt ODBC, wurde urspr�nglich von Microsoft entwickelt. ODBC hat sich mittlerweile als eine Art Standard f�r den Zugriff von Anwendungen auf unterschiedliche Datenbanken etabliert und ist neben Windows auch f�r Betriebssysteme wie Unix oder macOS verf�gbar. Die ODBC-Schnittstelle verwendet die Datenbanksprache SQL (Structured Query Language) und stellt Anwendungen eine offenes, standardisiertes API (Application Programming Interface) zur Verf�gung. Beispielsweise k�nnen ODBC-Anweisungen an Datenbanken wie dBase, Access und DB2 oder direkt an Excel-Dateien gesendet werden. 
F�r den Zugriff auf die verschiedenen Datenbanken ben�tigt ODBC spezielle Treiber. Dank der Nutzung von SQL k�nnen die Anwendungen mit den Datenbanken kommunizieren, ohne die propriet�ren Protokolle oder Schnittstellen der verschiedenen Datenbankmanagementsysteme zu kennen. Die SQL-Befehle der Anwendungen wandelt ODBC in die Sprache und Formate der Datenbanken um. Der eigentliche Zugriff auf die Daten erfolgt niemals direkt auf die Datenbank oder die Tabelle, sondern immer �ber die zugeh�rige ODBC-Komponente. Open Database Connectivity erlaubt sowohl den Zugriff auf lokale als auch auf entfernte Datenbanken �ber Netzwerkverbindungen. 
8.6 R-Verkn�pfung mit Datenbank, Datenbanktabelle lesen
Zuerst Package RODBC installieren wie das package abind.
Laden package RODBC, library(RODBC)
Help(RODBC) Index, hier findet man alle RODBC-Befehle
close.RODBC
ODBC Close Connections
getSqlTypeInfo
Specify or Query a Mapping of R Types to DBMS Types
odbcClearError
Low-level ODBC functions
odbcClose
ODBC Close Connections
odbcCloseAll
ODBC Close Connections
odbcConnect
ODBC Open Connections
odbcConnectAccess
ODBC Open Connections
odbcConnectAccess2007
ODBC Open Connections
odbcConnectDbase
ODBC Open Connections
odbcConnectExcel
ODBC Open Connections
odbcConnectExcel2007
ODBC Open Connections
odbcDataSources
List ODBC Data Sources
odbcDriverConnect
ODBC Open Connections
odbcEndTran
ODBC Set Auto-Commit Mode
odbcFetchRows
Low-level ODBC functions
odbcGetErrMsg
Low-level ODBC functions
odbcGetInfo
Request Information on an ODBC Connection
odbcQuery
Low-level ODBC functions
odbcReConnect
ODBC Open Connections
odbcSetAutoCommit
ODBC Set Auto-Commit Mode
odbcTables
Low-level ODBC functions
RODBC
ODBC Database Connectivity
setSqlTypeInfo
Specify or Query a Mapping of R Types to DBMS Types
sqlClear
Deletion Operations on Tables in ODBC databases
sqlColumns
Query Column Structure in ODBC Tables
sqlCopy
ODBC Copy
sqlCopyTable
ODBC Copy
sqlDrop
Deletion Operations on Tables in ODBC databases
sqlFetch
Reading Tables from ODBC Databases
sqlFetchMore
Reading Tables from ODBC Databases
sqlGetResults
Query an ODBC Database
sqlPrimaryKeys
Query Column Structure in ODBC Tables
sqlQuery
Query an ODBC Database
sqlSave
Write a Data Frame to a Table in an ODBC Database
sqlTables
List Tables on an ODBC Connection
sqlTypeInfo
Request Information about Data Types in an ODBC Database
sqlUpdate
Write a Data Frame to a Table in an ODBC Database
Ihr solltet auf Euren Windows-Rechner ein Office installiert haben. Von Microsoft oder Openoffice. Dann wurde das ODBC Programm automatisch mitinstalliert.
In unserem Beispiel besteht die Datenbank nur aus den Excel-Tabellen Bibiliotheksdaten books.xls und den Umfragedaten Umfrage.xls.
Verwaltung ODBC-Datenquellen. 
Hinzuf�gen einer ODBC-Datenquelle.
Hinweis:� Sie m�ssen ein Mitglied der Administratorgruppe auf dem lokalen Computer sein, um eine ODBC-Datenquelle hinzuzuf�gen oder zu konfigurieren.
Windwos 10:
Geben Sie auf der Seite Start den Text ODBC-Datenquellen 32 bit ein. Die Desktop-App �ODBC-Datenquellen� sollte als Option angezeigt werden. Das Dialogfeld ODBC-Datenquellen-Administrator wird angezeigt. ODBC Installationsprogramm starten.

Klicken Sie auf Benutzer-DSN, System-DSN oder Datei-DSN, je nach dem Typ der hinzuzuf�genden Datenquelle. Weitere Informationen finden Sie im Abschnitt Informationen zu ODBC-Datenquellen.
Es reicht, die Dateien books.xls und umfrage.xlsx als Benutzer-datenquelle (Benutzer-SN) f�r Excel-Treiber zu registrieren.
Danach Verbindung im R mit den Umfragedaten. Package RODBC muss geladen sein.
> chan<- odbcDriverConnect("")
> chan
RODBC Connection 1
Details:
  case=nochange
  DSN=Umfrage
  DBQ=D:\Lehre\Statistik3\Umfrage.xls
  DefaultDir=D:\Lehre\Statistik3
  DriverId=790
  FIL=excel 8.0
  MaxBufferSize=2048
  PageTimeout=5
Variable chan ist der Kanal.
sqlTables(chan)
                            TABLE_CAT TABLE_SCHEM TABLE_NAME TABLE_TYPE REMARKS
1 D:\\Lehre\\Statistik3\\Umfrage.xlsx        <NA> 'Sheet 1$'      TABLE    <NA>
Die (einzige) Tabelle in der Datenbank Umfrage hei�t Sheet 1$.
Query-Befehl mit SQL-Befehl. Die Daten werden normalerweise nicht importiert, es gibt nur eine Vern�pfung. Das ist ein Vorteil der Datenbanktechnik, besonders bei gro�en Datenmengen.

> umfr<-sqlQuery(chan, "SELECT * from \"Sheet 1$\" ")
> dim(umfr)
[1] 3471   17
> mode(umfr)
[1] "list"
> class(umfr)
[1] "data.frame"

SQl: Einschr�nkung durch Where Bedingung

> ab1<-sqlQuery(chan, "SELECT * from \"Sheet 1$\" where NETTO > 10000")
> dim(ab1)
[1]  3 17
> ab1
    ID    GESCHL GEBJAHR         BERUFSTAETIG ARBEITSSTD ARZTBES RAUCH GRO GEW     SCHULABSCHLUSS   SCHULABSCHLUSS_V   SCHULABSCHLUSS_M      HOE_ABSCHLUSS
1 2110 MAENNLICH    1972 HAUPTBERUFL.GANZTAGS         50       1    JA 185  84 VOLKS-,HAUPTSCHULE               <NA> VOLKS-,HAUPTSCHULE              LEHRE
2 3364 MAENNLICH    1968 HAUPTBERUFL.GANZTAGS         60       0  NEIN 170  75     HOCHSCHULREIFE FACHHOCHSCHULREIFE VOLKS-,HAUPTSCHULE HOCHSCHULABSCHLUSS
3  744 MAENNLICH    1986 HAUPTBERUFL.GANZTAGS         60       0  NEIN 186  95     HOCHSCHULREIFE VOLKS-,HAUPTSCHULE     MITTLERE REIFE HOCHSCHULABSCHLUSS
       HOE_ABSCHLUSS_V      HOE_ABSCHLUSS_M NETTO ZUFR
1 GEWERBL.,LANDW.LEHRE KAUFMAENNISCHE LEHRE 17500    7
2 FACHHOCHSCHULABSCHL. GEWERBL.,LANDW.LEHRE 60000    9
3   MEISTER, TECHNIKER   MEISTER, TECHNIKER 60000    8
SQL: nur die genannte Attribute (Variable)

> ab2<-sqlQuery(chan, "SELECT ID,GESCHL,GRO,GEW,NETTO from \"Sheet 1$\" where NETTO > 10000")
> ab2
    ID    GESCHL GRO GEW NETTO
1 2110 MAENNLICH 185  84 17500
2 3364 MAENNLICH 170  75 60000
3  744 MAENNLICH 186  95 60000

Anwendung von Aggregatfunctionen (MAX, count, AVG etc)
> sqlQuery(chan, "SELECT count(ID) as Anzahl_Befragte from \"Sheet 1$\" where NETTO > 10000")
  Anzahl_Befragte
1               3

> sqlQuery(chan, "SELECT AVG(NETTO) as Mittel_NETTO from \"Sheet 1$\" where NETTO > 10000")
  Mittel_NETTO
1     45833.33

Gruppierung nach Geschlecht mit GROUP BY
> sqlQuery(chan, "SELECT Geschl,AVG(NETTO) as Mittel from \"Sheet 1$\" group by GESCHL")

     Geschl   Mittel
1 MAENNLICH 1908.214
2  WEIBLICH 1195.946

Gruppierung nach Geschlecht und Geburtsjahr mit GROUP BY
ab4<-sqlQuery(chan, "SELECT Geschl,Gebjahr,AVG(NETTO) as Mittel from \"Sheet 1$\" group by GESCHL,GEBJAHR")
> ab4
       Geschl Gebjahr    Mittel
1   MAENNLICH    1923  940.0000
2   MAENNLICH    1924  520.0000
3   MAENNLICH    1925  645.0000
4   MAENNLICH    1926 1666.6667
5   MAENNLICH    1927  883.2000
6   MAENNLICH    1928 1393.7500
7   MAENNLICH    1929 1509.5714
8   MAENNLICH    1930 1645.7143
9   MAENNLICH    1931 1051.4000
10  MAENNLICH    1932 1045.0000
11  MAENNLICH    1933 1705.2222
12  MAENNLICH    1934 1635.3333
13  MAENNLICH    1935 1524.5833
14  MAENNLICH    1936 1727.5556
15  MAENNLICH    1937 1328.0000
16  MAENNLICH    1938 1736.0000
17  MAENNLICH    1939 1459.0556
18  MAENNLICH    1940 1591.9000
19  MAENNLICH    1941 2079.5238
20  MAENNLICH    1942 1242.0000
21  MAENNLICH    1943 1742.6400
22  MAENNLICH    1944 2182.7778
23  MAENNLICH    1945 2178.5714
24  MAENNLICH    1946 1345.5000
25  MAENNLICH    1947 1234.0667
26  MAENNLICH    1948 1175.4615
27  MAENNLICH    1949 1437.7391
28  MAENNLICH    1950 1683.5000
29  MAENNLICH    1951 1841.3889
30  MAENNLICH    1952 2073.0400
31  MAENNLICH    1953 2328.5769
32  MAENNLICH    1954 2008.0952
33  MAENNLICH    1955 1854.6000
34  MAENNLICH    1956 1835.8750
35  MAENNLICH    1957 2103.4800
36  MAENNLICH    1958 2050.3056
37  MAENNLICH    1959 2416.4516
38  MAENNLICH    1960 2013.9024
39  MAENNLICH    1961 1973.5714
40  MAENNLICH    1962 2298.6364
41  MAENNLICH    1963 2136.1481
42  MAENNLICH    1964 1813.2667
43  MAENNLICH    1965 2226.0238
44  MAENNLICH    1966 2110.8378
45  MAENNLICH    1967 1839.4000
46  MAENNLICH    1968 4388.0714
47  MAENNLICH    1969 1911.5417
48  MAENNLICH    1970 2662.3478
49  MAENNLICH    1971 2560.0000
50  MAENNLICH    1972 2681.9048
51  MAENNLICH    1973 1804.0000
52  MAENNLICH    1974 1440.3333
53  MAENNLICH    1975 1866.7500
54  MAENNLICH    1976 2383.1818
55  MAENNLICH    1977 2049.4118
56  MAENNLICH    1978 1866.4815
57  MAENNLICH    1979 1595.8095
58  MAENNLICH    1980 1694.2500
59  MAENNLICH    1981 1956.2000
60  MAENNLICH    1982 1855.4595
61  MAENNLICH    1983 1968.4375
62  MAENNLICH    1984 1864.4286
63  MAENNLICH    1985 2079.0909
64  MAENNLICH    1986 4099.3077
65  MAENNLICH    1987 1408.7037
66  MAENNLICH    1988 1493.0769
67  MAENNLICH    1989 1888.2632
68  MAENNLICH    1990 1278.4211
69  MAENNLICH    1991  859.2500
70  MAENNLICH    1992 1420.2500
71  MAENNLICH    1993  874.5357
72  MAENNLICH    1994 1049.6471
73  MAENNLICH    1995 1148.2609
74   WEIBLICH      NA  958.6667
75   WEIBLICH    1922  450.0000
76   WEIBLICH    1923  450.0000
77   WEIBLICH    1926  550.0000
78   WEIBLICH    1927  580.0000
79   WEIBLICH    1928  916.2857
80   WEIBLICH    1929  966.6667
81   WEIBLICH    1930  856.2500
82   WEIBLICH    1931  990.0000
83   WEIBLICH    1932  932.3333
84   WEIBLICH    1933  859.9375
85   WEIBLICH    1934  788.6000
86   WEIBLICH    1935 1268.1429
87   WEIBLICH    1936 1090.6667
88   WEIBLICH    1937  851.2500
89   WEIBLICH    1938 1151.4167
90   WEIBLICH    1939 1003.0588
91   WEIBLICH    1940  985.5556
92   WEIBLICH    1941 1011.8889

Die Ausgabe der Datenbankabfrage entspricht dem SQL-Standard, sollte in R direkt weiterbearbeitet werden mit tapply.


> erg<- tapply(Mittel, list(Gebjahr, Geschl), mean) ; erg    # gek�rzt
     MAENNLICH  WEIBLICH
1922        NA  450.0000
1923  940.0000  450.0000
1924  520.0000        NA
1925  645.0000        NA
1926 1666.6667  550.0000
1927  883.2000  580.0000
1928 1393.7500  916.2857
1929 1509.5714  966.6667
1930 1645.7143  856.2500
1931 1051.4000  990.0000
1932 1045.0000  932.3333
1933 1705.2222  859.9375
1934 1635.3333  788.6000
1935 1524.5833 1268.1429
1936 1727.5556 1090.6667
1937 1328.0000  851.2500
1938 1736.0000 1151.4167
1939 1459.0556 1003.0588
1940 1591.9000  985.5556
1941 2079.5238 1011.8889
1942 1242.0000 1400.0000
1943 1742.6400 1166.0400
1944 2182.7778  937.1667
1945 2178.5714  875.0000
1946 1345.5000 1064.6842
1947 1234.0667  604.8571
1948 1175.4615  790.9333
1949 1437.7391  877.8571
1950 1683.5000  918.5833
1951 1841.3889 1310.0000
1952 2073.0400 1635.2069
1989 1888.2632 1082.1765
1990 1278.4211  919.5238
1991  859.2500 1089.5000
1992 1420.2500  967.2727
1993  874.5357  698.8421
1994 1049.6471  700.7857
1995 1148.2609  754.4211


R-Plot nur Frauen nach Geburtsjahr

> Gebjahr<-as.numeric(dimnames(erg)[[1]])
> plot(erg$WEIBLICH ~ Gebjahr, type="l", col="Pink")

F�r noch sch�nere Graphik Datenexport nach Excel.
> write.csv2(erg, file="netto_geb.csv")


Zusammenfassung der Arbeitsschritte:
* Verbindung zur Datenbank (Exceltabelle)
* SQL-Abfrage auf Tabelle in Datenbank
* Client-Server-Technik
* Kreuztabelle auf den Datenbank-Output mit tapply
* Export nach Excel
* Liniengraphik mit �berschrift, Abszisse Geburtsjahr
Wichtig: Zuletzt im R-System Verbindung zur Datenbank beenden
odbcCloseAll()
Stundenaufgabe:
Auswertung mit Datenbank der Arztbesuche nach Geschlecht und Geburtsjahr. Graphische Darstellung mit Excel oder R.
 

9 Explorative Datenanalysen (EDA) 
Die explorative Datenanalyse (EDA) oder explorative Statistik ist ein Teilgebiet der Statistik. Sie untersucht und begutachtet Daten, von denen nur ein geringes Wissen �ber deren Zusammenh�nge vorliegt. Viele EDA-Techniken werden im Data-Mining eingesetzt. Au�erdem werden sie h�ufig in Statistik-Veranstaltungen als Einf�hrung in das statistische Denken gelehrt (wiki).
9.1 Datenkontrolle durch Graphik, BSR-Daten von 1994
Es fehlen noch Programme f�r Klassifikation von stetigen Merkmale, f�r diskrete Variable vorhanden
Testen mit Variable text.

text<-unlist(strsplit("eins zwei drei zwei drei eins zwei", " "))

function strsplit                        # Trennzeichen blanks
Split the elements of a character vector x into substrings according to the matches to substring split within them.

Die eingebaute R-Funktion table. Ermittelt die Anzahl.

>table(text)
text
drei eins zwei 
   2    2    3 
Das eigene Programm freq.count f�r continuierliche Variable 
Zuerst Prototyping. Test mit BSR-Originaldaten.
> bsr<-read.table("bsrorg.txt", header=T)

> dim(bsr)
[1] 15580     7

> head(bsr,3)
   tag zeit   Mg    kfznr bsrkey  Laga P
1 2609  514 0.76 B-EE2388    UNB 91200 A
2 2609  536 6.38 B-CX2104    UNB 91200 A
3 2609  542 2.28  B-CR332    UNB 91200 A

> (vek<-bsr$Mg[1:13])
 [1] 0.76 6.38 2.28 1.00 2.36 0.98 6.80 2.96 4.52 1.36 4.04 2.70 2.94

# Intervalluntergrenze = 0, Intervallbreite = 1, gesucht Intervallzahl

> max(vek)     # 7 Intervalle bei Intervallbreite 1
[1] 6.8

# Aufrunden Daten minus Intervalluntergrenze, geteilt durch Intervallbreite

> (tmp<-ceiling((vek-0)/1))                #Funktion Aufrunden
 [1] 1 7 3 1 3 1 7 3 5 2 5 3 3

# Outer Produkt (Algorithmus) Intervalle und Fallzahl in den Intervallen 

> 1*outer((1:7), tmp, "==")
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
[1,]    1    0    0    1    0    1    0    0    0     0     0     0     0
[2,]    0    0    0    0    0    0    0    0    0     1     0     0     0
[3,]    0    0    1    0    1    0    0    1    0     0     0     1     1
[4,]    0    0    0    0    0    0    0    0    0     0     0     0     0
[5,]    0    0    0    0    0    0    0    0    1     0     1     0     0
[6,]    0    0    0    0    0    0    0    0    0     0     0     0     0
[7,]    0    1    0    0    0    0    1    0    0     0     0     0     0

# Summe �ber Spalten

> rowSums(outer((1:7), tmp, "=="))
[1] 3 1 5 0 2 0 

> freq.cont                             # eigenes Program
function(dat, vek)
{
# H�ufigkeitsberechnung f�r metrische Variablen (Klassen)
# vek[1] Untergrenze, vek[2] Intervallbreite, vek[3] Intervallzahl
	tmp <- ceiling((dat - vek[1])/vek[2])	# Standardisierung 
	res <- rowSums(outer((1:vek[3]), tmp, "==")) # outer product
	res }
Testen des Programms mit Vektor vek
> freq.cont(vek, c(0,1,7))
[1] 3 1 5 0 2 0 2
> sum(freq.cont(vek, c(0,1,7)))
[1] 13
> sum(freq.cont(vek, c(0,1,7))) == length(vek)
[1] TRUE

Anwendung von Programm freq.cont auf BSR-daten bsr

> anz<-freq.cont(bsr$Mg, c(0,1,32))

> anz
 [1] 2798 2051 1765 1245 1101 1083 1127  962  791  728  501  184   64   25   16   12  106   11  178   73   86  106   80
[24]   99  113  127   66   41   23    7    6    1

# Hinzuf�gen von Labels f�r Graphik

names(anz)<-c(1:32)           # Intervallbezeichnung

> head(anz)                      # mit Intervallbezeichnung
   1    2    3    4    5    6 
2798 2051 1765 1245 1101 1083

# andere Methode f�r Anzahl mit Aufrunden und Funktion table, wenn die Intervallbreite 1 sein soll. Nicht immer geeignet, geht nur bei Klassenbreite 1.

> anz<-table(ceiling(dat$Mg))
> length(anz)
[1] 33

barplot(gew, col=rainbow(33))
title("Zahl der Anlieferungen im Herbst 1994", sub = "Daten von BSR")



(anz<-freq.cont(bsr$zeit/100), c(0,1,24))
 [1]    2    0    1    0    0   78  655  747 1262 2181 1964 2645 2213 1155  793
[16]  569  342  303  327  311   27    0    1    4

> names(anz)<-c(1:24)
title("Anlieferungszahl nach Zeit 0 bis 24 Uhr", sub = "Daten von BSR")

# Blitzauswertung mit Histogramm
Ein Histogramm ist eine graphische Darstellung der H�ufigkeitsverteilung metrisch skalierter Merkmale. Es erfordert die Einteilung der Daten in Klassen (engl. bins), die eine konstante oder variable Breite haben k�nnen (wiki).
hist(dat$Mg, breaks=32, col = "lightblue", border = "pink")
Eine andere Variante mit Klassenbreite 1 und Erzeugung einer Liste durch split
(anz<-sapply(split(s<-ceiling(bsr$Mg),s),length))   # anzahl
   
   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17 
   4 2798 2051 1765 1245 1101 1083 1127  962  791  728  501  184   64   25   16   12  106 

  18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 
  11  178   73   86  106   80   99  113  127   66   41   23    7    6    1

head(anz<sapply(split(rep(1,(dim(bsr)[1])),ceiling(bsr$Mg)),sum))
   0    1    2    3    4    5                    # Einsen gruppiert
   4 2798 2051 1765 1245 1101

Auswertung Wochentag
Umkodieren Datum -> Wochentag, Jahr 1994
bsr$datum enth�lt Position und Information, deshalb Bitmatrix aufspannen mit function outer.

> bit<-outer(bsr$tag, unique(bsr$tag), FUN="==") ; dim(bit)
[1] 15580    11
> head(bit)
     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]
[1,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[2,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[3,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[4,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[5,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[6,] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE

> Wochentage                              # entspricht Datum
 [1] 1 2 3 4 5 6 2 3 4 5 6
> codm<-Wochentage*t(bit) ; dim(codm)     # Multiplikation
[1]    11 15580
> fact<-apply(codm,2,sum) ; length(fact)  # fact nun Wochentag
[1] 15580
> erg<-tapply(dat$Mg, list(fact), FUN=sum) # Summe Wochentag
> names(erg)<-c("Montag","Dienstag","Mittwoch","Donnerstag","Freitag","Sonnabend") ; erg
   Montag   Dienstag   Mittwoch Donnerstag    Freitag  Sonnabend 
   9355.77   19934.63   18717.72   16681.25   16529.91    4845.65
> mittel<-erg/table(Wochentage) ; mittel    # mean pro Woche
    Montag   Dienstag   Mittwoch Donnerstag    Freitag  Sonnabend 
  9355.770   9967.315   9358.860   8340.625   8264.955   2422.825 

> barplot(mittel, col="Green")
> title("Abfallmenge pro Tag 1994 in Mg", sub="Daten BSR")



Weitere Plausibilit�tskontrollen
Die Entdeckung der Fehler in den Datens�tzen z.B. Mehrfachf�lle (Doubletten) oder aber gibt es Messwerte kleiner oder gleich 0 etc? Die Messwerte wurde durch Verwiegung automatisch gemessen, doch es gibt die M�glichkeit von Manupulation bzw. Betrug oder technischer Fehler.
9.2 Standard R-Variante f�r Entdeckung schlechter F�lle 
# Funktion duplicated, Erg�nzungsfunktion zu unique
Beispiel mit Vektor text
text<-c("eins","zwei","drei")[sample(3,15,repl=T)]

> mode(text)
[1] "character"

> length(text)
[1] 15

> unique(text)
[1] "zwei" "eins" "drei"

> text[duplicated(text)]                      # die Mehrfachf�lle                    
 [1] "zwei" "eins" "eins" "eins" "eins" "drei" "drei" "zwei" "drei" "zwei" "eins" "eins"
> length(text[duplicated(text)])
[1] 12
text ohne die Duplikate
> text[!duplicated(text)]
[1] "zwei" "eins" "drei"

1. Frage: Ist �eins� in text enthalten?               # Mengenlehre
> "eins" %in% text
[1] TRUE

2. Frage: Wo ist "eins" in text enthalten?

> (1:15)[text %in% "eins"]                            #match, Index
[1]  2  4  5  6  7 14 15

> text[text %in% "eins"]
[1] "eins" "eins" "eins" "eins" "eins" "eins" "eins"   #7 mal eins
9.3 BSR-Datensatz ohne Wiederholungen und Fehler
> dim(bsr)                              # Datensatz bsrorg.txt
[1] 15580     7
# bsrorg<-bsr                           # Kopie sichern

> bsr<-bsr[!duplicated(bsr),]             # ohne Mehrfachf�lle
> dim(bsr)
[1] 15405     7

# die Mehrfachf�lle                       
> dup<-bsrorg[duplicated(bsrorg),]
> dim(dup)
[1] 175   7

> head(dup)
      tag zeit    Mg   kfznr bsrkey  Laga P
570  2609  725  5.76 B-S1759    UNB 91401 C
3149 2609 1319 18.94     UNB    UNB 91101 X
3154 2609 1332 18.94     UNB    UNB 91101 X
3155 2609 1332 18.94     UNB    UNB 91101 X
3160 2609 1343 18.94     UNB    UNB 91101 X
3162 2609 1344 18.94     UNB    UNB 91101 X

Wie gro� war der Fehler?
> sum(dup$Mg)
[1] 3252.37
> 100*sum(dup$Mg)/sum(bsrorg$Mg)           #Gewicht 3,7 % zu hoch
[1] 3.778972
> sum(dup$Mg) * 26                         # Wochen
[1] 84561.62                               # Fehler Total pro Jahr

Gibt es unplausible Werte?
> summary(dat$Mg)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   1.500   3.960   5.524   7.520  31.300

Weitere M�glichkeit boxplot.
Korrektur, da das Gewicht=0 oder kleiner 0 unplausibel ist

# ohne F�lle Gewicht <= 0
> sum(bsr$Mg<=0)
[1] 4

> sum(0<bsr$Mg)
[1] 15401

> bsr[bsr$Mg==0,]
      tag zeit Mg    kfznr bsrkey  Laga P
598  2609  835  0  B-S3329    UNB 91401 C
2613  610 1112  0  B-S3329    UNB 91701 C
5642 2809  739  0 B-AL8235   SFNG     0 N
6007 2909  715  0 B-AS2288   SFNG     0 N

# die finale Korrektur     

> bsr<-bsr[0<bsr$Mg,]

> dim(bsr)                  # nun ohne Mehrfachf�lle und Nuller
[1] 15401     7
9.3 Vergleichende graphische Analyse von Anzahl und Gewicht
(anz<-freq.cont(bsr$Mg,c(0,1,32))
   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 
2798 2051 1765 1245 1101 1082 1127  962  791  728  501  184   64   25   16   12 
  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 
  56   11  107   54   69   89   80   99  113  127   66   41   23    7    6    1

# Die Funktion cumsum

> cumsum(1:5)
[1]  1  3  6 10 15

# Kumulierte Anlieferungen

> cum.anz<-cumsum(anz)
# Bildschirm als 1 x 2 Matrix definieren

> par(mfrow=c(1,2))                           # eine Zeile, 2 Spalten

# Zwei Graphiken nacheinander erzeugen plus Titel

> barplot(anz, col=rainbow(32))

> title("Zahl der Anlieferungen im Herbst 1994")

> barplot(cum.anz, col=rainbow(32))

> title("Kumulierte Zahl der Anlieferungen im Herbst 1994")




Zus�tzlich soll das Gewicht der Anlieferungen abgebildet werden

sum.cont                             
function (dat,vek) 
{                          # Gewicht in Klassen summiert
# Total f�r metrische Variablen mit split
# vek[1] Untergrenze, vek[2] Intervallbreite, vek[3] Intervalle
tmp <- ceiling((dat - vek[1])/vek[2])# Standardisierung 
res <- outer(tmp,1:vek[3], FUN= "==")  # outer product
res<-colSums(res*dat)                  # Produkt Daten x Bitmatrix
names(res)<-c(1:vek[3])                # Ergebnis mit labels
res } 

Alternativ kann hierf�r das R-eingebaute Programm tapply verwendet werden.
9.4 Vergleich Anzahl-Gewicht in einer Graphik
# Total in Gewichtsklassen 0 bis 32, Breite 1
> gew<-sum.cont(bsr$Mg, c(0,1,32)) oder
> gew<-tapply(bsr$Mg, ceiling(bsr$Mg), sum) 
> cum.gew<-cumsum(gew)

# zuerst Plotparameter einstellen, Graphikoutput formatieren
> par(mfrow=c(2,2))

# 4 plots erzeugen

> barplot(anz, col=rainbow(32))
> title("Zahl der Anlieferungen im Herbst 1994")

> barplot(cum.anz, col=rainbow(32))
> title("Kumulierte Anlieferungen im Herbst 1994")

> barplot(gew, col=rainbow(32))
> title("Abfallgewicht in Gewichtsklassen im Herbst 1994")

> barplot(cum.gew, col=rainbow(32))
> title("Kumuliertes Abfallgewicht in Gewichtsklassen 1994")



9.4 Boxplots mit BSR-Daten
boxplot(bsr$Mg ~ bsr$bsrkey)
title("Boxplot Abfallgwicht nach Bsrkey", sub="BSR-Daten Herbst 1994")


Vergleichende Boxplots nur mit Bsrkey=HM und IND
> as.matrix(summary(bsr$bsrkey))
      [,1]
AC      479
APC     261
BSA      36
GUL     214
HM     5486
HOSUS     2
HOSUSE   13
IBES      2
IKHH     74
IND    4205
INDGAS  112
INS      36
KEH     540
LAU       3
MAR      27
MOEBEL    4
PAP       4
RBMAE     2
SBND      1
SBNG      1
SBNK      2
SBNN     33
SBNP      1
SBNR      3
SBNS      1
SFND      1
SFNE      5
SFNG      6
SFNN     25
SFNS     17
SFNV      2
SPM     965
UNB    2838 

> boxplot(bsr$Mg[bsr$bsrkey=="HM"], bsr$Mg[bsr$bsrkey=="IND"], 
names=c("HM", "IND"))
title("Boxplot Abfallgewicht Hausm�ll und Gerwerbeabfall Herbst 1994")

Stundenaufgabe: vergleichende Darstellung von HM, AC und APC
10 Eier und H�hner
10.1 Datenerhebung Eier

Der pensionierte Statistiker Martin hat im Urlaub auf dem Dorfe 72 Eier von 3 H�hnerh�fen gesammelt, sie einzeln gewogen und L�nge wie Breite gemessen. Unter �eier.csv� sind die Daten im csv-Format (comma-separeted-value) gespeichert.

Wir importieren und speichern sie unter dem Variablenamen
�eier�.
>eier<-read.csv2("eier.csv")

Als n�chstes fragen wir einige formale Eigenschaften der importierten Datei ab.

>dim(eier)[1] 72 4 # Spalten und Zeilen

class(eier)
[1] data.frame

Die Daten sind als sogenannter Data-Frame gespeichert.
Wir lassen die ersten drei Zeilen der Datei anzeigen:

>head(eier,3)
  Gewicht Laenge Breite Hof
1      64   58.5   44.0   1
2      62   57.6   43.5   1
3      58   59.5   41.9   1

Jede Spalte in diesem Dataframe definiert eine eigene Variable,
die durch ihren Namen ansprechbar und aufrufbar ist.

Der Befehl
>summary(eier)
liefert die Extremwerte, Mediane, Mittelwerte sowie erste
und dritte Quantile der Variablen Gewicht, Laenge, Breite und Hof
> summary(eier)
    Gewicht          Laenge          Breite           Hof      
 Min.   :44.00   Min.   :51.40   Min.   :40.00   Min.   :1.00  
 1st Qu.:56.75   1st Qu.:55.00   1st Qu.:42.52   1st Qu.:1.00  
 Median :60.50   Median :56.85   Median :43.50   Median :1.50  
 Mean   :60.17   Mean   :56.80   Mean   :43.44   Mean   :1.75  
 3rd Qu.:64.25   3rd Qu.:58.52   3rd Qu.:44.62   3rd Qu.:2.25  
 Max.   :70.00   Max.   :63.90   Max.   :46.30   Max.   :3.00

Mit dem Kommando
>attach(eier)
sagen wir dem Programm, dass wir uns hier nur mit dem Eierdatensatz befassen wollen. 

Der Befehl
>sd(Laenge)
liefert die Standardabweichung und

>var(Laenge) die Varianz,
mit
>round(var(Laenge),2) erhalten wir sie auf zwei Kommastellen gerundet. 

Dabei ist zu beachten ist, dass auch R Gro�- und Kleinschreibung unterscheidet, also var ist nicht Var. Mit dem doppelten Gleichheitszeichen �berpr�fen wir, ob die Standardabweichung gleich der Wurzel der Varianz ist.

>sqrt(var(Gewicht))==sd(Gewicht)
Darauf antwortet R mit
>TRUE

Wir veranschaulichen die Daten in Boxplots:
>boxplot(eier[,1:3], col=c(8,2,3))
>title("H�hnereier, Gewicht, L�nge, Breite")

Die Angabe col legt die Farben (colour) f�r die drei Boxplots
fest, durch die Angabe c(8,2,3), (c wie �combine�)
werden die Farbangaben 8, 2 und 3 in einem Vektor zusammengefasst.

Wenn wir mehr �ber Boxplots wissen wollen, fragen wir nur
>??boxplot oder >help(boxplot)

In diesen Boxplot sind zwei Ausrei�er erkennbar. Ein Wert liegt bei Gewicht und Laenge jeweils au�erhalb des Kastens.

Durch
>hist(Gewicht)
erhalten wir ein Histogramm f�r das Merkmal Gewicht.

Optionen �ber die Wahl von Klassen, Farbe, Balkenbreite
usw. k�nnen wir wieder �ber help(hist) erfragen.
10.2 Empirische Verteilungsfunktion
Die empirische Verteilungsfunktion ecdf erzeugen wir aus den Originaldaten mit

>plot(ecdf(Gewicht),verticals=TRUE,
do.points=FALSE)

help(ecdf)
Compute an empirical cumulative distribution function, with several methods for plotting, printing and computing with such an �ecdf� object.
Die empirische Verteilungsfunktion � z.B. F(x) � gibt den kumulierten Anteil an, mit der ein Merkmal eine Auspr�gung bzw. einen Wert <= x annimmt. Diese kumulierte absolute oder relative H�ufigkeit kann ggfs. bereits der H�ufigkeitstabelle entnommen werden.
Typische Fragestellungen w�ren:
Wie viele Arbeitnehmer eines Unternehmens sind maximal 30 Jahre alt? (f�r ein metrisches Merkmal wie das Alter).
Wie viele Mensabesucher bewerten das Essen zumindest mit "gut"? (bei einer Ordinalskala z.B. mit den Werten "sehr gut", "gut", "geht so" und "schlecht").
Die Aufstellung einer empirischen Verteilungsfunktion setzt zumindest ordinalskalierte Daten voraus (nominalskalierte Merkmalsauspr�gungen wie "blond" und "rot" f�r die Haarfarbe k�nnen nicht sinnvoll kumuliert / aufaddiert werden).

Auf Grund der Option verticals=TRUE werden die senkrechten Sprunglinien der Verteilungsfunktion gezeichnet, do.points=FALSE verzichtet auf eine Hervorhebung der Sprungstellen.
10.3 Gibt es Doubletten?

Zum Schluss �berpr�fen wir die Daten auf doppelte Eintr�ge
(gleicht ein Ei dem anderen?).


Wir verwandeln dazu die Zeilen des Data.frame in eine Liste mit 72 Elementen. Der Dataframe wird zuerst transponiert und danach mit der
Funktion split als Liste angeordnet.

>eier.list<-split(tmp<-t(eier[,1:3]), col(tmp))

> head(eier.list,3)
$`1`
[1] 64.0 58.5 44.0

$`2`
[1] 62.0 57.6 43.5

$`3`
[1] 58.0 59.5 41.9

In dieser Liste suchen wir mit der Funktion duplicated eventuell mehrfach auftretende Elemente und speichern sie unter dem Namen dop.ei
>dop.ei<-eier.list[duplicated(eier.list)]

Rufen wir dop.ei auf, erhalten wir
>$�63� [1] 62.0 55.0 44.5

Also tritt das 63. Element der Liste eier.list mit den Auspr�gungen 62.0 55.0 44.5 mehrfach auf. 

Mit der R-Funktion match(%in%)
>eier.list %in% dop.ei
suchen wir in der Liste eier.list nach allen Elementen, die mit dop.ei �bereinstimmen. Das Ergebnis ist ein logischer Vektor mit 70 FALSE und 2 TRUE Elementen. Mit diesem Vektor selektieren wir 2 Elemente des urspr�nglichen eier-Dataframe.

>eier[(eier.list %in% dop.ei),]

Folgende Antwort nennt die Indizes der beiden Eier und ihre identischen
Merkmale.
> eier[eier.list %in% dop.ei,]
   Gewicht Laenge Breite Hof
57      62     55   44.5   3
63      62     55   44.5   3

Auswertung nach Bauernhof
Anzahl, Mittelwert und Standardabweichung nach Hof
> tapply(Gewicht, Hof, length)
 1  2  3                             # Anzahl durch Funktion length
36 18 18 

> tapply(Gewicht, Hof, mean)             # Mittel
       1        2        3 
61.47222 57.11111 60.61111 

> tapply(Gewicht, Hof, sd)             # Standardabweichung
       1        2        3 
5.495380 6.360622 4.132210

Sind die Unterschiede zwischen den H�fen signifikant?
Stundenaufgaben:
* Erstellen Sie die Korrelationsmatrix von eier ohne die letzte Spalte (Hof).
* Erstellen Sie eine Liniengraphik Gewicht ~ Breite
* Verwenden Sie f�r Regression die Funktion lm
* Interpretieren Sie das Ergebnis
10.3 K�ken und H�hner
Datensatz ChickWeight ist vorhanden.
> data(ChickWeight)
> H�hner <- ChickWeight
> str(H�hner)                                    # Struktur
Classes �nfnGroupedData�, �nfGroupedData�, �groupedData� and 'data.frame':      578 obs. of  4 variables:
 $ weight: num  42 51 59 64 76 93 106 125 149 171 ...
 $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...
 $ Chick : Ord.factor w/ 50 levels "18"<"16"<"15"<..: 15 15 15 15 15 15 15 15 15 15 ...
 $ Diet  : Factor w/ 4 levels "1","2","3","4": 1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, "formula")=Class 'formula'  language weight ~ Time | Chick
  .. ..- attr(*, ".Environment")=<environment: R_EmptyEnv> 
 - attr(*, "outer")=Class 'formula'  language ~Diet
  .. ..- attr(*, ".Environment")=<environment: R_EmptyEnv> 
 - attr(*, "labels")=List of 2
  ..$ x: chr "Time"
  ..$ y: chr "Body weight"
 - attr(*, "units")=List of 2
  ..$ x: chr "(days)"
  ..$ y: chr "(gm)"

> attach(H�hner)
> head(H�hner)
  weight Time Chick Diet
1     42    0     1    1
2     51    2     1    1
3     59    4     1    1
4     64    6     1    1
5     76    8     1    1
6     93   10     1    1
Die H�hner wurde zu verschiedenen Zeiten unterschiedlich gef�ttert (Diet).
> table(Time)
Time
 0  2  4  6  8 10 12 14 16 18 20 21 
50 50 49 49 49 49 49 48 47 47 46 45 

> table(Chick)
Chick
18 16 15 13  9 20 10  8 17 19  4  6 11  3  1 12  2  5 14  7 24 30 22 23 27 28 26 25 29 21 33 37 36 31 39 38 32 40 34 35 44 45 43 41 
 2  7  8 12 12 12 12 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 10 12 12 12 
47 49 46 50 42 48 
10 12 12 12 12 12 

> table(Diet)
Diet
  1   2   3   4 
220 120 120 118
Zun�chste Klassierung des Gewichts nach Time.
> h<-tapply(weight, list(Time), FUN="mean")
> h
        0         2         4         6         8        10        12        14        16        18        20        21 
 41.06000  49.22000  59.95918  74.30612  91.24490 107.83673 129.24490 143.81250 168.08511 190.19149 209.71739 218.68889 

> matrix(h)
           [,1]
 [1,]  41.06000
 [2,]  49.22000
 [3,]  59.95918
 [4,]  74.30612
 [5,]  91.24490
 [6,] 107.83673
 [7,] 129.24490
 [8,] 143.81250
 [9,] 168.08511
[10,] 190.19149
[11,] 209.71739
[12,] 218.68889
Andere Variante mit function aggregate. Help(aggregate).
> aggregate(weight~Time, data = H�hner, FUN = "mean")
   Time    weight
1     0  41.06000
2     2  49.22000
3     4  59.95918
4     6  74.30612
5     8  91.24490
6    10 107.83673
7    12 129.24490
8    14 143.81250
9    16 168.08511
10   18 190.19149
11   20 209.71739
11   21 218.68889

Ein �hnliches Ergebnis bekommt man mit
> cbind(unique(Time),tapply(weight, Time, mean))
   [,1]      [,2]
0     0  41.06000
2     2  49.22000
4     4  59.95918
6     6  74.30612
8     8  91.24490
10   10 107.83673
12   12 129.24490
14   14 143.81250
16   16 168.08511
18   18 190.19149
20   20 209.71739
21   21 218.68889

Regressionsanalyse
Regression  weigth ~ Time
> erg<-lm(weight ~ Time)
> erg

Call:
lm(formula = weight ~ Time)

Coefficients:
(Intercept)         Time  
     27.467        8.803  

> summary(erg)

Call:
lm(formula = weight ~ Time)

Residuals:
     Min       1Q   Median       3Q      Max 
-138.331  -14.536    0.926   13.533  160.669 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  27.4674     3.0365   9.046   <2e-16 ***
Time          8.8030     0.2397  36.725   <2e-16 ***
---
Signif. codes:  0 �***� 0.001 �**� 0.01 �*� 0.05 �.� 0.1 � � 1

Residual standard error: 38.91 on 576 degrees of freedom
Multiple R-squared:  0.7007,    Adjusted R-squared:  0.7002 
F-statistic:  1349 on 1 and 576 DF,  p-value: < 2.2e-16
> aggregate(weight~Diet, data = H�hner, FUN = "mean")
  Diet   weight
1    1 102.6455
2    2 122.6167
3    3 142.9500
4    4 135.2627

> help(Chick.Weight)
> coplot(weight ~ Time | Chick, data = ChickWeight,
       type = "b", show.given = FALSE)


> unique(Chick)
 [1] 1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43
[44] 44 45 46 47 48 49 50
50 Levels: 18 < 16 < 15 < 13 < 9 < 20 < 10 < 8 < 17 < 19 < 4 < 6 < 11 < 3 < 1 < 12 < 2 < 5 < 14 < 7 < 24 < 30 < 22 < 23 < ... < 48

50 H�hner
> plot(weight ~ Time, type="l", col='Blue')
> title("Gewicht in Gramm von H�hnern gegen Lebenszeit in Tagen", sub="Datensatz ChickWeight")

11. Der praktische Dataframe und seine subsets
11.1 Data.frame versus Matrix und array
Im Unterschied zu einer Matrix k�nnen in einem Data-Frame Objekte von unterschiedlichem Datentyp, z.B. numeric und factor zusammengef�hrt werden.
> help(data.frame)
The function data.frame() creates data frames, tightly coupled collections of variables which share many of the properties of matrices and of lists, used as the fundamental data structure by most of R's modeling software.
> mat<-matrix(seq(1,12), c(3,4))�; mat       # nicht immer nur 1:12
     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12

> mode(mat)                     # homogen numerisch (oder character)
[1] "numeric"
Selektive Zuweisung von Daten oder update mit der Funktion replace.
> mat<6                                # logische Abfrage
     [,1]  [,2]  [,3]  [,4]
[1,] TRUE  TRUE FALSE FALSE
[2,] TRUE  TRUE FALSE FALSE
[3,] TRUE FALSE FALSE FALSE

> mat[mat<6]<-55   ; mat               # selektive Zuweisung mit 55
     [,1] [,2] [,3] [,4]
[1,]   55   55    7   10
[2,]   55   55    8   11
[3,]   55    6    9   12

> mat==55                              # logische Abfrage
     [,1]  [,2]  [,3]  [,4]
[1,] TRUE  TRUE FALSE FALSE
[2,] TRUE  TRUE FALSE FALSE
[3,] TRUE FALSE FALSE FALSE
Andere Variante durch function replace        # ohne Klammern

> mat<-replace(mat, mat==55, 99)  ; mat
     [,1] [,2] [,3] [,4]
[1,]   99   99    7   10
[2,]   99   99    8   11
[3,]   99    6    9   12


Update Kopfzeile, Anwendung der function strsplit.
Selektive Zuweisung auf dimnames von mat.

> dimnames(mat)[[2]]<-unlist(strsplit("IMI ATA PERSIL TANDIL"," ")) 

> dimnames(mat)[[1]]<-c("eins","zwei","drei") ; mat
     IMI ATA PERSIL TANDIL
eins  99  99      7     10
zwei  99  99      8     11
drei  99   6      9     12

Mittel �ber die Zeilen.
> apply(mat, 2, mean)
   IMI    ATA PERSIL TANDIL 
    99     68      8     11
Diese Matrix soll nun zu einer Datenmatrix erweitert und zu einem data.frame umgewandelt werden. Hierf�r Ziehung einer Zufallsstichprobe.

> stich<-sample(3,45,repl=T) ; length(stich)
[1] 45
Es werden durch function repeat 45 (3x15) Zeilen f�r mat erzeugt.

>  tab<-apply(mat, 2, FUN=rep,15) ;  class(tab) ; dim(tab)
[1] "matrix"
[1] 45  4

> head(tab)
     IMI ATA PERSIL TANDIL
eins  99  99      7     10
zwei  99  99      8     11
drei  99   6      9     12
eins  99  99      7     10
zwei  99  99      8     11
drei  99   6      9     12
Definition von 2 neuen Variablen Fewa und Auto.
> Fewa<-5:7/7   ; Fewa
[1] 0.7142857 0.8571429 1.0000000

> Fewa<-Fewa[stich]        ; length(Fewa)
[1] 45

> Auto<-c("Wartburg 311", "VW K�fer 1200", "Ente 2CV4")

> Auto<-Auto[stich]      ; length(Auto)
[1] 45
Fewa und Auto sollen als neue Spalten mit tab verbunden werden.

> tabn<-cbind(tab, Fewa)  ; class(tabn) ; dim(tabn)
[1] "matrix"
[1] 45  5

> mode(tabn[,"Fewa"])                       # Datentyp?
[1] "numeric"

> head(tabn,3)
     IMI ATA PERSIL TANDIL      Fewa
eins  99  99      7     10 1.0000000
zwei  99  99      8     11 1.0000000
drei  99   6      9     12 0.8571429

> unique(tabn[,"Fewa"])
[1] 1.0000000 0.8571429 0.7142857

> tabn<-cbind(tabn, Auto)  ; class(tabn) ; dim(tabn)
[1] "matrix"
[1] 45  6

> mode(tabn[,"Auto"])                      # nicht numerisch
[1] "character"

> head(tabn,3)                            # ganze Matrix jetzt text
     IMI  ATA  PERSIL TANDIL Fewa                Auto           
eins "99" "99" "7"    "10"   "1"                 "Ente 2CV4"    
zwei "99" "99" "8"    "11"   "1"                 "Ente 2CV4"    
drei "99" "6"  "9"    "12"   "0.857142857142857" "VW K�fer 1200"

> class(tabn)                        # matrix homogen
[1] "matrix"
Leider wurde alles in text umgewandelt. Also -> data.frame.

> data<-as.data.frame(tabn)   ; class(data)     # nun data.frame
[1] "data.frame"

> head(data,3)
     IMI ATA PERSIL TANDIL              Fewa          Auto
eins  99  99      7     10                 1     Ente 2CV4
zwei  99  99      8     11                 1     Ente 2CV4
drei  99   6      9     12 0.857142857142857 VW K�fer 1200

> summary(data)
 IMI     ATA     PERSIL TANDIL                 Fewa               Auto   
 99:45   6 :15   7:15   10:15   0.714285714285714:13   Ente 2CV4    :15  
         99:30   8:15   11:15   0.857142857142857:17   VW K�fer 1200:17  
                9:15   12:15   1                :15   Wartburg 311 :13  
Anzahl nach Auto.

> table(data$Auto)
    Ente 2CV4 VW K�fer 1200  Wartburg 311 
           15            17            13

Struktur von data
> str(data)
'data.frame':   45 obs. of  6 variables:
 $ IMI   : Factor w/ 1 level "99": 1 1 1 1 1 1 1 1 1 1 ...
  ..- attr(*, "names")= chr  "eins" "zwei" "drei" "eins" ...
 $ ATA   : Factor w/ 2 levels "6","99": 2 2 1 2 2 1 2 2 1 2 ...
  ..- attr(*, "names")= chr  "eins" "zwei" "drei" "eins" ...
 $ PERSIL: Factor w/ 3 levels "7","8","9": 1 2 3 1 2 3 1 2 3 1 ...
  ..- attr(*, "names")= chr  "eins" "zwei" "drei" "eins" ...
 $ TANDIL: Factor w/ 3 levels "10","11","12": 1 2 3 1 2 3 1 2 3 1 ...
  ..- attr(*, "names")= chr  "eins" "zwei" "drei" "eins" ...
 $ Fewa  : Factor w/ 3 levels "0.714285714285714",..: 3 3 2 1 3 1 1 2 2 2 ...
  ..- attr(*, "names")= chr  "eins" "zwei" "drei" "eins" ...
 $ Auto  : Factor w/ 3 levels "Ente 2CV4","VW K�fer 1200",..: 1 1 2 3 1 3 3 2 2 2 ...
  ..- attr(*, "names")= chr  "eins" "zwei" "drei" "eins" ...
Viel zuviel Factor-Variable.

> data[,1:5]<-as.numeric(data[,1:5])
Fehler: (list) Objekt kann nicht nach 'double' umgewandelt warden

Geht nicht auf eimal, nur einzeln

> data[,1]<-as.numeric(data[,1])
> data[,2]<-as.numeric(data[,2])
> data[,3]<-as.numeric(data[,3])
> data[,4]<-as.numeric(data[,4])
> data[,5]<-as.numeric(data[,5])

> str(data)
'data.frame':   45 obs. of  6 variables:
 $ IMI   : num  1 1 1 1 1 1 1 1 1 1 ...
 $ ATA   : num  2 2 1 2 2 1 2 2 1 2 ...
 $ PERSIL: num  1 2 3 1 2 3 1 2 3 1 ...
 $ TANDIL: num  1 2 3 1 2 3 1 2 3 1 ...
 $ Fewa  : num  3 3 2 1 3 1 1 2 2 2 ...
 $ Auto  : Factor w/ 3 levels "Ente 2CV4","VW K�fer 1200",..: 1 1 2 3 1 3 3 2 2 2 ...

One-way Klassifizierung

> oneway<-tapply(data$Fewa, data$Auto, FUN=sum)


oneway
> tapply(data$Fewa, data$Auto, FUN=sum)      # eindimensional
    Ente 2CV4 VW K�fer 1200  Wartburg 311 
    15.000000     14.571429      9.285714 

Zweifach Klassifizierung

> twoway<-tapply(data$Fewa, list(data$Auto, data$PERSIL), FUN=sum) ; twoway; dim(twoway)
               7  8  9
Ente 2CV4      6 21 18
VW K�fer 1200 14 12  8
Wartburg 311   6  2  5
[1] 3 3                                  # 2-dimensional

Dreifach Kreuztabelle

dreifach<-tapply(data$Fewa, list(data$Auto, data$PERSIL, data$ATA), FUN=sum) ; dreifach ; dim(dreifach)
, , 6                        # ATA hat nur die Auspr�gungen 6 und 99

               7  8  9
Ente 2CV4     NA NA 18
VW K�fer 1200 NA NA  8
Wartburg 311  NA NA  5

, , 99

               7  8  9
Ente 2CV4      6 21 NA
VW K�fer 1200 14 12 NA
Wartburg 311   6  2 NA

[1] 3 3 2                              # 3 dim array dreifach

Welche Struktur haben die Ergebnis-Objekte?
> class(oneway)                          # Warum?
[1] "array"

> class(twoway)
[1] "matrix"

> class(dreifach)
[1] "array"
11.2 Subsets von Data.frames
> dim(subset(data, ATA==99))
[1] 30  6

> subset(data, (ATA==99&PERSIL==7&Auto=="Wartburg 311"))
       IMI ATA PERSIL TANDIL      Fewa         Auto
eins    99  99      7     10 0.7142857 Wartburg 311
eins.1  99  99      7     10 0.7142857 Wartburg 311
eins.2  99  99      7     10 0.7142857 Wartburg 311
eins.3  99  99      7     10 0.7142857 Wartburg 311
eins.4  99  99      7     10 0.7142857 Wartburg 311
eins.5  99  99      7     10 0.7142857 Wartburg 311

> subset(data, (ATA==99&PERSIL==7&Auto=="Wartburg 311"))
       IMI ATA PERSIL TANDIL      Fewa         Auto
eins    99  99      7     10 0.7142857 Wartburg 311
eins.1  99  99      7     10 0.7142857 Wartburg 311
eins.2  99  99      7     10 0.7142857 Wartburg 311
eins.3  99  99      7     10 0.7142857 Wartburg 311
eins.4  99  99      7     10 0.7142857 Wartburg 311
eins.5  99  99      7     10 0.7142857 Wartburg 311

# nur bestimmte Attribute

> head(subset(data, ATA==99, select=c(IMI, PERSIL)))
       IMI PERSIL
eins    99      7
zwei    99      8
eins.1  99      7
zwei.1  99      8
eins.2  99      7
zwei.2  99      8

# Selection mit der Funktion match (%in%)

> subset(data, PERSIL %in% 8, select=c(IMI, PERSIL))
        IMI PERSIL
zwei     99      8
zwei.1   99      8
zwei.2   99      8
zwei.3   99      8
zwei.4   99      8
zwei.5   99      8
zwei.6   99      8
zwei.7   99      8
zwei.8   99      8
zwei.9   99      8
zwei.10  99      8
zwei.11  99      8
zwei.12  99      8
zwei.13  99      8
zwei.14  99      8

Im R-System vorhandene Datens�tze und eigene data.frames k�nnen direkt verf�gbar gemacht werden mit attach(DF). Daraufhin kann man die Spalten des data.frame direkt anspechen, was die Kommandos vereinfacht. Dadurch kann man auch den Arbeitsspeicher im Workspace schonen.

Bitte beachten: mit detach werden die Kopien wieder gel�scht. �nderungen sind nur beim DF selbst m�glich, nicht bei den attachten Kopien.


12 Simulation - Splines
Der Begriff Spline  (l�ngliches, d�nnes St�ck Holz oder Metall) wurde zuerst in einer englischen Ver�ffentlichung von Isaac Jacob Schoenberg im Jahr 1946 f�r glatte, harmonische, zusammengesetzte mathematische Kurven dritten Grades benutzt. 
Splines werden vor allem zur Interpolation und Approximation benutzt. Durch die st�ckweise Definition sind Splines flexibler als Polynome und dennoch relativ einfach und glatt. Dadurch ergeben sich bei der Spline-Interpolation nicht die Nachteile, die durch die starke Oszillation von Polynomen h�heren Grades und deren Unbeschr�nktheit bei der Polynominterpolation entstehen (Runges Ph�nomen). Splines lassen sich auch gut benutzen, um Kurven darzustellen. Hier finden sie Einsatz im CAD. Mathematisch analog lassen sich auf beide Weisen nicht nur Kurven, sondern auch Fl�chen beschreiben. 
Wortherkunft: Der Begriff stammt aus dem Schiffbau: eine lange d�nne Latte (Straklatte, englisch spline), die an einzelnen Punkten durch Molche fixiert wird, biegt sich genau wie ein kubischer Spline mit nat�rlicher Randbedingung. Dabei wird die Spannungsenergie minimal (wiki).

Splines im R-System
help(spline)
Perform cubic (or Hermite) spline interpolation of given data points, returning either a list of points obtained by the interpolation or a function performing the interpolation.  
12.1 Spline-Funktion f�r Hausm�llmenge 
Eine Jahresganglinie f�r die Hausm�ll-Gesamtmenge soll durch eine Spline-Funktion gefunden werden. Es wurden aus Kostengr�nden (Handsortierung, Stichproben) nur drei Messungen (Hausm�llanalysen) pro Jahr in einem Gebiet durchgef�hrt. Durch ein Spline-Programm sollen die zwischen den Messungen liegenden Punkte simuliert werden.
Empirische Daten:
x<- 0, 4, 7.5 12
y<- 3, 2, -5, 3 
Es handelt sich um die prozentuale Abweichnung der M�lmenge vom Mittelwert, gemessen wurde drei mal, der erste Wert entspricht dem letzten. Gesch�tzt werde soll die sogenannte Jahresganglinie des M�lls.
R-Spline-function:
spline(x, y = NULL, n = 3*length(x), method = "fmm",
       xmin = min(x), xmax = max(x), xout, ties = mean)

Gezeichnet werden sollen die Messpunkte, Nulllinie (y=0), und die ermittelte Spline-Kurve.

Mit xlim und ylim kann man den Graphikbereich (problemspace) formatieren.
> Monate
[1]  0.0  4.0  7.5 12.0
> Proz_Abweichung
[1]  3  2 -5  3

> plot(Monate, Proz_Abweichung, ylim=c(-6,6), pch=19,  col="Red")
> lines(a<-0:12,b<-rep(0,13))
> lines(spline(x, y), col="Blue")
> title("Saisonale Schwankung der M�llmenge, gesch�tzt durch Splines")


13.2 Simulation eines digitalen Sterns
> d<-read.table("stern.csv")
    V1       V2
1 90.0 30.00000
2 82.5 42.99038
3 90.0 55.98076
4 75.0 55.98076
5 67.5 68.97114
6 60.0 55.98076

> dim(d)
[1] 39  2

> class(d)
[1] "data.frame"
Erstellung von Vektorgraphiken
Eine Vektorgrafik ist eine Computergrafik die auf sogenannten grafischen Primitiven wie Farben, Formen, Kurven (Splines) und Linien basiert. Es ist eine auf einer Ebene dargestellte 2D Grafik. Vektorgrafiken basieren nicht auf einzelnen Pixeln wie Bilder. 
Was ist der Vorteil von Vektorgrafiken?
Vektorgrafiken basieren auf einer Bildbeschreibung, die die Objekte, aus denen das Bild aufgebaut ist, exakt definiert. Ein Beispiel: Ein Kreis in einer Vektorgrafik beinhaltet Lage des Mittelpunktes, Radius, Linienst�rke und Farbe. Nur diese Parameter werden gespeichert. Aus diesem Grund sind Vektoren verlustfrei und platzsparend. Ein weiterer Vorteil ist die stufenlose und verlustfreie Skalierbarkeit der Grafik. Pixelgrafiken werden unscharf und verlieren an Qualit�t, wenn man sie gr��er skaliert und sind nicht mehr richtig erkennbar. 
Verktorgrphik und Pixelgraphik
Vektorgrafik
Pixelgrafik 
Egal wie hoch oder niedrig die Aufl�sung des Ausgabeger�tes ist, Vektorgrafiken erlauben es, Dokumente mit der h�chstm�glichen Aufl�sung auf Bildschirmen darzustellen. 
Durch die Aufl�sungsunabh�ngigkeit sind Vektorgrafiken sowohl f�r die Wiedergabe auf dem Bildschirm als auch im Druck geeignet. Besonders bei der Erstellung von Illustrationen, Grafiken und Logos finden Vektorgrafiken ihren Einsatz. Logos werden beispielsweise immer in verschiedenen Gr��en verwendet und daher als Vektorgrafik erstellt. Genauso Landkarten.

help(matplot)
Plot the columns of one matrix against the columns of another.


d<-stern

> matplot(d[,1], d[,2], type="l", xlab="", ylab="", col="green")
> par(new=TRUE)

Der zweite Stern wird proportional verkleinert durch Multiplikation mit 0,3.

> matplot(0.3*d[,1], 0.3*d[,2], type="l", xlab="", ylab="", xlim=c(0,90), ylim=c(0,70), col="blue")
> title("Zwei simulierte Sterne

Gef�llter Stern mit polygon
polygon(d[,1], d[,2], col = "orange", lwd = 3, border = "green")

12.3 Digitale Schmetterlinge
An der Universit�t von Southern Mississippi wurden von Temple Fay verschiedene Schmetterlingskurven aus relativ einfachen mathematischen Formeln entwickelt. Einige dieser Formeln findet man im Buch von C. A. Pickover "Mit den Augen des Computers". Die Gleichung f�r die Schmetterlingskurve in Polarkoordinaten:
??ecos(?)-2cos(4?)+sin(????)5
? ist der Radiale Abstand der Bahnpunkte vom Ursprung. Theta berechnet man durch Multiplikation mit Pi.
Ein R-Programm f�r einen simulierten Schmetterling.

butterfl<-function(points) {
# Schmetterlingskurve als X-Y Graphik
# Points: Anzahl von Punkten
theta<-pi*(0:points)/100            
r<-exp(1)^cos(theta)-2*cos(4*theta)+sin(theta/12)^5   # Gleichung
x<-r*sin(theta)                  # Konvertieren aus Polarkoordinaten
y<-r*cos(theta)                  # Konvertieren aus Polarkoordinaten
res<-cbind(x,y)
res
}

> d<-butterfl (2000)                   # 2000 Punkte
> par(mfrow=c(1,1))
> matplot(d[,1],d[,2], type="l", xlab="", ylab="", col="green")
> title("Gr�ner digitaler Schmetterling mit 2000 Punkten")

Um den Schmetterling mit Farben zu f�llen, wird das Programm polygon ben�tigt.
> matplot(d[,1], d[,2], xlab="", ylab="", type='l')
> polygon(d[,1], d[,2], col = "orange", lwd = 3, border = "green")
> polygon(d[1:100,1], d[1:100,2], col = "blue", lwd = 3, border = "green")
> title("Bunter digitaler Schmetterling mit 200 Punkten")

12.4 Entwicklungsphasen f�r Schmetterlinge
Die �Entwicklungsphasen� eines digitalen Schmetterlings sollen berechnet und auf einem 3 x 4 Display gemeinsam dargestellt werden. 
Z.B.mit der function butterfl_out.

butterfl_out<-function (dis) 
{
par(mfrow=c(3,4))                   # Bildschirm 3 x 4
for(i in 1:12)                      # For-Schleife
    {
d<-butterfl (p<-i*17)
matplot(d[,1],d[,2], type="l", xlab="", ylab="", col=i)  # col(i)
title(c(p,"Punkte"))
    }
}

Durch Aufruf von butterfl_out (12) erh�lt man:


Ver�nderung und Darstellung von Objekten, diese befinden sich im workspace vektor.
load("vektor.RData")             # workspace wird zus�tzlich geladen
Darin befinden sich die Vektordateien hase, auto, saurier, pferd und stern. Diese haben 3 Spelten, also Spalte 2 und 3 aufrufen.

> matplot(auto[,2], auto[,3], type="l", xlab="", ylab="", col="green")
> polygon(auto[,2], auto[,3], col = "orange", lwd = 3, border = "blue")
array


13 Auswertung Einkommen in Deutschland
Alle f�nf Jahre werden in Deutschland private Haushalte im Rahmen der Einkommens- und Verbrauchsstichprobe (EVS) zu ihren Einnahmen und Ausgaben, zur Verm�gensbildung, zur Aus-stattung mit Gebrauchsg�tern und zur Wohnsituation befragt. 2013 fand die EVS nach den Erhebungen der Jahre 1962/63, 1969, 1973, 1978, 1983, 1988, 1993, 1998, 2003 und 2008 zum elften Mal statt.
Die Ergebnisse aus der Befragung der 53 490 Stichprobenhaushalte wurden auf die Grundgesamtheit von 39,3 Mill. privaten Haushalten in Deutschland hochgerechnet. In den Stichprobenhaus-halten lebten insgesamt 112 930 Personen, die auf die Grundgesamtheit von 79,5 Mill. Personen hochgerechnet wurden.
Haushaltsnettoeinkommen �ber 18 000 Euro im Monat sind in der vorliegenden Publikation nicht ausgewiesen (Abschneidegrenze), da sich nicht gen�gend Haushalte mit so hohen Einkommen an der EVS beteiligen.
Datenquelle: Diesen Link bitte im Browser eingeben! Stand 2008
https://www.destatis.de/DE/Publikationen/Thematisch/EinkommenKonsumLebensbedingungen/EinkommenVerbrauch/Einkommensverteilung2152606089004.pdf?_blob=publicationFile
Rohdaten�:
Monatliches Haushaltsnettoeinkommen 2008 (gleiche Klassenbreite) 1.4.1 Deutschland

1 Erfasste Haushalte (Anzahl) ................................... 207 3 272 5 199 6 422 6 353 5 722 5 377 4 711 4 032 3 306 2 574 2 029 5 244 662
2 Hochgerechnete Zahl der Haushalte (1 000) ......... 226 4 252 5 744 5 807 4 852 3 809 3 268 2 613 2 065 1 610 1 187 941 2 585 451
Aktuell auf Seite 31:
https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Einkommen-Konsum-Lebensbedingungen/Einkommen-Einnahmen-Ausgaben/Publikationen/Downloads-Einkommen/einkommensverteilung-2152606139004.pdf?__blob=publicationFile&v=4
1 Erfasste Haushalte (Anzahl) ................... 178 3 379 4 976 5 9336 3125 7415 0674 4203 7583 1172 6281 9965 317 668
2 Hochgerechnete Zahl der Haushalte (1 000) .... 179 3 7375 1905 2734 7143 9103 2402 6872 2541 7771 5021 1063 170 587

Aufgabe: Anfertigung einer Vergleichsgraphik
Arbeitsschritte:

1. In Textdatei Leerzeichen bei 1000-Trennung entfernen
2. In Excel importieren, Text in Spalten und als csv exportieren
3. In R importieren und Nachbearbeitung
eink<-read.table("D:\\Lehre\\eink2008.csv", sep=";")

# Zeilenlabels ersetzen
dimnames(eink)[[1]]<-eink$V1

# ehemalige Labelspalte entfernen
eink<-eink[,2:15]

# Spaltenlabels auktualisieren
dimnames(eink)[[2]]<-500*1:14

# die letzten beiden Spaltenlabel im Editor einsetzen
fix(eink) oder Zuweisung

> dimnames(eink)[[2]][c(13,14)]<-c(10000,18000)

# letzte Version von eink
              500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 10000 18000
Haushalte     207 3272 5199 6422 6353 5722 5377 4711 4032 3306 2574 2029  5244   662
Hochgerechnet 226 4252 5744 5807 4852 3809 3268 2613 2065 1610 1187  941  2585   451

# Datenkorrektur
> eink[,13]<-eink[,13]/8
> eink[,14]<-eink[,14]/16

> barplot(eink[1,], col=rainbow(14))
Fehler in barplot.default(eink[1, ], col = rainbow(14)) : 
  'height' muss ein Vektor oder eine Matrix sein
13.1 Verteilung des Einkommens
> ein<-t(eink)
> par(mfrow=c(1,2))
> barplot(ein[,1], col=rainbow(14))
> title("Einkommensverteilung Deutschland 2008 EVS",sub="EVS Einkommmens- und Verbrauchsstichprobe")
> barplot(ein[,2], col=rainbow(14))
> title("Einkommensverteilung Deutschland 2008 Hochrechnung")



# Symmetrie der Einkommensverteilung?

Anzahl replicate Klassenmitte, Datenrekonstruktion

> ein_d<-rep(as.numeric(dimnames(ein)[[1]]),floor(ein[,2]))

> length(ein_d)
[1] 36725                              1/1000 der Haushalte

Es gibt ca. 40 Mio Haushalte in Deutschland

> summary(ein_d)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    500    1500    2500    2813    3500   18000 [1] 67000      

Verteilung des Einkommens in Deutschland 2008
hist(eind_d)

13.2 Daten von 2017

https://www.statistik-berlin-brandenburg.de/produkte/zeitschrift/2019/HZ_201902.pdf

Textdatei von Statis    
von   bis   anz Hoch HHg
   0  1300  562 5997 1.1
 300  1700  443 3038 1,3 
1700  2600 1287 7759 1,6
2600  3600 1389 6718 1,9
3600  5000 1727 6619 2,4
5000 18000 2277 7598 2,9

> eink<-read.table("eink2018.txt", header=T)   # Import

> attributes(eink)
$names
[1] "von"  "bis"  "anz"  "Hoch" "HHg" 

$class
[1] "data.frame"

$row.names
[1] 1 2 3 4 5 6

> eink.mit<-apply(eink[,1:2], 1 , mean) ; eink.mit   # Klassenmitte
[1]   650  1000  2150  3100  4300 11500

> rep(eink.mit[1:3], 1:3)             # Test repeat
[1]  650 1000 1000 2150 2150 2150

> haush<-rep(eink.mit, eink$Hoch) ; length(haush)  # Haushalte
[1] 37729

> table(haush)
haush
  650  1000  2150  3100  4300 11500 
 5997  3038  7759  6718  6619  7598 

> summary(haush)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    650    2150    3100    4248    4300   11500

Hist(haush)

> barplot(table(haush), border="Blue")
> title("Verteilung des monatlichen Haushaltseinkommens 2017")

14 Kreuztabelle, OLAP und data cube
Pivot-Tabellen (engl. pivot [p?v?t], im Deutschen h�ufig franz�sisch [pi.vo] ausgesprochen: Dreh-, Angelpunkt, Schl�sselfigur, Achse; auch Pivot-Table-Berichte oder pivot table reports) sind eine spezielle Art von Tabellen, die die M�glichkeit bieten, Daten einer Tabelle in verschiedener Art darzustellen und auszuwerten, ohne die Ausgangsdaten bzw. -tabelle(n) dabei �ndern zu m�ssen.[1] 
Eine Pivot-Tabelle stellt aufgrund der verwendeten Aggregierung in den Datenfeldern die Ausgangsdaten in verdichteter, zusammengefasster Form dar. Das ist zwar mit Informationsverlust verbunden, aber genau darin liegt der Nutzen einer Pivot-Tabelle. Sie ist ein Hilfsmittel, um gro�e Datenmengen auf �berschaubare Gr��en zu reduzieren und einfache Auswertungen durchzuf�hren. Pivot-Tabellen k�nnen nur f�r die Abfrage, nicht zur Vermehrung von Daten verwendet werden. Die Eintr�ge einer Pivot-Tabelle sind entweder schreibgesch�tzt oder eine �nderung wirkt sich nicht auf die zugrundeliegenden Originaldaten aus.
Bei einer Kreuztabelle, auch als Kontingenztabelle bezeichnet, handelt es sich um ein bedeutendes Analyseinstrument der deskriptiven Statistik. Kreuztabellen beinhalten die absoluten und relativen H�ufigkeiten von Kombinationen zweier Merkmalsauspr�gungen einer statistischen Einheit oder Mittelwerte bzw. andere statistische Kenngr��en. Sie liefern Erkenntnisse dar�ber, inwiefern die untersuchten Variablen miteinander in Beziehung stehen. Generell lassen sich mithilfe von Kreuztabellen auch gro�e Datenmengen kompakt zusammenfassen und�strukturieren und falls gew�nscht einen data cube erzeugen.
Ein OLAP-W�rfel oder Datenw�rfel (englisch OLAP cube oder engl. data cube), auch Cube-Operator genannt, ist ein in der Data-Warehouse-Theorie gebr�uchlicher Begriff zur logischen Darstellung von Daten. Die Daten werden dabei als Elemente eines mehrdimensionalen W�rfels (engl. cube) angeordnet. Die Dimensionen des W�rfels beschreiben die Daten und erlauben auf einfache Weise den Zugriff. Daten k�nnen �ber eine oder mehrere Achsen des W�rfels ausgew�hlt werden. Die Bezeichnung OLAP (Online Analytical Processing) stammt aus der Datenanalyse. 
Diese Art der Darstellung ist f�r die Analyse von Daten von Vorteil, da auf verschiedene Aspekte (Dimensionen) der Daten auf gleiche Weise zugegriffen wird. Daher auch der Einsatz bei OLAP-Anwendungen, welche die Daten in einem Data-Warehouse analysieren oder visuell aufbereiten. 
Beispielfragen f�r Kreuztabellen
Die folgenden Beispiele verdeutlichen, weshalb die Kontingenztabellen in der statistische Analyse f�r viele Anwendungen relevant�sind:
* Welche Obstsorte verkauft sich zu welcher Jahreszeit am�besten?
* Pr�ferieren bestimmte Altersgruppen bestimmte�Produkte?
* Welche K�ufergruppe trinkt z.B. gerne Bier oder isst M�sli?
* F�hren gesunde Ern�hrung oder Sport zu einem l�ngeren�Leben?
* In welchen L�ndern verkauft sich eine bestimmte software besser?
* Welche K�ken wachsen bei welcher Nahrung am besten?
14.1 Daten und Einheiten einer Kreuztabelle
Um eine klassische Kreuztabelle erstellen zu k�nnen, ist ein zweidimensionaler Datensatz erforderlich. Dieser sollte folgende Informationen�beinhalten:
Statistische Einheit: Hierbei handelt es sich um ein einzelnes Objekt in einer statistischen Erhebung. Die Einheiten werden auch als Merkmalstr�ger bezeichnet, da sie die Merkmale und Informationen in sich tragen, deren Zusammenhang ermittelt werden soll. Aus dem Datensatz muss haupts�chlich die Anzahl der statistischen Einheiten ersichtlich�werden.
Merkmale: Im statistischen Kontext werden Merkmale vorwiegend als Variablen bezeichnet, ihnen sind jeweils mehrere Merkmalsauspr�gungen zuzuschreiben. Mit einer Kreuztabelle kann die Beziehung zweier Variablen (Merkmal 1 und 2) untersucht werden, wobei von Interesse sein kann, welches Merkmal als Ursache und welches als Wirkungzu betrachten ist.�
Merkmalsauspr�gungen: Die Merkmalsauspr�gungen gestalten sich entsprechend der Merkmale und sind in ihrer Anzahl grunds�tzlich unbegrenzt. Aus dem vorliegenden Datensatz sollte demnach hervorgehen, welche Merkmale von statistischen Einheiten untersucht werden und wie h�ufig welche Kombination zweier Auspr�gungen�auftritt.
Erkenntnisse aus einer Kreuztabelle
Ist eine Kreuz- bzw. H�ufigkeitstabelle mit s�mtlichen relevanten Daten gef�llt, k�nnen daraus mehrere Erkenntnisse gewonnen werden. Hierzu gibt es verschiedene Vorgehensweisen.�
Absolute und relative H�ufigkeiten
Die in der Kontingenztabelle eingetragenen Daten k�nnen als absolute sowie relative H�ufigkeiten ausgedr�ckt�werden:
Bei absoluten H�ufigkeiten handelt es sich um die zeilen- bzw. spaltenweise Summierung der einzelnen H�ufigkeiten jeder untersuchten Merkmalsauspr�gung.�
Zum Erhalt der relativen H�ufigkeiten hingegen werden die absoluten Werte (ganze Zahlen) in Relation zur Gesamtheit aller statistischen Einheiten gesetzt.�
Mithilfe der relativen H�ufigkeiten kann der prozentuale Ansatz einer jeden Kombination von Auspr�gungen ermittelt werden, was wiederum die Darstellung in Form von Diagrammen (z. B. Balken, S�ulen- oder Kreisdiagramme) erm�glicht.�
Signifikanz der Kreuztabelle mit Chi-Quadrat-Analyse
Der Chi-Quadrat-Unabh�ngigkeitstest dient der �berpr�fung der statistischen Signifikanz einer Kreuztabelle. Es wird also die Wahrscheinlichkeit ermittelt, ob die H�ufigkeitsverteilungen und daraus erschlossenen Zusammenh�nge zuf�llig sind oder nicht. Im Falle der Kontingenztabelle handelt es sich dabei um die Beziehung der beiden untersuchten Variablen. Es gilt herauszufinden, inwieweit sie abh�ngig bzw. unabh�ngig sind und die Ergebnisse der statistischen Erhebung demnach als signifikant oder nicht gelten.�Ist z.B. die Ern�hrungsweise f�r das Wachstum der K�ken signifikant?
Der Chi-Quadrat-Test sieht zun�chst die Berechnung eines Chi-Quadrat-Wertes f�r jede einzelne Zelle vor, der dann zu einem Gesamtwert f�r die Tabelle zusammengefasst wird. Die grundlegende Formel gestaltet sich dabei folgenderma�en:�
(Beobachteter Wert � Erwarteter Wert)? / (Erwarteter Wert) =�Chi-Quadrat-Wert
Der erwartete Wert ergibt sich, in dem die Randh�ufigkeiten multipliziert und durch ihre Gesamtsumme dividiert werden. Bewegt sich die durch die obige Formel erhaltene Prozentzahl zwischen .05 und 5, gelten die Ergebnisse als statistisch signifikant.
14.2 Count frequencies, Zahl der F�lle, BSR-Daten korrigiert
> dim(bsr)
[1] 15401     7

> head(bsr, 3)
   tag zeit   Mg    kfznr bsrkey  Laga P
1 2609  514 0.76 B-EE2388    UNB 91200 A
2 2609  536 6.38 B-CX2104    UNB 91200 A
3 2609  542 2.28  B-CR332    UNB 91200 A
Anzahl feststellen mit R-Funktion table
> help(table)
Eindimensionale Analysen � One-way

Zahl der F�lle, attach(bsr)

> table(P)                            # Anzahl Abf�lle nach Anlieferort
P
   A    C    E    N    S    X 
 558 2579 1152 4222 6651  239 

> sum(table(P))                      # Alle Einheiten zugeordnet?
[1] 15401

> table(ceiling(Mg))                     # Anzahl nach Gewichtsklassen 

   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24 
2798 2051 1765 1245 1101 1082 1127  962  791  728  501  184   64   25   16   12   56   11  107   54   69   89   80   99 
  25   26   27   28   29   30   31   32 
 113  127   66   41   23    7    6    1
> sum(table(ceiling(Mg)))
[1] 15401
Mit unserem eigenen freq.cont programm.

> freq.cont(Mg, c(0,1,32))
   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24 
2798 2051 1765 1245 1101 1082 1127  962  791  728  501  184   64   25   16   12   56   11  107   54   69   89   80   99 
  25   26   27   28   29   30   31   32 
 113  127   66   41   23    7    6    1
Zweidimensionale Analyse � 2 -way
> zwei<-table(tag, Laga)   ; dim(zwei)
[1] 11 66

# Transponieren und Anzeigen der Tabelle

> head(t(zwei),5)
       tag
Laga    110 410 510 610 710 810 2609 2709 2809 2909 3009
  0       0   4   8   6  13   0   15    9   16   13   15
  11102   0   1   1   1   1   0    1    1    1    1    2
  11114   0   0   0   0   0   0    0    0    0    1    0
  11402   0   0   0   0   0   0    0    0    1    0    0
  11403   0   1   0   0   0   0    0    0    0    0    0
Dreidimensionale Analyse - 3-way � Ergebnis ein array
Anzahl nach 3 Achsen summiert

drei<-table(tag, Laga, P)
> dim(drei)
[1] 11 66  6

> sum(drei)                                 # alle Werte da
[1] 15401   

> is.array(drei)
[1] TRUE

> mode(drei)
[1] "numeric"

> class(drei)
[1] "table"

> is.data.frame(drei)           # data.frame immer nur 2-dimensional
[1] FALSE 
14.3 Mehrdimensionale Kreuztabelle mit gemessenen Daten
help(tapply)

Description:

     Apply a function to each cell of a ragged array, that is to each
     (non-empty) group of values given by a unique combination of the
     levels of certain factors.

tapply(X, INDEX, FUN = NULL, simplify = TRUE, ...)
Datenanayse mit Messdaten von bsr

Es gibt Gruppierungsvariable (nominal, kategorial) und gemessene, numerische Daten (Messwerte in Kg, km, Mbyte, K�rpergr��e, CO2-Emission etc.)
Die Funktion tapply wird aufgerufen mit folgender �bergabe:
Messwerte, Gruppierungsvariable und anzuwendende Funktion. Mehrere Gruppierungsvariable werden einlelistet.
Eindimensionale Analyse, One-way, bsr
> ein<-tapply(Mg, ceiling(Mg), sum)     # nur Gewichtsklasse 1
> length(ein)
[1] 32
> ein
      1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16 
1432.70 3062.71 4394.87 4358.30 4961.67 5964.00 7313.15 7209.14 6721.96 6914.29 5237.49 2100.82  794.06  337.22  230.60  185.08 
     17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32 
 922.95  194.04 2012.59 1052.28 1413.46 1920.37 1799.33 2333.63 2767.67 3235.90 1742.79 1124.31  653.86  207.58  182.44   31.30 

Oder mit split und sapply

> ein<-sapply(split(Mg, ceiling(Mg)), sum)
> ein
      1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16 
1432.70 3062.71 4394.87 4358.30 4961.67 5964.00 7313.15 7209.14 6721.96 6914.29 5237.49 2100.82  794.06  337.22  230.60  185.08 
     17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32 
 922.95  194.04 2012.59 1052.28 1413.46 1920.37 1799.33 2333.63 2767.67 3235.90 1742.79 1124.31  653.86  207.58  182.44   31.30
Oder mit unserem eigenen Programm sum.cont (verbessert).
> sum.cont(Mg, c(0,1,32))
      1       2       3       4       5       6       7       8       9      10      11      12      13      14      15 
1432.70 3062.71 4394.87 4358.30 4961.67 5964.00 7313.15 7209.14 6721.96 6914.29 5237.49 2100.82  794.06  337.22  230.60 
     16      17      18      19      20      21      22      23      24      25      26      27      28      29      30 
 185.08  922.95  194.04 2012.59 1052.28 1413.46 1920.37 1799.33 2333.63 2767.67 3235.90 1742.79 1124.31  653.86  207.58 
     31      32 
 182.44   31.30
Das verbesserte Programm sum.cont.

sum.cont<-function (dat,vek) 
{
# Total f�r metrische Variablen mit split
# vek[1] Untergrenze, vek[2] Intervallbreite, vek[3] Intervalle
tmp <- ceiling((dat - vek[1])/vek[2])# Standardisierung 
res <- outer((1:vek[3]),tmp, FUN= "==")  # outer product
res<-matrix(res %*% dat)        # Inneres Produkt Daten x Bitmatrix
res<-as.vector(res)             # Matrix ung�nstig f�r plot
names(res)<-c(1:vek[3])                # mit labels
res }
Zweidimensionale Analyse � 2 -way
> zwei<-tapply(Mg, list(tag, P), sum)          # Liste von Tag und Ort
> dim(zwei)
[1] 11  6
> class(zwei)
[1] "matrix"

> zwei
          A       C      E       N       S      X
110   27.02  141.70  19.40  219.69      NA     NA
410  138.78 3660.56 720.86 2722.18 2984.98 417.56
510  150.28 3805.80 627.84 2412.00 2613.44 538.33
610  171.20 2479.02 739.16 1757.25 2230.64 315.76
710  152.02 3069.20 608.96 2075.91 2151.02 379.33
810   39.30 1611.66 347.20 1171.46 1267.62   0.60
2609 144.32 2720.72 658.00 2441.26 2685.30 473.13
2709 137.12 2499.96 613.62 2278.00 2494.63 516.80
2809 113.44 2579.88 596.79 1777.98 2129.01 583.90
2909 137.12 3001.46 613.46 1926.40 2267.05 305.32
3009 124.18 2177.48 531.14 2056.47 2055.07 405.82


3-D-Graphic mit Excel, Datei: write.csv2(zwei, file="gew3d.csv")


Dreidimensionale Analyse - 3-way
Gewicht nach 3 Achsen summiert, ein data cube (array)

drei<-tapply(Mg, list(tag, Laga, P), sum)

> dim(drei)
[1] 11 66  6

> class(drei)
[1] "array"
Die Summe zur Kontrolle.
> sum(drei)                             # geht nicht
[1] NA

> sum(drei[!is.na(drei)]) == sum(Mg)           # alles OK
[1] TRUE

Hier noch eine Variante, um mit tapply die Anzahl zu berechnen.

> anz3<-tapply(Mg, list(tag, Laga, P), length)
> dim(anz3)
[1] 11 66  6
> sum(anz)
[1] 15401

Selbstverst�ndlich lassen sich auch 4- und mehrdimensionale Kreuztabellen mit R erzeugen.
15 Speicherbedarf und Verschl�sselung
15.1 Speicherbedarf von Daten nach Datentyp

Kodieren, Entkodieren als Vorbereitung f�r die verschiedene Zwecke.
Warum werden Daten kodiert?
(Lateinische) Buchstaben der W�rter und Index sind auch Kodierung.

Wie k�nnen verschiedene Daten nach Typ repr�sentiert werden? 
Wieviel Speicherplatz beanspruchen verschiedene Daten nach Typ im R-Workspace?
Integer
> zahlen1<-1:4
> zahlen1
[1] 1 2 3 4

> object.size(zahlen1)
[1] 40 bytes

Real

 > zahlen2<-(1:4)/2:5
 > zahlen2
[1] 0.5000000 0.6666667 0.7500000 0.8000000

> object.size(zahlen2)
[1] 56 bytes

Character, Textliste

> liste1<-c("eins","zwei","drei","vier")
> liste1
[1] "eins" "zwei" "drei" "vier"

> object.size(liste1)
[1] 168 bytes

Liste

> liste2<-as.factor(liste1)
> liste2
[1] eins zwei drei vier

> object.size(liste2)
640 bytes

# Repr�sentation durch Index
> as.integer(liste2)
[1] 2 4 1 3

> str(liste2)                               Struktur
 Factor w/ 4 levels "drei","eins",..: 2 4 1 3
15.2 Kodieren, Verschl�sseln, Verbesserung der Sicherheit, Datenschutz
Digitale Information ist Information, die durch Ziffern (engl. Digit) beschrieben wird. Dies kann z.�B. im Dualsystem durch die Ziffern 0 und 1 erfolgen. Computertechnik basiert auf der Verarbeitung digitaler Information. 
Mit der Verbreitung der Computer, die Systeme, die nur mit zwei unterschiedlichen Zust�nden arbeiten, wie Nein und Ja, An und Aus oder 0 und 1, am leichtesten interpretieren k�nnen, wurde auch die Digitalisierung eingef�hrt. Dazu schrieb Rafael Capurro 1999 in seinem Essay zum �Digitalen Weltentwurf�: 
�Das Seinsverst�ndnis hat umgeschlagen. Im Kontrast zum auf der empirischen Anschauung basierenden Entwurf der Neuzeit, wonach das Sein der Dinge von der Anschauung untrennbar ist (Kant), ja sogar mit dem Wahrgenommensein identisch ist �Their esse is percipi� (Berkeley 1965, 62), gilt: jetzt: esse est computari. Die Welt ist alles, was digitalisierbar ist.�  (Wiki)
Wir rechnen seit unserer Kindheit mit dem Dezimalsystem (lat. decimus, der Zehnte) und verwenden dabei die zehn Ziffern 0, 1, ... 9. Der Wert einer Ziffer in einer Zahl h�ngt von ihrer Position ab, die erste 3 in 373 hat z.B. einen anderen Wert als die zweite 3, n�mlich dreihundert und nicht drei. Im Dezimalsystem entspricht jeder Stelle eine Potenz der Basis 10: 100=1, 101=10, 102=100 usw. 

> c(10^2, 10^1, 10^0)                 # Zehner-Potenzen
[1] 100  10   1

> c(10^2, 10^1, 10^0) %*% c(3,7,3)    # Inneres Matrix Produkt
     [,1]
[1,]  373
Nach diesem Schema kann man auch Zahlensysteme erzeugen, die eine andere Basis besitzen als 10. Jede Stelle steht f�r ein Vielfaches der entsprechenden Potenz der Basis, und der Ziffernvorrat ist stets 0 bis Basis-1. Das System zur Basis 2 hat damit nur die beiden Ziffern 0 und 1. Da "0 und 1" auch f�r "ja oder nein" oder "an oder aus" oder "Strom oder nicht-Strom" stehen kann, ist dies das Zahlensystem, in denen eigentlich Computer "rechnen" und Daten speichern: Die kleinste Informationseinheit, das Bit, ist gerade die Information �ber die beiden M�glichkeiten 1 oder 0. Die Grundlage der Digitalisierung.
Ebenfalls in der Computertechnik gebr�uchlich ist das Oktalsystem (Basis 8) und das Hexadezimalsystem, das Zahlensystem mit der Basis 16. Da nur 10 Zahlenzeichen zur Verf�gung stehen, verwendet man die ersten sechs Buchstaben des Alphabets f�r die Zahlen 10 bis 15.
Die Standardeinheit der Informationsgr��e ist ein Byte, das sind 8 Bit. Ein Byte ist die Information �ber eine aus 256 M�glichkeiten, denn die je zwei Zust�nde der acht Bits erm�glichen insgesamt 28=256 M�glichkeiten. (In dezimaler Darstellung: 0, 1, 2, ... bis 255, im Bin�rsystem: 00000000, 00000001, 00000010, ... bis 11111111.)
Diese Einheit l��t sich mit dem 16er-System viel besser handhaben als mit dem Dezimalsystem, denn 256 ist gerade 162. Somit entsprechen in einer Hexadezimalzahl immer genau zwei Ziffern einem Byte.
15.3 Alphabetische Verschl�sselung
Bei dieser vielfach auf Julius Caesar zur�ckgef�hrten Verschl�sselungsmethode handelt es sich um einen Verschiebealgorithmus. Das originale Alphabet wird einfach um einen bestimmten Betrag verschoben. Der Betrag ist der Schl�ssel.1
Wir wollen den folgenden Satz mit dieser Methode in R verschl�sseln.

�Unser Zeit-Datum Zahlensystem ist abh�ngig vom Kalendersystem (Sonne und Mond).�
> text<-"Unser Zeit-Datum Zahlensystem ist abhaengig vom Kalendersystem (Sonne und Mond)"




Mit der Funktion utf8ToInt wird der Text zun�chst in Indexzahlen umgerechnet. Die Umkehrfunktion in R hei�t intToUtf8.
>ind<-utf8ToInt(text)  ; ind
[1]  85 110 115 101 114  32  90 101 105 116  45  68  97 116 117 109  32  90  97 104 108 101
[23] 110 115 121 115 116 101 109  32 105 115 116  32  97  98 104  97 101 110 103 105 103  32
[45] 118 111 109  32  75  97 108 101 110 100 101 114 115 121 115 116 101 109  32  40  83 111
[67] 110 110 101  32 117 110 100  32  77 111 110 100  41
Der Schl�ssel, d.h. die Verschiebung soll 3 Positionen nach rechts sein, also 
> ind.k<-ind+3
Mit diesem Index kann kann nun einen verschl�sselten Text erzeugen, den man offen Versenden kann.
> send<-intToUtf8(ind.k) ;  send
[1]"Xqvhu#]hlw0Gdwxp#]dkohqv|vwhp#lvw#dekdhqjlj#yrp#Ndohqghuv|vwhp#+Vrqqh#xqg#Prqg,"
Nach Empfang dieses Geheimtextes wird dieser wie folgt entschl�sselt:

> dec<-intToUtf8(utf8ToInt(send)-3) ; dec
 [1] "Unser Zeit-Datum Zahlensystem ist abhaengig vom Kalendersystem (Sonne und Mond)"
Kontrolle, ob der Text mit dem Original �bereinstimmt:
> text==dec
[1] TRUE
15.4 Numerische Verschl�sselung mit Encode und Decode.

Frage: Wie kann man 8160 Sekunden umrechnen, kodieren in unser Zeitsystem? Vgl. Kalendersysteme 

http://de.wikipedia.org/wiki/Astronomischer_Kalender
Was haben wir f�r einen Kalender? Dem gregorianischen Kalender liegt eine durchschnittliche Jahresl�nge von 365,2425 Tagen statt der julianischen 365,25 Tage zugrunde

Wieviel Tage, Stunden, Minuten, Sekunden sind 8130 Sekunden? 
Wir brauchen ein Programm und einen Schl�ssel (code) f�r die Umrechnung von einem Zahlensystem in ein anderes.
Es gibt verschiedene Zahlensysteme, z.B. 16, 10, 8, 2 oder beliebig.

> encode(8130, c(24,60,60))      # 24 Stunden, 60 Minuten, 60 Sek.
        2   15   30

Der Algorithmus:

> (2*60*60) + (15*60) + 30        # 2 Stunden, 15 Minuten, 30 Sekunden
[1] 8130
15.4 Dekodieren, Entschl�sseln als Umkehrfunktion 

encode<-function (dat, code) 
{
# dat: mit decode entcodierte Zahlen       # geschrieben 1993
        tmpcode <- rev(c(1, (cumprod(rev(code[-1])))))  # modify cod
        R <- NULL
        for(i in 1:length(code)) {
             R <- c(R, (dat %/% tmpcode[i])) # integer division
                dat <- dat %% tmpcode[i]        # modulo dat[i]
        }
        R <- as.vector(R)
        R 
}

decode<-function (dat, code) 
{
# Daten als Vector                      Programm geschrieben 1993
        tmpcode <- rev(c(1, (cumprod(rev(code[-1])))))  # modify code
        R <- as.vector(tmpcode %*% (dat))       # Inner product
        R
}

p(u)=1+(x1?1)u0+(x2?1)u1+?+(xm?1)um?1

> decode(c(2,4,6,3),c(8,8,8,8))     # Oktal-Code
[1] 1331

> (2*8^3)+(4*8^2)+(6*8^1)+(3*8^0)    # Wie wird gerechnet?
[1] 1331
> decode(c(2,15,30) ,c(24,60,60))    # Neues Beispiel, Zeit-Code
[1] 8130

Noch viel mehr Sekunden. Wieviele Jahre, Wochen, Stunden, Minuten, Sekunden sind 1.5 Mio Sekunden?

> encode(1.5E6, c(52,7,24,60,60))
[1]  2  3  8 40  0

Wieviele Jahre, Tage sind 4000 Tage?

> encode(4000, c(10,365))
[1]  10 350

Wieviele Monate, Tage sind 450 Tage?

> encode(450, c(12, 365/12))
[1] 14 24
15.5 Geburtstag, wieviele Tage haben Sie schon gelebt
Wieviel Tage haben Sie schon gelebt? Bitte geheimhalten, denn ich k�nnte Euern Geburtstag ausrechnen.

Jemand hat heute, am 12.6.2020 Geburtstag und ist 1997 geboren worden. Wieviele Tage hat sie/er schon gelebt?

Wieviel Tage hat der Monat, wenn das Jahr 365 Tage hat und es 12 Monate gibt? (Achtung Schaltjahre!)
> 365/12
[1] 30.41667

> 366/12
[1] 30.5

Dekodieren des heutigen Tages.             # Jahr, Monat, Tag
Der Schl�ssel lautet: 1 Jahrzehnt, 12 Monate, 30.42 Tage.

> d1<-decode(c(2020,6,12), c(10,12,365/12)) ; d1
[1] 737494.5
> d2<-decode(c(1997,6,12), c(10,12,365/12)) ; d2
[1] 729099.5
> d1-d2                        # Zahl der Lebenstage ohne Schalttage
[1] 8395

Wieviel Tage sind es genau, denn es fehlen die Februartage der Schaltjahre, die Schalttage? 

>  23/4                          # 6 Schalttage
[1] 5.75

Die Person, welche heute ihren Geburtstag hat, hat schon 8401 Tage gelebt.
> 6+d1-d2
[1] 8401


Viele weitere Beispiele zum Kodieren-�ben und Verstehen:

Dezimalsystem

> encode(1234, c(10,10,10,10))
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4

> decode(c(1,2,3,4), c(10,10,10,10))
     [,1]
[1,] 1234

Bin�rsystem

> encode(128, rep(2,8))
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    0    0    0    0    0    0    0

> decode(rep(1,8), rep(2,8))              # gr��te Zahl bei 8 bit
[1] 255

Oktalsystem

> encode(128, c(8,8,8,8))               # k�rzerer Code
   
0  2  0  0

> decode(c(0,2,0,0), c(8,8,8,8))
[1] 128

Neunersystem von Martin. 
Eure Geheimzahl 3707 bei der Targobank wird verschl�sselt nach dem Neunersystem

> v<-encode(3707, c(9,9,9,9)) ; v  # diese Zahl kann man bekannt machen
[1] 5 0 6 8

> decode(v, c(9,9,9,9))
[1] 3707

Verschl�sseln nach dem System (100 101), man muss sich nur 2 Zahlen merken.

> encode(3707, c(100,101)) #kann bei Facebook rein, Schl�ssel geheim
[1] 36 71

> decode(c(36,71), c(100,101))
[1] 3707

16 Graphik mit R, Schwerpunkt BSR-Daten
> demo(graphics)                 # viele Beispiele mit R-code
Beispiel:

16.1 Boxplot
Ein Boxplot besteht immer aus einem Rechteck, genannt Box, und zwei Linien, die dieses Rechteck verl�ngern. Diese Linien werden als "Antenne" oder seltener als "F�hler" oder "Whisker" bezeichnet und werden durch einen Strich abgeschlossen. In der Regel repr�sentiert der Strich in der Box den Median der Verteilung (wiki).

> boxplot(bsr$Mg[bsr$bsrkey=="HM"], horizontal=TRUE)
> title("Boxplot mit Gewichtsdaten nur Hausm�ll Herbst 1994")

16.2 Dot-Plot
Der Dot-Plot bzw. das Punktdiagramm wird auch als das eindimensionale Streudiagramm bezeichnet. In ihm wird eine Variable entweder auf der x-Achse oder auf der y-Achse dargestellt. Je nachdem wie viele Werte die Beobachtungen der Variablen annehmen, ergibt sich das Problem, dass man nur einen Datenpunkt sieht, obwohl sich hinter ihm (viele) weitere Beobachtungen verbergen k�nnen.

Der Dotplot erlaubt Einblicke in die Verteilung einer Variablen, z.�B. wo die Beobachtungen besonders dicht sind oder sich die Beobachtungen auf nur wenige Werte verteilen.
Der Vorteil des Dotplots ist es, dass man jeden Datenpunkt in der Graphik sehen kann, also nicht wie beim z.B. Barplot die Daten zusammengefasst.
# ben�tigt wird das lattice package
library(lattice)                        # package laden

xyplot(bsrkey ~ Mg, main="Dotplot mit Gewichtsdaten nach bsrkey�)

Beim dotplot sieht man jeden Datenpunkt. Deshalb sollte man vorher mit der function unique pr�fen, ob es Wiederholungen bez�glich der Wertepaare Mg und bsrkey gibt.

> non.dup<-bsr[!duplicated(bsr[,c(3,5)]),c(3,5)]
> dim(bsr) ; dim(non.dup)
[1] 15401     7
[1] 3414    2
> xyplot(bsrkey ~ Mg, main="Dotplot mit Gewichtsdaten nach bsrkey", sub="doppelte Datenpunkte entfernt")

16.3 Stem&Leaf
Das Stamm-Blatt-Diagramm (auch Zweig-Bl�tter- oder St�ngel-Blatt-Diagramm sowie in englischer Sprache stem-and-leaf plot oder stemplot) ist ein grafisches Werkzeug der deskriptiven und explorativen Statistik. Als Entwickler dieser Diagrammart gilt John W. Tukey. Beim St&Bl handelt es sich um eine sogenannte ASCII-Graphik.
�hnlich wie das Histogramm oder der Boxplot dient das Stamm-Blatt-Diagramm der Visualisierung von H�ufigkeitsverteilungen. Anders als bei jenen bleibt die Darstellung der Werte jeder einzelnen Beobachtung mit gew�nschter Genauigkeit erhalten. Aus einem Stamm-Blatt-Diagramm lassen sich statistische Kennzahlen wie Modalwert, Median und Quantile ablesen. Allerdings st��t diese Art der Darstellung bei einer gro�en Zahl von Merkmalen an ihre Grenzen. (wiki)
Beachte: Im ST&Bl ist jeder einzele Datenpunkt darstellbar.
BSR-Datensatz enth�lt zuviele Daten f�r St&Bl, deshalb hier nur eine Stichprobe mit 500 Datens�tzen, Auswahl nur HM und IND.

> stich<-sample(15401, 500)
> d1<-bsr$Mg[stich]    ; length(d1)
[1] 500
> d2<-bsr$bsrkey[stich]    ; length(d2)
[1] 500
> stem(d1[d2=="HM"])

  The decimal point is at the |

   0 | 79
   1 | 
   2 | 
   3 | 01222566678899
   4 | 0112334455555677789999
   5 | 01223445555556667777889
   6 | 00011223333467777777788888999
   7 | 0001112223455679999
   8 | 1122344446666788889
   9 | 011112222223444556667899
  10 | 0035557778889
  11 | 022
  12 | 116

> stem(d1[d2=="IND"])

  The decimal point is at the |

   0 | 011122222222222233333333444444455555555566666777888888889999
   1 | 00000000001112223333344455566667777889
   2 | 000233467779
   3 | 034677
   4 | 125667899
   5 | 478
   6 | 4
   7 | 2
   8 | 
   9 | 1
  10 | 0
  11 | 
  12 | 
  13 | 6
  14 | 
  15 | 2
# Untersuchung der Anlieferzeit
> length(zeit[tag==110])
[1] 137

> stem(zeit[tag==110])

  The decimal point is 2 digit(s) to the right of the |

   5 | 36
   6 | 001122335
   7 | 011112233334455555
   8 | 0011122233334455666
   9 | 1122333344456
  10 | 00111222222333455555555666
  11 | 00011111222222234444455556
  12 | 0111112223333444455566
  13 | 
  14 | 
  15 | 
  16 | 
  17 | 
  18 | 
  19 | 
  20 | 
  21 | 
  22 | 
  23 | 44

Zweiseitiges St&Bl

Direkter Vergleich zweier Datenreihen. Ben�tigt wird aus dem package aplpack die function stem.leaf.backback.
help(stem.leaf.backback)
Unlike the stem function in the base package, stem.leaf produces classic stem-and-leaf displays, as described in Tukey's Exploratory Data Analysis. The function stem.leaf.backback creates back-to-back stem and leaf displays.

Umfragedaten, Zufallstichprobe im Umfang 10 %

> stich<-sample(3471, 250)

> reduct<-umfr[stich,]

> attach(reduct)

stem.leaf.backback(NETTO[GESCHL=="WEIBLICH"], NETTO[GESCHL=="MAENNLICH"], m=1)
____________________________________________________________________
  1 | 2: represents 1200, leaf unit: 100 
                        NETTO[GESCHL == "WEIBLICH"]     NETTO[GESCHL == "MAENNLICH"]                   
___________________________________________________________________________________________________________
   39       999999988888877777777776666655444332110| 0 |1223334445677899                              16   
  (42)   988888777665555544444333222222221111100000| 1 |0000000001111222222333334444555556667888999  (43)  
   17                                54432221100000| 2 |0000001122223455555555556688                  38   
    3                                            50| 3 |000557                                        10   
    1                                             0| 4 |25                                             4   
___________________________________________________________________________________________________________
                                                        HI: 5000 5000                                      
n:                                              120     130                                            
NAs:                                             22     33                                             
___________________________________________________________________________________________________________
16.4 Surface, Contor und Image Graphik
Simulierte Daten, x von 0 bis Pi
> x <- seq(0, pi, length= 50)
> y<-sin(x)
> z<-outer(y, y, "*") 
> persp(x, x, z, col = "lightblue", main="Sinush�gel")




> persp(x, x, z, theta = 30, phi = 30, col=rainbow(10), main="Regenbogen")


> contour(x, x, z, col=rainbow(10), main="Regenbogen mit Kontur")


16.5 Echte persische Teppiche
## Persian Rug Art:
x <- y <- seq(-4*pi, 4*pi, len = 27)
r <- sqrt(outer(x^2, y^2, "+"))
opar <- par(mfrow = c(2, 2), mar = rep(0, 4))
for(f in pi^(0:3))
      contour(cos(r^2)*exp(-r/f),
      drawlabels = FALSE, axes = FALSE, frame = TRUE)


Image Graphik
image( z, col=rainbow(20), main="Regenbogen Sinus Bild")

> zuf<-matrix(sample(113, 113), ncol=50, nrow=50)
> image(zuf, col=rainbow(100))
> title("Rainbow Zufallsmuster")

16.6 Dastellung eines Vulkans
# A prettier display of the volcano, aus help (image)
x <- 10*(1:nrow(volcano))
y <- 10*(1:ncol(volcano))
image(x, y, volcano, col = terrain.colors(100), axes = FALSE)
contour(x, y, volcano, levels = seq(90, 200, by = 5),
            add = TRUE, col = "peru")
axis(1, at = seq(100, 800, by = 100))
axis(2, at = seq(100, 600, by = 100))
box()
title(main = "Maunga Whau Volcano", font.main = 4)

16.7 Echte Fotos im R
Der erste ICE, der Berlin am 23.5.1993 verlassen hat, fuhr nach Michendorf in Brandenburg. Es handelt sich um eine Pixelgrafik.
> bitmap<-read.table("ice.txt")
> class(bitmap)
[1] "data.frame"
> bild<-as.matrix(bitmap)
> dim(bild)
[1] 217 216
> bild[1:10,1:20]
      V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20
 [1,]  0  1  0  1  0  0  1  0  0   1   0   0   1   1   0   2   1   1   1   1
 [2,]  0  0  0  0  1  0  0  0  1   0   0   0   0   0   1   1   2   1   1   1
 [3,]  0  1  0  0  0  0  1  0  1   0   0   1   1   1   1   1   1   2   1   2
 [4,]  0  0  0  1  0  1  0  0  0   0   1   0   1   1   1   1   1   1   1   1
 [5,]  0  1  0  1  0  0  0  1  0   0   1   0   1   0   1   1   1   1   0   1
 [6,]  0  0  0  0  0  0  1  0  1   1   2   1   1   2   1   1   1   0   0   1
 [7,]  0  0  0  0  0  1  1  1  0   1   0   2   1   0   0   1   1   0   0   0
 [8,]  1  1  1  1  1  1  2  2  2   1   1   0   0   0   0   1   1   1   1   1
 [9,]  3  1  2  2  3  3  4  3  4   3   3   2   2   1   2   1   1   1   0   1
[10,]  6  7  6  5  5  6  4  3  4   4   4   4   2   5   4   2   1   1   1   0

> unique(c(bild))               # funktion c vektorisiert die Matrix
 [1]  0  1  3  6  4  2  5  7  8  9 10 11 12 13 14 15  # hexadez
> length(c(bild))               #  alle Datenpunkte
[1] 46872
Die Matrix bild muss transponiert werden, sonst ist das Bild quer.
> image(t(bild), col=gray.colors(16,rev=T))
> title("Der erste ICE in Michendorf 1993")

16.8 Fraktale und Graphik
Fraktale sind Gebilde oder Muster und haben eine gebrochene � daher der Name � Dimension und weisen zudem einen hohen Grad von Skaleninvarianz bzw. Selbst�hnlichkeit auf. Das ist beispielsweise der Fall, wenn ein Objekt aus mehreren verkleinerten Kopien seiner selbst besteht. Neben Mandelbrot geh�ren Wac?aw Sierpi?ski und Gaston Maurice Julia zu den namensgebenden Mathematikern.
Hier soll beispielhaft das sogenannte Sierpinski Dreieck behandelt werden aus dem package spt, a collection of algorithms related to Sierpinski.

>install.packages("spt")
>library(spt)
>help(spt)

> (abc = spt(50,60))

         SPT(50,60,70) 

Dimension =  1.604103   Viewport=( 0 , 0 , 94.13205 , 100 )

  Angles        x   y
A     50  0.00000   0
B     70 94.13205   0
C     60 57.73503 100

> plot(abc, iter=7)

16.9 Animation in R, mountain in motion
Funkioniert im R-Studio nicht (zuwenig Arbeitsspeicher).
dreh <- function (dat) {
 x <- seq(0, pi, length= 30)
y<-sin(x)
z<-outer(y, y, "*") 
for(i in 1:360) {
persp(x, x, z, theta = 45, phi = i, expand = 0.5, col = rainbow(10))}
for(i in 1:360) {
persp(x, x, z, theta = i, phi = 45, expand = 0.5, col = rainbow(10))}
}
16.10 Landkartengraphik (mapping)
Installieren des pakets maps
maps()





Verbindung mit statistischer oder geographischer Information

Choropleth
Eine Choroplethenkarte (auch Fl�chenkartogramm) ist eine thematische Karte, bei der die Gebiete im Verh�ltnis zur Verteilungsdichte des thematischen Objektes eingef�rbt, schattiert, gepunktet oder schraffiert sind.
Installation package mapdata
> library(mapdata)
> map("worldHires", "Germany")
> map.cities(country="Germany", minpop=1e6, capital=1)



https://gadm.org/download_country_v3.html


Beispiel: Karte von Frankreich 
data(franceMapEnv)
map('france' ,fill=T, col=rainbow(100))
Wieviele D�partements hat Frankreich?

Beipiel Italien analog.
Beispiel aus help(map) Arbeitslosigkeit USA 2009



Weitere Beispiele:
map("world", "China")
> map.cities(country = "China", capitals = 2
https://cran.r-project.org/web/packages/maps/maps.pdf
17 Data Mining
Neulich war es mal wieder soweit: Nach nur zwei Tagen Abwesenheit quoll der Briefkasten fast �ber - nicht wegen der Unmenge an Post, sondern wegen der F�lle an Werbewurfsendungen. Manchmal sind die Handzettel sogar interessant. Das passiert selten, kommt aber vor. Oft ist es gerade die pers�nlich adressierte Werbung, die ankommt. Das sogenannte "direct mailing" setzt nat�rlich zun�chst voraus, da� die Empf�ngeradresse bekannt ist. Wer nun auch noch den Bezirk oder den Zustand des Wohnhauses ber�cksichtigt, kann noch gezielter werben.
Doch woher wei� der Werber, da� nun ausgerechnet dieses Produkt f�r Frau Meier interessant sein k�nnte und f�r Herrn Schulze nicht? Entweder hat der Absender einfach Gl�ck gehabt, oder er hat "Data Mining" betrieben. Data Mining hei�t eigentlich �bersetzt "Daten sch�rfen". Doch Daten gibt es viele. Unz�hlige. Hier gilt es, die Spreu vom Weizen zu trennen. Das wei� jeder, der aus diesem Grunde schon einmal im Internet eine der Suchmaschinen besch�ftigt hat. Je nach Abfragegeschick liefern Datenbankabfragen immerhin eine sinnvolle Vorauswahl. Die Voraussetzung aber ist, da� man wei�, wonach man sucht.
Beim Data Mining braucht man das nicht. Ungezielt suchen, nennt man das. Denn der Clou beim Data Mining ist die Analyse von riesigen Datenmengen, um noch unbekannte und nicht selten auch v�llig unerwartete Zusammenh�nge herauszufinden. Deswegen bezieht sich die Analyse-Technik Data Mining auch immer auf bereits vorhandene Daten. "Es geht beim Data Mining nicht darum, einfach nur Daten zu gewinnen, sondern es geht um Informationen, und zwar um interessante, relevante Informationen", erkl�rt Martin Barghoorn von Fachbereich Informatik der TU Berlin.
Eine relevante Information f�r den freien Markt ist beispielsweise die Kundensegmentierung, also die Zusammenf�hrung von Kundenadresse und Kundeninteresse. Denn Konsumenten gibt es viele. Hat man durch Data Mining erstmal herausgefunden, da� wom�glich Migr�nepatienten �berproportional h�ufig auch unter Magenschmerzen leiden, k�nnte man diese Verbindung genauer unter die Lupe nehmen, also segmentieren. Vielleicht stellt sich dann heraus, da� es vor allem berufst�tige Frauen um die 30 sind, die unter den Kopfschmerzattacken leiden. Wenn sich nun auch noch entdecken l��t, da� diese Frauen weder Kosten noch M�hen scheuen, teure Medikamente zu erwerben, k�nnte man sicher jemanden finden, dem diese Information einiges wert sein wird. Ein fiktives Beispiel. Tatsache ist jedoch, da� �berall dort, wo automatisch Daten erhoben werden, ein Datenschatz schlummert, der jederzeit mittels Data Mining gehoben werden k�nnte. Und auch gehoben wird.
Durch verschl�sselte E-Mails oder mit Sternchen versehene Adressenangaben in Newsgroups kann man sich vor dem Personen-Check versuchen zu sch�tzen. Im Internet. Zahlt man irgendetwas per Kreditkarte, ist das schon schwieriger. Und selbst bei v�llig anonymen Vorg�ngen, beispielsweise Barzahlung im Supermarkt, k�nnen zumindest noch die durch Scannerkassen erfa�ten Produkte der Einkaufszettel auf m�gliche Zusammenh�nge untersucht werden. In der Bank, beim Arzt oder in Bibliotheken fallen automatisch Daten an, auch solche sehr pers�nlicher Natur. Sie alle k�nnten mittels Data Mining verkn�pft und analysiert werden - und zwar prinzipiell sowohl zum Schaden als auch zum Nutzen des einzelnen. Um m�glichen Mi�brauch auszuschlie�en, ist hier der Datenschutz gefordert. "Man kann sich als einzelne Person so einfach und langfristig nicht grunds�tzlich vor Data Mining sch�tzen", erkl�rt Martin Barghoorn. "Wichtig ist zumindest, da� ein Bewu�tsein dar�ber entsteht, da� Data Mining existiert und �ber kurz oder lang auch angewendet werden wird."
In Deutschland h�lt sich die Informationstechnologie bei der Datenverbindungserkennung noch zur�ck. Noch, wohlgemerkt. In den USA ist das datenbasierte Marketing und somit auch die Analyse-Technik Data Mining schon lange kein Fremdwort mehr. So ist es vermutlich nur eine Frage der Zeit, bis das Datendiamantenfieber �ber den gro�en Teich geschwappt sein wird  
MARIE-LUISE GOERKE , Interview 1999
17.1 Warenkorbanalysen
Die Warenkorbanalyse ist eine Sammlung von statistischen Analysemethoden zur Erstellung von Kundenprofilen. Unter einem Warenkorb versteht man in diesem Zusammenhang die Menge aller innerhalb eines bestimmten Zeitraums gekauften Produkte. Einerseits interessiert bei der Warenkorbanalyse die Kaufwahrscheinlichkeit f�r ein Produkt, andererseits die Bildung von Kundentypen nach ihren Kaufpr�ferenzen.
Im station�ren Einzelhandel werden unter anderem Kundenkarten f�r diesen Zweck verwendet. Mit ihrer Hilfe werden pers�nliche und demographische Daten mit dem Einkaufsverhalten kombinierbar. Gleichzeitig dienen diese Karten der Kundenbindung, da ihre Verwendung mit Rabatten, oder anderen Angeboten �belohnt wird�. Eine sehr einfache, daf�r aber anonyme Erfassungsmethode ist die Abfrage der Postleitzahl des Wohnortes des Kunden, w�hrend des Bezahlvorgangs an der Kasse (z.B. bei Obi).
Im Onlinehandel wird das Kaufverhalten mit Hilfe von Web Analytics �ber Cookies und z.B. Benutzerkonten erfasst.
17.2 Erzeugen einer Co-occurence-Matrix
# Einlesen einer Einkaufs-Textdatei

eink<-read.csv2("einkauf.csv", header=F)

> dim(eink)                             # 176 Eink�ufe
[1] 176   9
> head(eink)                                  # leere Platzhalter
         V1     V2      V3            V4         V5 V6 V7 V8 V9
1    Butter Hanuta Joghurt Mineralwasser      Wurst            
2      Bier   Brot    Eier        Hanuta     Geb�ck            
3      Brot Butter  Brandy     Camembert Emmentaler            
4 Du_Darfst Hanuta Martini         Milch  Mozarella            
5      Bier  Chips  Kandis     Party_Mix  Toblerone            
6      Brot Butter  Hanuta         M�sli   Zitronen        
# ein langer Einbkaufsvektor
> vek<-unlist(eink)  ; length(vek)
[1] 1584
> head(vek)
      V11       V12       V13       V14       V15       V16 
   Butter      Bier      Brot Du_Darfst      Bier      Brot 
216 Levels: Apfelsinen ArienderWeltCD Ata Auto Bananen BeetlesCD Bier Blumen Blumentopf Brot B�cher Butter ... Sekt

# die sortierten einzigen Produkte

> uni<-sort(unique(vek))   ; length(uni)
[1] 216
> head(uni)
[1] Apfelsinen     ArienderWeltCD Ata            Auto           Bananen        BeetlesCD     
216 Levels: Apfelsinen ArienderWeltCD Ata Auto Bananen BeetlesCD Bier Blumen Blumentopf Brot B�cher Butter ... Sekt

# Definition der Produktmatrix proma f�r 2 Produkte

> proma<-outer(uni, uni, paste)  ; dim(proma)      # outer mit paste
[1] 216 216 
# Ausschnitt aus proma
> proma[1:4,1:4]
     [,1]                        [,2]                            [,3]                 [,4]                 
[1,] "Apfelsinen Apfelsinen"     "Apfelsinen ArienderWeltCD"     "Apfelsinen Ata"     "Apfelsinen Auto"    
[2,] "ArienderWeltCD Apfelsinen" "ArienderWeltCD ArienderWeltCD" "ArienderWeltCD Ata" "ArienderWeltCD Auto"
[3,] "Ata Apfelsinen"            "Ata ArienderWeltCD"            "Ata Ata"            "Ata Auto"           
[4,] "Auto Apfelsinen"           "Auto ArienderWeltCD"           "Auto Ata"           "Auto Auto"          

# Vergleich der einzelnen Eink�ufe mit proma
# der 1. Einkauf

e1<-0<match(proma, outer(e<-eink[1,][eink[1,]!=""],e,paste), "==", nomatch=0)

# die 2. bis 5. K�ufe
> e2<-0<match(proma, outer(e<-eink[2,][eink[2,]!=""],e,paste), "==", nomatch=0)
> e3<-0<match(proma, outer(e<-eink[3,][eink[3,]!=""],e,paste), "==", nomatch=0)
> e4<-0<match(proma, outer(e<-eink[4,][eink[4,]!=""],e,paste), "==", nomatch=0)
> e5<-0<match(proma, outer(e<-eink[5,][eink[5,]!=""],e,paste), "==", nomatch=0)
> e6<-0<match(proma, outer(e<-eink[6,][eink[6,]!=""],e,paste), "==", nomatch=0)
> e7<-0<match(proma, outer(e<-eink[7,][eink[7,]!=""],e,paste), "==", nomatch=0)
> e8<-0<match(proma, outer(e<-eink[8,][eink[8,]!=""],e,paste), "==", nomatch=0)
> e9<-0<match(proma, outer(e<-eink[9,][eink[9,]!=""],e,paste), "==", nomatch=0)
> e10<-0<match(proma, outer(e<-eink[10,][eink[10,]!=""],e,paste), "==", nomatch=0)
> e<-e1+e2+e3+e4+e5+e5+e6+e7+e8+e9+e10
> comat<-matrix(e, nrow=216, ncol=216, byrow=T)
> length(uni)
[1] 216
> dimnames(comat)[[1]]<-uni
> dimnames(comat)[[2]]<-uni
> comat[1:12,1:12]
               Apfelsinen ArienderWeltCD Ata Auto Bananen BeetlesCD Bier Blumen Blumentopf Brot B�cher Butter
Apfelsinen              1              0   0    0       0         0    0      1          0    1      0      0
ArienderWeltCD          0              0   0    0       0         0    0      0          0    0      0      0
Ata                     0              0   0    0       0         0    0      0          0    0      0      0
Auto                    0              0   0    0       0         0    0      0          0    0      0      0
Bananen                 0              0   0    0       0         0    0      0          0    0      0      0
BeetlesCD               0              0   0    0       0         0    0      0          0    0      0      0
Bier                    0              0   0    0       0         0    3      0          0    1      0      0
Blumen                  1              0   0    0       0         0    0      1          0    1      0      0
Blumentopf              0              0   0    0       0         0    0      0          1    0      0      0
Brot                    1              0   0    0       0         0    1      1          0    4      0      2
B�cher                  0              0   0    0       0         0    0      0          0    0      0      0
Butter                  0              0   0    0       0         0    0      0          0    2      0      4
17.3 Data-Mining mit Datenbank
Die relevante Tabelle hei�t books. Zuerst Lesen aus Tabelle.
Verbindung der Bibliotheksdaten in books.
> chan<- odbcDriverConnect("")
Ausw�hlen books.xls.dsn in demselben Verzeichnis, wo sich books.xls befindet.
Welche Tabelle sind vorhanden?
> sqlTables(chan)
                         TABLE_CAT TABLE_SCHEM TABLE_NAME   TABLE_TYPE REMARKS
1 D:\\Lehre\\Statistik3\\books.xls        <NA>     BOOKS$ SYSTEM TABLE    <NA>
2 D:\\Lehre\\Statistik3\\books.xls        <NA>   Database        TABLE    <NA>
> b<-sqlQuery(chan, "select * from \"books$\" ")

> dim(b)
[1] 26659     6
> b[1:7,]
  AUTOR                                                      TITEL JAHR        GEBIET SIGN   ID
1  <NA>  MATHEM.MODELLE U.VERFAHREN D.UNTERNEHMENSFORSCHUNG...1968 1968        52T843   OR 7406
2  <NA>   HERRN VON LEIBNITZ' RECHNUNG MIT NULL UND EINS.2.ED.1966 1966        53A026 AEDV    2
3  <NA> MATHEMATISCHE STANDARDMODELLE DER OPERATIONSFORSCHUNG.1971 1971        52Q034   OR 7408
4  <NA> MATHEMATISCHE STANDARDMODELLE DER OPERATIONSFORSCHUNG.1971 1971        52Q034   OR 7409
5  <NA>       MATHEMATISCHE THEORIE OPTIMALER PROZESSE.2.AUFL.1967 1967 52I206 06N506   OR 7410
6  <NA>           METHODES A CHEMIN CRITIQUE.INTERN.CONF.1967.1969 1967        52T402   OR 7411
7  <NA>                NETZPLANTECHNIK FUER DEN MATERIALFLUSS.1968 1968        52T400   OR 7412
# Jeder Buchtitel ist ein Begriffstupel. Es gibt nur gro�e Buchstaben.
# Beispiel f�r unscharfe Suche

> unsch<-sqlQuery(chan, "select * from \"books$\" where titel like '%THEO%' and titel like '%COMPUT%' and autor like '%BA%'")

????????????
???????


???????
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????????????????????????????????????????????????????????????

# Untersuchen verschiedener Hypothesen.

Entwicklung der Programmiersprachen ALGOL, FORTRAN, SIMULA, APL, BASIC, LISP, C, PASCAL, SNOBOL von 1950 bis heute.

Weiter interessante Themen:
# Gegensatz Theory-Practice

# Methoden-Applicationen, class-classification

Artificial Inteligence-Networks, parallel, 
Data base-databank, Datenbank, Datenschutz, Security

Die Ergebnisse k�nnen mit der Buchzahl pro Jahr gewichtet werden.
M�glichst Zusammenfassung der Ergebnisse in Tabellen!
Auch Graphik ist erw�nscht.
Beispiel:

SELECT JAHR,TITEL FROM books WHERE TITEL LIKE '%LISP%'ORDER BY JAHR
                                                                              
 JAHR  TITEL                                                                  
                                                                              
 1967  LISP 1.5 PRIMER.1967                                                   
 1967  LISP 1.5 PRIMER.1967                                                   
 1967  LISP 1.5 PRIMER.1967                                                   
 1971  PROGRAMMER'S INTRODUCTION TO LISP.1971                                 
 1972  PROGRAMMER'S INTRODUCTION TO LISP.1972                                 
 1972  LISP 1.5 PROGRAMMER'S MANUAL.2.ED.1972                                 
 1972  LISP 1.5 PROGRAMMER'S MANUAL.2.ED.1972                                 
 1972  LISP 1.5 PROGRAMMER'S MANUAL.2.ED.1972                                 
 1972  LISP 1.5 PROGRAMMER'S MANUAL.2.ED.1972                                 
 1974  PROGRAMMING LANGUAGE LISP:ITS OPERATION AND APPLICATIONS.1974          
F 1974  PROGRAMMING LANGUAGE LISP:ITS OPERATION AND APPLICATIONS.1974          
 1974  PROGRAMMING LANGUAGE LISP:ITS OPERATION AND APPLICATIONS.1974          
 1974  LITTLE LISPER.1974                                                     
 1976  LET'S TALK LISP.1976                                                   
 1976  LET'S TALK LISP.1976                                                   
 1978  ANATOMY OF LISP.1978                                                   
 1978  ANATOMY OF LISP.1978                                                   
 1980  LISP.1980                                                              
 1981  LISP-DETAILS.1981                                                      
 1981  LISP-DETAILS.1981                                                      
 1981  LISP-DETAILS.1981                                                      
 1981  LISP.1981                                                              
 1982  EINFUEHRUNG IN DAS PROGRAMMIEREN IN LISP.1982                          
 1982  EINFUEHRUNG IN DAS PROGRAMMIEREN IN LISP.1982                          
 1984  CONF.RECORD OF THE ACM SYMP.ON LISP AND FUNCTIONAL PROGRAMMING.1984    
 1984  LEARNING LISP.1984                                                     
 1984  BEGINNER'S GUIDE TO LISP.1984                                          
 1984  BEGINNER'S GUIDE TO LISP.1984                                          
 1984  COMMON LISP.8.PRINT.1984                                               
 1984  COMMON LISP.8.PRINT.1984                                               
 1984  LISP.1984                                                              
 1984  LISP.1984                                                              
 1984  LISP.1984                                                              
 1984  LISPCRAFT.2.PRINT.1984                                                 
 1984  LISP.2.ED.1984                                                         
 1984  LISP.2.ED.1984                                                         
 1985  EINFUEHRUNG IN DAS PROGRAMMIEREN IN LISP.2.ED.1985                     
 1985  INTRODUCTION TO LISP.1985                                              
 1985  UEBERSETZUNG VON LISP IN DIE REDUKTIONSSPRACHE BRL.1985                
 1985  CONF.RECORD OF THE LISP CONF.1980.REPR.1985                            
 1985  LISP.1985                                                              
 1986  LISP.1.NACHDR.1986                                                     
 1986  PERFORMANCE AND EVALUATION OF LISP SYSTEMS.3.PR.1986                   
 1987  AUTOLISP.VERSION 2.6.HANDBUCH FUER PROGRAMMIERER.1987                  
 1987  LISP.1987                                                              
 1987  PROGRAMMER'S GUIDE TO COMMON LISP.2.PR.1987                            
 1987  THE LITTLE LISPER.1987                                                 
 1988  PROC. OF THE ACM CONF. ON LISP AND FUNCTIONAL PROGRAMMING.1988         
 1988  AUTOLISP IN DER ANWENDUNG.1988                                         
 1988  AUTOLISP.1988                                                          
 1988  LISP AND SYMBOLIC COMPUTATION.JG.1.1988 FF.                            
 1989  COMMON LISP PROGRAMMING FOR ARTIFICAL INTELLIGENCE.1989                
 1989  OBJECT-ORIENTED PROGRAMMINGIN COMMON LISP.1989                         
 1989  GOLDEN COMMON LISP.1989                                                
 1989  LISP POINTERS.VOL.3.1989 FF.                                           
 1990  PARALLEL LISP:LANGUAGES AND SYSTEMS.1990                               
 1990  COMMON LISP.2.ED.1990                                                  
 1990  PROC. OF THE 1990 ACM CONF. ON LISP AND FUNCTIONAL PROGRAMMING.1990    
 1992  ACM CONFERENCE ON LISP AND FUNCTIONAL PROGRAMMING.1992                 
 1994  ACM CONF. ON LISP AND FUNCTIONAL PROGR.1994 IN: LISP POINTERS.7,3.1994 
       LISP PROGRAMMING LANGUAGE                                              
       PROGRAMMERS INTRODUCTION'S TO LISP                                     
       LISP 1.5 PROGRAMMER'S MANUAL                                           
       LISP 1.5 PROGRAMMER'S MANUAL                                           
       LISP 1.5 PROGRAMMER'S MANUAL                                           
       LISP 1.5 PROGRAMMER'S MANUAL                                           
       PROGRAMMATION NON NUMERIQUE LISP 1.5                                   
       LET'S TALK LISP                                                        
       LISP 1.5 PRIMER                                                        
       LISP 1.5 PRIMER                                                        
       ACM CONF. ON LISP AND FUNCTIONAL PROGRAMMING    

SQL Klausel Group by und Order by:       
               
> ab2<-sqlQuery(chan, "SELECT JAHR, COUNT(ID) AS ANZAHL FROM \"BOOKS$\" GROUP BY JAHR ORDER BY JAHR")
> dim(ab2)
[1] 60  2
> head(ab2)
  JAHR ANZAHL
1   NA   5941
2 1871      1
3 1900      1
4 1907      1
5 1912      1
6 1916      1
> ab2<-ab2[-1,] ; dim(ab2)
[1] 59  2
> head(ab2)
  JAHR ANZAHL
2 1871      1
3 1900      1
4 1907      1
5 1912      1
6 1916      1
7 1935      1   
barplot(ab2$ANZAHL, col=rainbow(60), names.arg=ab2$JAHR, main="Aufstieg und Untergang einer Bibliothek", sub="Anzahl B�cher")

17.4 Text-Mining
Komponenten des analytischen CRMs sind das Data Warehouse und Analysewerkzeuge wie das Online Analytical Processing (OLAP) und das Data Mining mit jeweils unterschiedlichen Aufgaben im Kontext der Wissensgewinnung und Entscheidungsunterst�tzung f�r kundengerichtete Prozesse. (Zitat: enzyklopaedie der wirtschaftsinformatik).
In Bezug auf die zugrunde liegenden Datenquellen des Data Mining im CRM, lassen sich ferner die Auspr�gungen des Text Mining und des Web Mining - zur Analyse unstrukturierter Kundeninformationen in Form von Texten, bzw. von Weblog Daten � als Spezialformen des Data Mining, welche im CRM Anwendung finden, unterscheiden [Bensberg, Schultz 2001, S. 680].
Die zentrale Zielsetzung, die mit dem Konzept des Customer Relationship Managements (CRM) verfolgt wird, liegt in der langfristigen Bindung profitabler Kunden an das Unternehmen. Als wesentliche Grundlage hierf�r gilt ein umfassendes Wissen �ber die Struktur, das Verhalten und die Bed�rfnisse der Kunden. Die Organisation dieses Wissens � d. h. dessen Bewahrung, Bereitstellung und Analyse � obliegt im CRM-Konzept dem analytischen CRM (aCRM).
Im Rahmen des Knowledge Discovery in Databases (KDD) besch�ftigt sich Text Mining, als Erg�nzung zum Data Mining, mit der Analyse von halb- oder unstrukturierten Textdatenbest�nden. Hauptziel ist es, weitgehend automatisiert aus gro�en Dokumentenkollektionen aussagekr�ftige Muster sowie Inhalte zu identifizieren und sie dem Anwender komprimiert als interessantes Wissen zu pr�sentieren. Aufgrund der qualitativen Natur der Daten sind die Problemstellungen algorithmisch komplexer als beim klassischen Data Mining.
Das noch recht junge Forschungsgebiet "Text Mining" umfa�t eine Verbindung von Verfahren der Sprachverarbeitung mit Datenbank- und Informationssystemtechnologien. Es entstand aus der Beobachtung, dass ca. 85% aller Datenbankinhalte nur in unstrukturierter Form vorliegen, so dass sich die Techniken des klassischen Data Mining zur Wissensgewinnung nicht anwenden lassen. Beispiele f�r solche Daten sind Volltextdatenbanken mit B�chern, Unternehmenswebseiten, Archive mit Zeitungsartikeln oder wissenschaftlichen Publikationen, aber auch Str�me kontinuierlich auflaufender Emails oder Meldungen von Nachrichtenagenturen (Newswires).

Im Gegensatz zum Information Retrieval geht es beim Text Mining nicht darum, lediglich Dokumente anhand von Anfragen aufzufinden, sondern aus einem einzelnen oder einem Satz von Dokumenten neues Wissen zu gewinnen, etwa durch automatische Textzusammenfassungen, die Erkennung und Verfolgung benannter Objekte oder die Aufdeckung neuer Trends in Forschung und Industrie. Durch die st�ndig wachsende Zahl elektronisch verf�gbarer Texte werden automatisch arbeitende Verfahren zur Bew�ltigung der Informationsflut immer dringender, was Text Mining zu einem sehr aktiven und auch kommerziell interessanten Forschungsgebiet macht. 
Volltext: http://digbib.ubka.uni-karlsruhe.de/volltexte/1000005161

Wegen Umstellung auf Windows 10 ist die Datenbank leider derzeit nicht erreichbar. Deshalb zun�chst das Thema Text-Mining.

Wir verwenden das R-Package tm. Bitte herunterladen und in R laden.
Gegebenenfalls werden weitere Pakete ben�tigt.

Wir arbeiten zuerst mit txt.Dateien (ascii). Die Texte werden nicht importiert, sondern man erzeugt Verkn�pfungen und spart Arbeitsspeicher.

Durch Defintion einer Verbindung (Assoziation) wird ein Abbild von Dateien (Datei) im aktiven Workspace erzeugt, mit dem man arbeiten kann, als w�ren die Daten tats�chlich im workspace, eine moderne und sehr effiziente Technik.

library�                                      # package tm laden
                          
# Pfad-Definition f�r Textdateien

txt<-"D:\\Lehre\\dokus"

# Corpus erzeugen, representing a collection of text documents.

> mine <- VCorpus(DirSource(txt),readerControl=list(language="de"))
> mine
<<VCorpus>>
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 4

Was ist in mine enthalten? Inspizieren.

> inspect(mine[1:2])
<<VCorpus>>
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 2

[[1]]
<<PlainTextDocument>>
Metadata:  7
Content:  chars: 6856

[[2]]
<<PlainTextDocument>>
Metadata:  7
Content:  chars: 1253


# Normale Funktionen lassen sich anwenden

> summary(mine)
                   Length Class             Mode
blechtrommel.txt   2      PlainTextDocument list
karlsruhe.txt      2      PlainTextDocument list
mining.txt         2      PlainTextDocument list
 rolling_stones.txt 2      PlainTextDocument list

# verschiedene Transformationen mit Paket tm und Funktion tm_map m�glich.

as.PlainTextDocument, removeNumbers, removePunctuation, removeWords, stemDocument, stripWhitespace

# vergl. tm-Dokument D:/Programme/R/R-3.1.2/library/tm/doc/tm.pdf

Texte vor der Bearbeitung, Auswahl 3. Text

> mine[[3]][1]
$content
[1] "Neulich war es mal wieder soweit: Nach nur zwei Tagen Abwesenheit quoll der Briefkasten fast �ber - nicht wegen der Unmenge an Post

Verschiedene Bearbeitungen

> mine <- tm_map(mine, content_transformer(tolower))
> mine[[3]][1]
[1] "neulich war es mal wieder soweit: nach nur zwei tagen abwesenheit quoll der briefkasten fast �ber - nicht wegen der unmenge an post

> mine <- tm_map(mine, removePunctuation)
> mine[[3]][1]
[1] "neulich war es mal wieder soweit nach nur zwei tagen abwesenheit quoll der briefkasten fast �ber  nicht wegen der unmenge an post

> mine <- tm_map(mine, removeNumbers)
> mine[[3]][1]
[1] "neulich war es mal wieder soweit nach nur zwei tagen abwesenheit quoll der briefkasten fast �ber  nicht wegen der unmenge an post


> mine <- tm_map(mine, removeWords, stopwords("german"))
> mine[[3]][1]
[1] "neulich   mal  soweit   zwei tagen abwesenheit quoll  briefkasten fast    wegen  unmenge  post

> mine <- tm_map(mine, stripWhitespace)
> mine[[3]][1]
[1] "neulich mal soweit zwei tagen abwesenheit quoll briefkasten fast wegen unmenge post

namespace(name) : there is no package called �SnowballC�, nachinstallieren
Achtung, andere Syntax!


> tm_map(mine, stemDocument)
<<VCorpus>>
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 4
> mine[[3]][1]

[1] neulich mal soweit zwei tagen abwesenheit quoll briefkasten fast wegen unmenge post

Creating Term-Document Matrices
A common approach in text mining is to create a term-document matrix from a corpus. In the tm package the classes TermDocumentMatrix and DocumentTermMatrix (depending on whether you want terms as rows and
documents as columns, or vice versa) employ sparse matrices for corpora.

> dtm <- DocumentTermMatrix(mine)

> dtm
<<DocumentTermMatrix (documents: 4, terms: 1036)>>
Non-/sparse entries: 1118/3026
Sparsity           : 73%
Maximal term length: 30
 Weighting          : term frequency (tf)

> inspect(dtm[,80:85])
> inspect(dtm[,80:85])
<<DocumentTermMatrix (documents: 4, terms: 6)>>
Non-/sparse entries: 6/18
Sparsity           : 75%
Maximal term length: 10
Weighting          : term frequency (tf)

                    Terms
Docs                 barghoorn barzahlung bedeckte bedeutet begriff begriffen
  blechtrommel.txt           0          0        1        0       0         2
  karlsruhe.txt              0          0        0        0       0         0
  mining.txt                 2          1        0        0       0         0
   rolling_stones.txt         0          0        0        1       1         0

>  findFreqTerms(dtm, 9)
 [1] "breit"      "data"       "gro�mutter" "lang"       "mehr"       "mining"    
 [7] "rihanna"    "schon"      "thomas"     "zeit"      

# Entfernen seltener W�rter

> inspect(removeSparseTerms(dtm, 0.4))
A document-term matrix (4 documents, 10 terms)

Non-/sparse entries: 32/8
Sparsity           : 20%
Maximal term length: 11 
Weighting          : term frequency (tf)

                    Terms
Docs                 automatisch beim geht immer lang mehr schon zeit zur�ck zwei
  blechtrommel.txt             0    1    0     1    7    8     3    1      1    4
  karlsruhe.txt                1    1    1     1    0    0     0    0      0    0
  mining.txt                   2    4    2     1    1    1     3    1      1    1
  rolling_stones.txt           1    1    4     2    1    3     5    7      3    2

# Wortschatzanalyse

> apply(dtm, 1 ,sum)
  blechtrommel.txt      karlsruhe.txt         mining.txt rolling_stones.txt 
               564                 93                307                709

# Vergleich mit der F�llw�rter-Version
 
> dtm2<-DocumentTermMatrix(mineorg)

> apply(dtm2, 1 ,sum)
  blechtrommel.txt      karlsruhe.txt         mining.txt rolling_stones.txt 
              1027                144                538               1238

> apply(dtm, 1 ,sum)/apply(dtm2, 1 ,sum)
  blechtrommel.txt      karlsruhe.txt         mining.txt rolling_stones.txt 
         0.5462512          0.6250000          0.5631970          0.5646204

# Finde Zusammenh�nge, Korrelationen

> findAssocs(dtm, "mining", 0.95)
      daten information        data 
0.98       0.98        0.97

17.5 Definition eines Lexikons
> inspect(DocumentTermMatrix(mine,
+ list(dictionary = c("data", "kartoffel", "nummer","mining"))))
<<DocumentTermMatrix (documents: 4, terms: 4)>>
Non-/sparse entries: 6/10
Sparsity           : 62%
Maximal term length: 9
Weighting          : term frequency (tf)

                    Terms
Docs                 data kartoffel mining nummer
  blechtrommel.txt      0         7      0      0
  karlsruhe.txt         1         0      4      0
  mining.txt           12         0     12      0
  rolling_stones.txt    0         0 






Inhalt
Digitale Datenanalyse und statistische Methoden	1
1.1 Was ist Statistik?	1
1.2 Geschichte, zwei Wurzeln	1
1.3 Entwicklung der Statistik	2
1.4 Definition heute	3
2.1 Deskriptive Statistik, Datenanalyse	3
2.2 Charakterisierung von Merkmalen	4
2.3 H�ufigkeits- und Kontingenztabellen	5
2.4 Kontingenztabellen, Kreuztabellen	6
2.5 Statistische Lage- und Skalenparameter	7
2.6 Zusammenhangsma�e	12
2.7 Fragen und Stundenaufgaben	16
3.1 Technische Grundlagen des R-Systems	17
3.2 Mathematische Operatoren und ihre Symbole im R-System	18
3.3 Logische Operatoren	18
3.4 Wichtige elementare Datenstrukturen in R, vgl. Video	19
3.5 Die R-Import- und Exportprogramme f�r Daten (Tabellen)	21
3.6 Programme sind Funktionen, R ist eine funktionale Sprache	22
3.7 Datenauswertung: Lebendgeborene seit 1950 in Deutschland	22
3.8 Berechnung von Prozentwerten, Spaltendivision	26
3.9 Analyse weiterer Datens�tze der amtlichen Statistik	30
4.0 Stichproben	30
4.1 Vorteile von Stichproben	30
4.2 Nachteile von Stichproben	31
4.3 Repr�sentativit�t von Stichproben	31
4.4 Zufallsstichproben	32
4.5 Sequentielle Stichprobenverfahren	32
4.6 Probability Proportional Size	32
4.7 Geschichtete Stichprobe	33
4.8 Klumpen-Stichproben (Cluster Sampling)	33
4.9 Auswertung von Zufallsstichproben der Umfragedaten	34
4.10 Das Problem fehlende Werte (NA)	35
4.10 Vergleich der einfachen Zufallsstichprobe und der       geschichteten Stichprobe, Einkommen und K�rpergr��e	36
Schichtung des Nettoeinkommens mit dem Merkmal Geschlecht	37
5. Besonderheiten der Markt- und Meinungsforschung	38
6 R-Datenstrukturen	39
6.1 Datentyp und Datenstruktur. Vektoren, Listen und Arrays	39
6.2 Indizierung von R-Objekten	41
6.3 Selektive Zuweisung (update)	42
6.4 Listen durch Gruppierung mit split, Gruppenvektor	42
6.5 Der Array und seine Regeln	48
6.6 Funktionen auf arrays	49
6.7 Zusammenf�gen von arrays	51
6.7 Datensatz geburt_land.csv umformen (3-D-Array, Data-Cube)	54
7	Vergleich diverser Statistik Programme	56
8 Datenbanken und R	56
8.1 Sprache SQL	56
8.2 Was ist eine Datenbank, ein DBMS, ein RDBMS	57
8.3 Vorteile von Datenbanken	57
8.4 Client-Server Prinzip	59
8.5 ODBC und RODBC	60
8.6 R-Verkn�pfung mit Datenbank, Datenbanktabelle lesen	60
9 Explorative Datenanalysen (EDA)	68
9.1 Datenkontrolle durch Graphik, BSR-Daten von 1994	68
9.2 Standard R-Variante f�r Entdeckung schlechter F�lle	73
9.3 BSR-Datensatz ohne Wiederholungen und Fehler	73
9.3	Vergleichende graphische Analyse von Anzahl und Gewicht	74
9.4 Vergleich Anzahl-Gewicht in einer Graphik	76
9.4	Boxplots mit BSR-Daten	77
10 Eier und H�hner	79
10.1 Datenerhebung Eier	79
10.2 Empirische Verteilungsfunktion	82
10.3 Gibt es Doubletten?	83
10.3 K�ken und H�hner	84
11. Der praktische Dataframe und seine subsets	89
11.1 Data.frame versus Matrix und array	89
11.2 Subsets von Data.frames	93
12 Simulation - Splines	95
12.1 Spline-Funktion f�r Hausm�llmenge	95
13.2 Simulation eines digitalen Sterns	96
Erstellung von Vektorgraphiken	96
Was ist der Vorteil von Vektorgrafiken?	97
Verktorgrphik und Pixelgraphik	97
12.3 Digitale Schmetterlinge	99
12.4 Entwicklungsphasen f�r Schmetterlinge	101
13 Auswertung Einkommen in Deutschland	103
13.1 Verteilung des Einkommens	104
13.2 Daten von 2017	106
14 Kreuztabelle, OLAP und data cube	108
14.1 Daten und Einheiten einer Kreuztabelle	109
14.2 Count frequencies, Zahl der F�lle, BSR-Daten korrigiert	110
Zweidimensionale Analyse � 2 -way	111
Dreidimensionale Analyse - 3-way � Ergebnis ein array	111
14.3 Mehrdimensionale Kreuztabelle mit gemessenen Daten	112
Eindimensionale Analyse, One-way, bsr	112
Zweidimensionale Analyse � 2 -way	112
Dreidimensionale Analyse - 3-way	113
15 Speicherbedarf und Verschl�sselung	114
15.1 Speicherbedarf von Daten nach Datentyp	114
15.2 Kodieren, Verschl�sseln, Sparen von Raum und Zeit, Verbesserung der Sicherheit, Datenschutz	115
15.2 Dekodieren, Entschl�sseln als Umkehrfunktion	117
15.3 Geburtstag, wieviele Tage haben Sie schon gelebt	118
16 Graphik mit R, Schwerpunkt BSR-Daten	119
16.1 Boxplot	120
16.2 Dot-Plot	121
16.3 Stem&Leaf	123
16.4 Surface, Contor und Image Graphik	125
16.5 Echte persische Teppiche	127
16.6 Dastellung eines Vulkans	128
16.7 Echte Fotos im R	129
16.8 Animation in R, mountain in motion	130
16.8 Landkartengraphik (mapping)	130
Wieviele D�partements hat Frankreich?	131
17 Data Mining	133
17.1 Warenkorbanalysen	134
17.2 Erzeugen einer Co-occurence-Matrix	134
17.3 Data-Mining mit Datenbank	136
17.4 Text-Mining	139
17.5 Definition eines Lexikons	143

1 Dr. Ing Herbert Vo�, Kryptografie mit JAVA, Franzis Verlag 2006
---------------

------------------------------------------------------------

---------------

------------------------------------------------------------

Martin Barghoorn   ???? FU-Berlin ???????Digitale Datenanalyse und statistische Methoden mit  ????146

	

